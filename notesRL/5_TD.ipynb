{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dff74663-f5ac-4cb2-ab40-57a4ce53d3b3",
   "metadata": {},
   "source": [
    "# Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97692760-6be0-4220-9d14-6f945ca68a3c",
   "metadata": {},
   "source": [
    "- <font color=blue>和MC learning一样，TD learning也是model-free类的方法。但是TD相比MC而言的优势体现在它的算法是incremental形式。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1005c7c5-9d80-44a6-9909-2de5adf8ef4d",
   "metadata": {},
   "source": [
    "#### 从RM到TD发生了哪些本质变化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94ef717-3d44-4cba-a4dc-25f7b11b0d54",
   "metadata": {},
   "source": [
    "- <font color=norange>RM算法是一种通过迭代法求解函数期望值的方法，但不能直接用于强化学习，TD算法是RM算法在强化学习的条件下进行改造后得到的算法。 </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f42b5d-153e-45e4-b6ca-b0fba4567c9d",
   "metadata": {},
   "source": [
    "1. RM算法要点：\n",
    "   - RM的迭代式中涉及单一变量x，估计所使用的样本是被估计目标Ef(x)的真实值的抽样，比如均值问题中 $\\hat g(w,\\eta)是g(w)=w-Ef(x)的样本$\n",
    "   - 所以<font color=blue>**RM算法本质上是supervised learning算法**</font>。迭代$w_{t+1}=w_t-\\alpha_t(w_t-f(x_t))$过程中，估计量w通过逼近Ef(x)的真实样本f(x)而向Ef(x)收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039cebbc-5666-4619-ab59-b0fe114bc939",
   "metadata": {},
   "source": [
    "2. TD算法要点：\n",
    "   - 但强化学习中，涉及的变量是多变量 $s_i\\in S$ 。更重要的是，此时学习过程中没有被估计目标真实值的样本。\n",
    "     - 如: 估计$v(s)$时，$v_t(s)的样本只有r_{t+1}+\\gamma v_t(s')，而不是r_{t+1}+\\gamma v^{\\pi}(s')$\n",
    "   - TD算法在RM算法基础上放宽了抽样条件，既然没有真实值，就<font color=green>**用当前对真实值的最佳估计来代替真实值**</font>——这就是TD算法相对RM算法做出的最大变化。\n",
    "   - 好在，<font color=blue>**在满足一定条件的时候，TD算法仍然满足almost surely convergence。**</font>\n",
    "     - RM的收敛性质是用Dvorotzky's theorem证明的，而本节介绍的TD类算法中的tabular Sarsa(on-policy)和tabular Q-learning(off-policy)的收敛性质是用Dvorotzky's theorem的多变量扩展形式证明的。\n",
    "     - <font color=red>而下一章中Sarsa/Q-learning with nonlinear function approximation就无法保证almost surely convergence</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555dd266-afbf-4f6c-9586-3daef08e0356",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## I. reinforcement learning的基本概念"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075eb188-99b3-409a-b54c-6cb96ac1c942",
   "metadata": {},
   "source": [
    "1. on-policy和off-policy\n",
    "   -  **behavior policy**：用于生成经验样本的策略\n",
    "   -  **target policy**: 学习的目标，在学习过程中不断被更新以converge到最优策略\n",
    "   -  <font color=blue>**on-policy learning**</font>：behavior policy和target policy是同一个策略\n",
    "   -  <font color=blue>**off-policy learning**</font>：behavior policy和target policy不是同一策略\n",
    "- <font color=green>**off-policy的优点**</font>：用别的策略生成的样本来learn最优策略会带来一些好处。比如：\n",
    "  1. 这个被学习的策略可以是人类的操作。\n",
    "  2. <font color=red>hebavior policy可以故意设为有很强的exploration能力的policy，这会提升模型learn最优策略的能力。</font>因为，估计action value要对所有state-action pair估计$Q(s,a)$。这意味着每个pair都需要大量sample。如果target policy本身没有很强的exploration的能力，就很难做到。比如Sarsa用$\\epsilon -greedy$来保证一定的exploration ability，但$\\epsilon$的值通常设置得很小，exploration的效果有限。但是如果能用一个有很强的exploration能力的策略专门来generate episodes，然后用off-policy learning来学习最优策略，那么learning效率会有大幅提升。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8191aaeb-c477-4913-b6ab-332daecf5784",
   "metadata": {},
   "source": [
    "2. online planning和offline planning\n",
    "   - ① **offline planning**：the agent updates the values and policies using pre-collected experience data without interacting with the environment.\n",
    "   - ② **online planning**：the agent updates the values and policies while interacting with the environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4674fc-ae47-40de-b23d-1f88b5c9d2a6",
   "metadata": {},
   "source": [
    "3. sample and episode\n",
    "   - ① **sample**: 在online planning中，agent的一次行为反馈过程$(s, a, s^{'}, r)$称为一个sample。\n",
    "   - ② **episode**: 在online planning中，agent持续take action，并collect samples，直到达到terminal state的整个过程称为一个episode。通常要经历多个episods才能完成exploration。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2558313c",
   "metadata": {},
   "source": [
    "4. model-based learning and model-free learning \n",
    "   - ① <font color=blue>**model-based learning**</font>：先利用exploration得到的episodes数据，来构造model估计transition function和rewards function，然后就可以将问题转化为MDP问题求解。\n",
    "     - a. 估计transition function：$\\hat T(s, a, s^{'})=\\frac{\\#(s, a, s^{'})}{\\#(s, a)}$ \n",
    "     - b. 估计reward function：$\\hat R(s, a, s^{'})= R(s, a, s^{'})$ \n",
    "   - ② <font color=blue>**model-free learning**</font>：不估计transition function和rewards function，而是直接估计value或者Q-values of states.比如MC和TD。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec67dc4",
   "metadata": {},
   "source": [
    "## II.basic TD：TD learning of state value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61496f1b-196d-49b5-834e-550822c0de9e",
   "metadata": {},
   "source": [
    "### II.1 思路：用RM迭代估计$V^{\\pi}(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40038cd0-c8bb-4814-9b3b-f0523e6808f5",
   "metadata": {},
   "source": [
    "- <font color=orange>和MC based direct learning相似，用抽样的方法来估$V^{\\pi}(s)$ 。但改用RM算法的迭代方式而不是MC方法来估。同样，估计了$V^{\\pi}(s)$之后，他们不能直接做policy improvement，还需要reward function和transition function的信息才能将$V^{\\pi}(s)$ 转化为$Q^{\\pi}(s,a)$才能做policy improvement。所以这个方法不会在实践中使用，只是作为引例方便介绍后续相关算法。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd92e16b-71d3-4e8b-a4fe-c5c35dffdd32",
   "metadata": {},
   "source": [
    "### II.2 basic TD迭代式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063811c2-0626-48bc-8ab0-e428cb3fdd87",
   "metadata": {},
   "source": [
    "#### II.2.1 迭代式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8301c36",
   "metadata": {},
   "source": [
    "$$\\begin{align} \n",
    "iterate: \\\\\n",
    "& v^{\\pi }_{t+1}(s_t) \\leftarrow v_t^{\\pi}(s_t)-\\alpha_t \\left [v_t^{\\pi}(s_t)-\\left ( r_{t+1} + \\gamma v^{\\pi}_t(s_{t+1}) \\right ) \\right ]\\\\\n",
    "& v_{t+1}^{\\pi}(s) \\leftarrow v^{\\pi}_t(s), \\forall s\\ne s_t\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba3cb02-d632-410d-b32b-b4217bb115dc",
   "metadata": {},
   "source": [
    "#### II.2.2 用RM推导迭代式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c8885e-1c70-4a53-921d-4930d5319762",
   "metadata": {},
   "source": [
    "1. 原目标问题：求$V^{\\pi}(s)$\n",
    "   - 符号： $V^{\\pi}(s)= E_{a\\sim\\pi(s),s'\\sim P(S'|s,a)}[r(s, a, s') + \\gamma V^{\\pi }(S')] $\n",
    "   - 简记为：$V^{\\pi}(s) = E[R(s)+\\gamma V^{\\pi}(S^{'})|s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a8c4a3-373b-430a-8e2a-6c72258b3992",
   "metadata": {},
   "source": [
    "2. 样本：t时刻的状态记为$s_t$，对应1个样本：$(s_t, a_t, s_{t+1}, r_{t+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74d252f-e7bc-4300-9a4e-93ad20409923",
   "metadata": {},
   "source": [
    "3. **分析**：\n",
    "   - <font color=norange>原目标问题表达式右边的期望表达式中有$V^{\\pi}(s')$，导致右式嵌套了左式，看起来和一般的RM算法求解案例不同。但实际上，在给定策略$\\pi$和环境模型$R, P(S'|s,a)$之后，右边的期望值本质上是s的函数。由于期望的作用，该式中没有不确定性，对应一个scalar constant，就是$V^{\\pi}(s)$的真实值。</font>\n",
    "   - 唯一的差别在于，这里要求多个状态s对应的状态函数均值，而不是1个。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0048b511-4f7f-4324-918a-e4ce9c880dc9",
   "metadata": {},
   "source": [
    "4. 转化为RM问题：$g(w(s))=w(s)-V^{\\pi}(s)=w-E[R(s)+\\gamma V^{\\pi}(S^{'})|s]=0$\n",
    "$$\\begin{align}\n",
    "g(w(s_t)) & = w(s_t) - E[R(s_t)+\\gamma V^{\\pi}(S_{t+1})|s_t] \\\\\n",
    "\\tilde g(w(s_t)) & = w(s_t) - [r_{t+1}+\\gamma V^{\\pi}(s_{t+1})] \\\\\n",
    "\\eta_t & = \\tilde g(w(s_t)) - g(w(s_t)) \\\\\n",
    "& = E[R(s_t)+\\gamma V^{\\pi}(S_{t+1})|s_t] - [r_{t+1}+\\gamma V^{\\pi}(s_{t+1})]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832f0bf3-10ca-477f-9ead-1c01e7de2363",
   "metadata": {},
   "source": [
    "5. 迭代式：\n",
    "   - 直接代入RM算法得到的迭代式：\n",
    "$$\\begin{align}\n",
    "w_{t+1}(s_{t}) & = w_t(s_t) -\\alpha_t(s_t)\\tilde g(w_t(s_t)) \\\\\n",
    "& = w_t(s_t) -\\alpha_t(s_t)[w_t(s_t) - (r_{t+1}+\\gamma V^{\\pi}(s_{t+1}))]\n",
    "\\end{align}$$\n",
    "     - 式中$V^{\\pi}(s_{t+1})是状态s_{t+1}$的真实状态价值。如果该值已知，那么直接代入迭代过程，本质上就是在做supervised learning。这也是RM算法的迭代方式。可是该值在迭代过程中并不知道，它本身也是迭代求解的对象。\n",
    "   - <font color=blue>**在当前步中对$V^{\\pi}(s_{t+1})$的最佳估计量是$w_t(s_{t+1})$，将$V^{\\pi}(s_{t+1})$替代为$w_t(s_{t+1})$就得到了basic TD算法的迭代式：**</font>\n",
    "$$w_{t+1}(s_{t}) = w_t(s_t) -\\alpha_t(s_t)[w_t(s_t) - (r_{t+1}+\\gamma w_t(s_{t+1}))]\n",
    "$$\n",
    "       - 符号说明：t时刻状态记为$s_t$。给定策略$\\pi$，t时刻对状态s的state value的估计记为$w_t(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98796cb4-211e-4556-b499-2430642f1a29",
   "metadata": {},
   "source": [
    "#### II.2.3 迭代式的收敛性：almost surely convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26038d33-ffb8-47d7-8e65-c3c3b9e9909a",
   "metadata": {},
   "source": [
    "- <font color=brown> [证明详见math of RL 7.1.3节] </font>\n",
    "- 简单说明：证明需要用到Dvorotzky's theorem的扩展形式\n",
    "  - 迭代式转换为Dvorotzky's theorem对应的迭代式形式如下：\n",
    "$$\\begin{align}\n",
    "w_{t+1}(s_{t}) \n",
    "& = w_t(s_t) -\\alpha_t(s_t)[w_t(s_t) - (r_{t+1}+\\gamma w_t(s_{t+1}))]\\\\\n",
    "w_{t+1}(s_{t})  - w^{\\pi }(s_{t})\n",
    "& = w_t(s_t)  - w^{\\pi }(s_{t})-\\alpha_t(s_t)[w_t(s_t) - w(s_{t}) - (r_{t+1}+\\gamma w_t(s_{t+1}) - w^{\\pi }(s_{t}))]\\\\\n",
    "\\\\\n",
    "\\Delta_{t+1} & = \\Delta_t - \\alpha_t[\\Delta_t - (r_{t+1}+\\gamma w_t(s_{t+1}) - w^{\\pi }(s_{t}))] \\\\\n",
    "& = (1-\\alpha_t)\\Delta_t+\\alpha_t[r_{t+1}+\\gamma w_t(s_{t+1}) - w^{\\pi }(s_{t})] \\\\\n",
    "& = (1-\\alpha_t)\\Delta_t+\\alpha_t(\\eta_t)\\\\\n",
    "其中，\\eta_t & =r_{t+1}+\\gamma w_t(s_{t+1}) - w^{\\pi }(s_{t})\n",
    "\\end{align}$$\n",
    "      - <font color=red>$\\eta_t中包含了对v_t(s)$而言来自外部的不确定性，此时可以证明它满足Dvorotzky's theorem所需性质，从而保证了迭代的收敛特征。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa98da23-faae-453c-82ad-1c5bdb788a2a",
   "metadata": {},
   "source": [
    "### II.3 算法的性质和特征"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7de622-ac3b-4f78-8f98-3350f8eed584",
   "metadata": {},
   "source": [
    "#### II.3.1 迭代式分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff27dd49",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align} \n",
    "\\overset{新估计}{\\overbrace{v^{\\pi}_{t+1}(s_t)}} \n",
    "& = \\underset{旧估计}{\\underbrace{v^{\\pi}_t(s_t)}} - \\alpha_t\\left [ \\underset{TD\\ error}{\\underbrace{v^{\\pi}_t(s_t)-(\\overset{TD\\ target\\ value}{\\overbrace{r_{t+1} + \\gamma v^{\\pi}_t(s_{t+1})} } )}}  \\right ] \\\\\n",
    "& = (1-\\alpha_t)\\underset{旧估计}{\\underbrace{v^{\\pi}_t(s_t)}}+\\alpha_t (\\underset{TD\\ target\\ value}{\\underbrace{r_{t+1} + \\gamma v^{\\pi}_t(s_{t+1})} } )\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fb9c00",
   "metadata": {},
   "source": [
    "1. 新估计是旧估计和TD target value的加权平均。所谓target value的意思就是新的样本会让估计值从旧估计结果向这个value方向偏移，因此叫target value。\n",
    "2. TD error也称为temperal-difference。它反映了时刻t对state value的估计值与新的单个样本获得的估计值的差异，也体现了新的单个样本提供的信息量大小。\n",
    "3. 这个迭代的前提是给定策略$\\pi$，不能用在策略不断变化的场景。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bbe816-1da8-49f5-91be-9180131781b7",
   "metadata": {},
   "source": [
    "#### II.3.2 TD learning和MC learning的特征对比"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecff93c-71f1-4b1d-b432-fbf32f8974b6",
   "metadata": {},
   "source": [
    "| TD | MC |\n",
    "|---------|---------|\n",
    "| incremental，因此可以处理连续任务，可以处理实时learning任务   | non-incremental，MC exploring starter要用整个episode从后往前取样本点，因此不能处理连续实时任务   |\n",
    "| bootstrapping，$v_{t+1}(s_t)依赖v_t(s_t)$。所以需要对估计量做initial guess，且initial guess的数值会影响算法效果 | non-bootstrapping,$v_{t+1}(s_t)不依赖v_t(s_t) $   |\n",
    "| low variance(almost surely convergence), 但有bias   | unbias, 但high variance   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae08c1c",
   "metadata": {},
   "source": [
    "## III. Tabular Sarsa：TD learning of action value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93558268-244e-44f4-a0df-ade6db4cf4a7",
   "metadata": {},
   "source": [
    "- 用RM迭代估计$Q^{\\pi}(s, a)$，<font color=blue>做policy evaluation</font>。<font color=green>与basic TD的区别就是把对$V^{\\pi}(s)$的估计改成直接估计$Q^{\\pi}(s, a)$。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8809eb5b-c8ef-4f51-9b47-94d62c20a891",
   "metadata": {},
   "source": [
    "### III.1 Sarsa algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fede5cee",
   "metadata": {},
   "source": [
    "#### III.1.1 迭代式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67fa1e8-4154-4551-8292-5548a6cf8369",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align} \n",
    "q_{t+1}(s_t, a_t) \n",
    "& \\leftarrow q_t(s_t, a_t) -\\alpha_t(s_t, a_t) [q_t(s_t, a_t) -(r(s_t,a_t,s_{t+1}) + \\gamma q_t(s_{t+1}, a_{t+1})) ]\\\\\n",
    "& = q_t(s_t, a_t) -\\alpha_t(s_t, a_t) [q_t(s_t, a_t) -( r_{t+1} + \\gamma q_t(s_{t+1}, a_{t+1}) ) ]\\\\\n",
    "q_{t+1}(s, a)& \\leftarrow q_t(s,a), \\forall (s, a)\\ne (s_t, a_t) \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ea9038",
   "metadata": {},
   "source": [
    "#### III.1.2 用RM算法推导迭代式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75291736-22b8-4006-9c01-d9f7d78e4f8d",
   "metadata": {},
   "source": [
    "1. 目标问题：行为价值函数的Bellman Equation\n",
    "$$q_{\\pi }(s, a)= E[R+\\gamma q^{\\pi }(S', A')|(s, a)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4167b67-b3cb-4845-bbe9-de6b5cb4bf24",
   "metadata": {},
   "source": [
    "2. RM迭代式推导：\n",
    "   - 转化问题：\n",
    "$$\\begin{align}\n",
    "g(w(s, a)) & = w(s,a)-E[R+\\gamma q^{\\pi }(S', A')|(s, a)] \\\\\n",
    "\\tilde g(w(s, a)) & = w(s,a)-[r(s,a,s^{'})+\\gamma q^{\\pi }(s^{'}, a^{'})] \\\\\n",
    "\\eta(s,a) & = \\tilde g(w(s, a)) - g(w(s, a)) \\\\\n",
    "& = [r(s,a,s^{'})+\\gamma q^{\\pi }(s^{'}, a^{'})]-E[R+\\gamma q^{\\pi }(S', A')|(s, a)]\\\\\n",
    "\\end{align}$$\n",
    "   - 迭代式：\n",
    "$$\\begin{align}\n",
    "\\tilde g(w(s, a)) & = w(s,a)-[r(s,a,s^{'})+\\gamma q^{\\pi }(s^{'}, a^{'})] \\\\\n",
    "w_{t+1}(s_t,a_t) & = w_t(s_t,a_t)-\\alpha_t(s_t,a_t)*\\tilde g(w_t(s_t, a_t)) \\\\\n",
    "& = w_t(s_t,a_t)-\\alpha_t(s_t,a_t)*[w_t(s_t,a_t)-(r_{t+1}+\\gamma w_t(s_{t+1}, a_{t+1})) ]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6c0997-ac6d-4387-a00a-7b6d308734af",
   "metadata": {},
   "source": [
    "#### III.1.3 收敛性：almost surely convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5664cead-463f-4285-be94-0b081c669103",
   "metadata": {},
   "source": [
    "- <font color=brown>证明思路和TD learning一样</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd58add5-632d-484a-927d-8bf423c8337c",
   "metadata": {},
   "source": [
    "#### III.1.4 算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50c2c28",
   "metadata": {},
   "source": [
    "- 算法伪码\n",
    "   - <img src=\"./pics/sarsa.png\" alt=\"alt text\" width=\"560\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f649ce3a-809c-4e7e-8dc7-a7d5e1b992a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### III.2 n-step Sarsa algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3924af4-2f33-4810-b5fa-a1bf5dc2d717",
   "metadata": {},
   "source": [
    "#### III.2.1 思路"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa78ca56",
   "metadata": {},
   "source": [
    "- <font color=blue>取MC和Sarsa的折中，使用多几步的真实reward+下一个状态的价值估计，以此来控制bias-variance tradeoff。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e91f125-5e8c-4e36-bde7-260039330da9",
   "metadata": {},
   "source": [
    "#### III.1.2 算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045ac64d",
   "metadata": {},
   "source": [
    "1. 迭代方式\n",
    "$$\n",
    "\\begin{align}\n",
    "& Q^{\\pi }_{t+1}(s_t,a_t) = \\\\\n",
    "& Q^{\\pi }_t(s_t,a_t)-\\alpha_t \\left [Q^{\\pi }_t(s_t,a_t)-\\left ( r_{t+1} + \\gamma r_{t+2}+...+ \\gamma^{n-1} r_{t+n} +\\gamma^nQ^{\\pi}_t(s_{t+n}, a_{t+n}) \\right ) \\right ]\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "2. 详细算法（略）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9225fc2-0bb4-4df4-b5d0-c5f35e6028fb",
   "metadata": {},
   "source": [
    "## IV. Tabular Q-learning: TD learning of optimal action value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f8b000-57c9-4e14-b29b-a9a590dae72b",
   "metadata": {},
   "source": [
    "### IV.1 思路"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb924358",
   "metadata": {},
   "source": [
    "1. 回顾Sarsa的算法思路：Sarsa在找最优策略的时候要分成两步，先做policy evaluation，再做policy improvement。原因是它的迭代式是在保持策略$\\pi$不变的条件下估计action value$Q^{\\pi}(s, a)$。得到action value的估计值之后再做policy improvement。\n",
    "2. Q-learning的出发点：<font color=blue>**直接用RM算法的迭代方式找optimal action value。**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39169f7e-d63c-47e3-b060-b37f008d4913",
   "metadata": {},
   "source": [
    "### IV.2 迭代式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5f0046-caf5-4262-9b6c-0ca52b18c6a4",
   "metadata": {},
   "source": [
    "#### IV.2.1 迭代式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0eda37",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align} \n",
    "sample\\_q(s_t, a_t)\n",
    "& = r(s_t,a_t,s_{t+1}) + \\gamma \\underset{a}{max}q^{\\pi}_t(s_{t+1}, a)\\\\\n",
    "& = r_{t+1} + \\gamma \\underset{a}{max}q^{\\pi}_t(s_{t+1}, a)\\\\\n",
    "q_{t+1}(s_t, a_t) \n",
    "& \\leftarrow q_{t}(s_t, a_t)-\\alpha_t \\left [q_{t}(s_t, a_t)-sample\\_v(s_t)\\right ]\\\\\n",
    "& = q_{t}(s_t, a_t)-\\alpha_t \\left [q_{t}(s_t, a_t)-\\left ( r_{t+1} + \\gamma \\underset{a}{max}q^{\\pi}_t(s_{t+1}, a)\\right ) \\right ]\\\\\n",
    "q_{t+1}^{\\pi}(s, a)& \\leftarrow q^{\\pi}_t(s, a), \\forall (s, a)\\ne (s_t, a_t)  \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ec1250",
   "metadata": {},
   "source": [
    "- **和Sarsa的取样方式差异**: sarsa在每次迭代的时候要用$(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1})$，但Q-learning只用$(s_t, a_t, r_{t+1}, s_{t+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdae252-4ab5-410d-85ef-e187db40a95c",
   "metadata": {},
   "source": [
    "#### IV.2.2 RM算法推导迭代式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ffb17f-a913-48ce-a356-025067a34db9",
   "metadata": {},
   "source": [
    "1. 目标问题：行为价值函数的Bellman Optimality Equation\n",
    "$$q(s, a)  = E[R+\\gamma \\underset{a^{'}}{max}q(S^{'}, a^{'})|(s, a)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9ec9bd-6474-4094-9c58-218989dd40f5",
   "metadata": {},
   "source": [
    "2. RM迭代式推导：\n",
    "   - 转化问题：\n",
    "$$\\begin{align}\n",
    "g(w(s, a)) & = w(s,a)- E[R+\\gamma \\underset{a}{max}q(S', a)|(s, a)]\\\\\n",
    "\\tilde g(w(s, a)) & = w(s,a)-[r(s,a,s^{'})+\\gamma \\underset{a^{'}}{max}q(s^{'}, a^{'})] \\\\\n",
    "w_{t+1}(s_t,a_t) & = w_t(s_t,a_t)-\\alpha_t(s_t,a_t)*\\tilde g(w_t(s_t, a_t)) \\\\\n",
    "& = w_t(s_t,a_t)-\\alpha_t(s_t,a_t)*[w_t(s_t,a_t)-(r_{t+1}+\\gamma \\underset{a\\in \\mathcal{A_{t+1}}}{max}w_t(s_{t+1}, a)) ]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30ec357-d6cf-425a-bc5c-2f28ae848d92",
   "metadata": {},
   "source": [
    "### IV.3 两种算法实现方式：off-policy 和 on-policy learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0288d87d-8c1b-48e4-9b8a-32793532f6cf",
   "metadata": {},
   "source": [
    "#### IV.3.1 on-policy和off-policy的使用场景区别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e61ee6-0062-4652-954c-1b533f62b3f4",
   "metadata": {},
   "source": [
    "- <font color=blue>**为什么Sarsa和MC learning要用on-policy learning，而Q-learning可以用off-policy learning？**</font>\n",
    "  - 因为Q-leaning要求解的目标问题是BOE，而Sarsa和MC learning要求解的问题是Bellman Equation。\n",
    "  - Bellman Equation评估的是给定策略$\\pi$。\n",
    "  - BOE并没有对应策略，可以直接选择每种state-action条件下的optimal value和optimal policies。\n",
    "    <img src='pics/off-policy.png' width='100%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b59cfe-acda-4572-9b3f-5e60bd430ca0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### IV.3.2 Q-learning算法的on-policy实现(实践中不用)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d787e9",
   "metadata": {},
   "source": [
    "1. on-policy learning\n",
    "   - 此时生成数据的策略也使用被优化的那个策略。\n",
    "   - 算法伪码：\n",
    "     - <img src=\"./pics/qlearningon.png\" alt=\"alt text\" width=\"560\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c5a599-a665-4733-8049-8357acd25a43",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### IV.3.3 Q-learning算法的off-policy实现(实践中使用的方法)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc25efc",
   "metadata": {},
   "source": [
    "- Q-learning通常是off-policy learning的形式\n",
    "- 这时候专门有一个behavior策略$\\pi_b$来生成数据，然后target策略会持续迭代优化。\n",
    "- 算法伪码：\n",
    "     - <img src=\"./pics/qlearningoff.png\" alt=\"alt text\" width=\"560\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc24796",
   "metadata": {},
   "source": [
    "## IV. 小结：用统一的视角来看待MC/Sarsa/Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636bc7d3-d951-4c32-86f4-6f5144515b20",
   "metadata": {},
   "source": [
    "- 目标问题：\n",
    "  - <img src=\"./pics/target_expression.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da434ae1-df31-46a6-9a8e-423f4b375859",
   "metadata": {},
   "source": [
    "| algs | 求解目标 |\n",
    "|---------|---------|\n",
    "| Sarsa   | $E[R_{t+1}+\\gamma Q(s_{t+1}, a_{t+1})|s_t, a_t]$   |\n",
    "| n-step Sarsa   | $E[R_{t+1}+\\gamma R_{t+2}+...+\\gamma^n Q(s_{t+n}, a_{t+n})|s_t, a_t]$   |\n",
    "| Q-learning   | $E[R_{t+1}+\\gamma max_a Q(s_{t+1}, a)|s_t, a_t]$  |\n",
    "| MC   | $E[R_{t+1}+\\gamma R_{t+2}+\\gamma^2R_{t+3}+...]$   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf0fdde-ef68-4896-ad0e-02133dcccb7b",
   "metadata": {},
   "source": [
    "- 迭代式形式：\n",
    "  - <img src=\"./pics/iterate_expression.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a75a41-f125-4f55-9d37-7c69e266a319",
   "metadata": {},
   "source": [
    "| algs | 迭代形式 |\n",
    "|---------|---------|\n",
    "| Sarsa   | $r_{t+1}+\\gamma q_t(s_{t+1}, a_{t+1})$   |\n",
    "| n-step Sarsa   |$r_{t+1}+\\gamma r_{t+2}+...+\\gamma^n q_t(s_{t+n}, a_{t+n})$  |\n",
    "| Q-learning   | $r_{t+1}+\\gamma max_aq_t(s_{t+1}, a)$ |\n",
    "| MC   | $r_{t+1}+\\gamma r_{t+2}+\\gamma^2r_{t+3}+...$   |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
