{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "692eb9fe",
   "metadata": {},
   "source": [
    "# 基本概念"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7570b3-4b01-4c44-ae8e-f0dacb6a27a1",
   "metadata": {},
   "source": [
    "## I. Terminologyies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8974a0b-16a2-4a0f-8835-a93d53b35fa2",
   "metadata": {},
   "source": [
    "- <img src=\"./pics/agentandenvironment.png\" style=\"zoom:70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d238ca6f",
   "metadata": {},
   "source": [
    "1. Agent和Environment：Agent在Environment中执行action，并得到对应reward。Agent是执行行为的主体，它在环境中如果执行action是reinforcement learning学习的目标。比如达里奥游戏中的游戏任务就是一个Agent。\n",
    "2. state s：agent所处的环境在不同时期的状态，agent根据不同的状态选择执行action，并得到对应的rewards。比如：达里奥中的当前state就是游戏当前所处的那一帧画面\n",
    "3. action a：agent在环境中的行为动作称为action。比如：达里奥中，agent可以做出的行为包括{左，右，上}，因此，$a \\in \\{left, right, up\\}$。\n",
    "4. <font color=blue>policy function $\\pi$：$\\pi(a|s)=P_\\pi(A=a|S=s)=P_\\pi(a,s)$</font>\n",
    "   - agent在给定环境状态s下所采取的策略称为policy function，简称policy。policy有随机性，给定state s，agent所采取的action的分布就是policy。\n",
    "5. <font color=blue>state transition function：$P(S_{t+1}|S_t=s, A_t=a)$</font>\n",
    "$$old\\ state \\overset{action}{\\rightarrow} new\\ state$$\n",
    "   - 状态转移函数描述了给定t时刻状态s，agent执行action a后，下一时刻状态$s_{t+1}$的分布情况。由于环境本身有随机性，因此状态转移函数也有随机性。比如，马里奥中，当马里奥前进吃掉一个coin之后，旁边的demon下一步可能靠近它，也可能走向反方向。\n",
    "6. <font color=blue>reward R：$r_t=R(s_t, a_t, s_{t+1})$</font>\n",
    "   - agent在给定的环境状态s中执行一次action就会得到对应的一个reward。$a_t$的reward的大小由$s_t$，$a_t$和$s_{t+1}$决定。比如：达里奥中，收集硬币R=1，什么都不做R=0，通关成功R=1000，通关失败R=-1000.\n",
    "7. utility/return U：从t时刻开始一直到行为结束的n时刻得到的所有rewards折现之和。$\\gamma是折现率$\n",
    "$$\n",
    "\\begin{align} \n",
    "U_0 & =R_0+\\gamma R_{1}+\\gamma^2 R_{2}+...++\\gamma^{n} R_{n} \\\\\n",
    "& =  {\\textstyle \\sum_{t=0}^{n}}\\gamma ^tR_t \\\\\n",
    "& =  {\\textstyle \\sum_{t=0}^{n}}\\gamma ^tR(s_t, a_t, s_{t+1}) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "   - (1)n是time horizon，当$n=\\infty$时，time horizon是infinitely。\n",
    "   - (2)agent的目标是找到最合理的action使得预期总体回报最大化。\n",
    "8. episode: agent根据其策略与环境互动直到terminal state的过程是构成一个trajectory，也称为一个episode或者trial。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bba9ba",
   "metadata": {},
   "source": [
    "## II. 随机性\n",
    "1. 整个状态变化过程中有两个随机性的来源：\n",
    "   - ① randomness in <font color=red>**actions**</font> from policy function: $A\\sim \\pi(a|s)=P(a|s)$ \n",
    "   - ② randomness in <font color=red>**states**</font> from state transition function: $S \\sim P(s_{t+1}|s_t, a_t)$\n",
    "2. rewards中的随机性：\n",
    "   - 由于action有随机性，如果按照给定策略$\\pi$行动，那么实际发生的$a_t$有随机性。同时，$s_t$取决于$P(s_t|s_{t-1},a_{t-1})$也有随机性，因此$r_t=R(s_t, a_t)$的随机性同时受action和states中的随机性影响。\n",
    "3. utility中的随机性： \n",
    "   - rewards中的随机性会自然传递给utility。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31272da",
   "metadata": {},
   "source": [
    "## III. value functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42479cd4",
   "metadata": {},
   "source": [
    "1. **action-value function**：<font color=orange>**评估action的优劣**</font>\n",
    "$$\\begin{align} \n",
    " Q_{\\pi}(s, a) & = E[ U|\\pi, s_0=s, a_0=a ] \\\\\n",
    " & =E[ {\\textstyle \\sum_{t=0}^{n}}\\gamma ^tR(s_t, a_t, s_{t+1})|\\pi, s_0=s, a_0=a ] \\\\\n",
    " \\\\\n",
    " Q^*(s, a) & = \\max_{\\pi} Q_{\\pi}(s, a) \\\\\n",
    " & = \\max_{\\pi}  E[ U|\\pi, s_0=s, a_0=a ] \\\\\n",
    "& =\\max_{\\pi} E[ {\\textstyle \\sum_{t=0}^{n}}\\gamma ^tR(s_t, a_t, s_{t+1})|\\pi, s_0=s, a_0=a ]\n",
    "\\end{align}$$\n",
    "   - **理解**：\n",
    "     - ① <font color=red>$Q_{\\pi}(s, a)$给定当前state s，假定从下一时刻(t=1)起都使用策略$\\pi$的条件下，action a可以获得的平均总回报。</font><font color=blue>衡量给定策略$\\pi$，在当前状态s下选择action a怎么样</font> \n",
    "     - ② <font color=red>$Q^*(s, a)$给定当前state s，假定从下一时刻(t=1)起都使用最优策略的条件下，action a可以获得的平均总回报。</font><font color=blue>衡量在最优决策条件下，在当前状态s下选择action a怎么样</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9c2b15",
   "metadata": {},
   "source": [
    "2. **state-value function**：<font color=orange>**评估state的优劣**</font>\n",
    "$$\\begin{align} \n",
    "V_{\\pi}(s) & = E_A[ Q_{\\pi}(s, A) ] \\ ,\\ 其中A\\sim \\pi(a|s)\\\\\n",
    "& = E[ {\\textstyle \\sum_{t=0}^{n}}\\gamma ^tR(s_t, a_t, s_{t+1})|\\pi, s_0=s ] \\\\\n",
    "\\\\\n",
    "V^*(s)& = \\max_{\\pi} V_{\\pi}(s)\\\\\n",
    "& = \\max_{\\pi} E[ U|\\pi, s_0=s ] \\\\\n",
    "& = \\max_{\\pi} E[ {\\textstyle \\sum_{t=0}^{n}}\\gamma ^tR(s_t, a_t, s_{t+1})|\\pi, s_0=s ]\n",
    "\\end{align}$$\n",
    "   - **理解**：\n",
    "     - ① <font color=red>$V_{\\pi}(s)$给定策略 𝜋的条件下，当前state s可以获得的平均总回报。</font><font color=blue>衡量给定策略$\\pi$，agent当前所处状态s怎么样</font> \n",
    "     - ② <font color=red>$V^*(s)$给定当前state s，假定一直使用最优策略的条件下，可以获得的平均总回报。</font><font color=blue>衡量在最优决策条件下，当前所处状态s怎么样</font> \n",
    "     - ③ $E_s(V_{\\pi}(s))$可以衡量策略$\\pi$本身的优劣"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
