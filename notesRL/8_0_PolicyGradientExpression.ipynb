{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f6db28e-ab4b-4cbd-b3ba-104ec91dc37a",
   "metadata": {},
   "source": [
    "# 几种不同角度的Policy Gradient Expression的推导"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce413bf5-2a0b-4a9a-98c3-f9af3c90fb6b",
   "metadata": {},
   "source": [
    "- <font color=brown>符号说明：文中 $a_t和u_t$ 混用，都表示t时刻的action。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3935311-9d64-4aff-b2a0-325eee035f2e",
   "metadata": {},
   "source": [
    "## I. 直接推导目标和梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7444f12b-d9fd-4306-aec7-37d803c29b43",
   "metadata": {},
   "source": [
    "### I.1 目标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555db879-d5eb-471a-8399-432173161dce",
   "metadata": {},
   "source": [
    "#### I.1.1 目标的两种展开方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a65a52f-ca53-4d85-88a5-5059ebedc45a",
   "metadata": {},
   "source": [
    "1. trajectory的期望rewards：\n",
    "$$J(\\theta)  = E_{\\tau }R(\\tau|\\pi_{\\theta}) = \\sum_{\\tau }P(\\tau ;\\theta )R(\\tau )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1982c6b1-225f-4ea3-9576-f072edd8525d",
   "metadata": {},
   "source": [
    "2. 不同状态价值的均值：\n",
    "$$\\begin{align}\n",
    "严格说应该是初始状态的价值期望值：\\\\\n",
    "J(\\theta) & = E_{s\\sim d(s_0)}[V^{\\pi_{\\theta }}(s)]=\\sum_{s_0\\in \\mathcal{S} }d(s_0)V^{\\pi_{\\theta }}(s_0)\\\\\n",
    "也可以取s处于stationary\\ distribution时的分布：\\\\\n",
    "J(\\theta) & = E_{s\\sim d(s)}[V^{\\pi_{\\theta }}(s)]=\\sum_{s\\in \\mathcal{S} }d(s)V^{\\pi_{\\theta }}(s)\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ebabcd-44ab-4eeb-b7ef-0c1b6f514bf1",
   "metadata": {},
   "source": [
    "#### I.1.2 目标展开式暗含的假设条件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924eccb8-c109-4253-8773-4bfd6b64b7fd",
   "metadata": {},
   "source": [
    "- 期望值合理的前提是随机变量的分布稳定。也就是说，trajectory展开式中，假设了trajectory的分布稳定；状态价值展开式中，假设了状态s的分布稳定。<font color=red>但这两个条件在现实条件下都不成立。</font>\n",
    "- 在状态价值展开式中，理论上，如果agent与环境交互的次数极大的条件下，$d(s)$可以用策略对应的stationary distribution。但是这个条件在真实环境中通常也很难满足。\n",
    "- 尽管他们的分布处于不稳定状态，实践中还是会用monte carol抽样法直接估计期望的方式来得到估计量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffd7646-fc5b-436c-836e-0779dc70b683",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### I.2 用trajectory的期望rewards推导梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977c9981-d37b-4d84-ac7d-c54302233a62",
   "metadata": {},
   "source": [
    "#### I.2.1 梯度的基本公式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f02fac-cd40-44ad-85ec-8e64946ee3dc",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "\\nabla _{\\theta }J(\\theta ) \n",
    "& = \\nabla _{\\theta } \\sum_{\\tau }P(\\tau ;\\theta )R(\\tau ) \\\\\n",
    "& =  \\sum_{\\tau }\\nabla _{\\theta }P(\\tau ;\\theta )R(\\tau ), {\\color{green}{这步是因为\\theta带给\\tau的随机性都体现在P(\\tau;\\theta)里面}} \\\\\n",
    "& =  \\sum_{\\tau }P(\\tau ;\\theta )\\frac{\\nabla _{\\theta }P(\\tau ;\\theta )}{P(\\tau ;\\theta )}R(\\tau ) \\\\\n",
    "& =  E_{\\tau }\\nabla _{\\theta }logP(\\tau ;\\theta )R(\\tau ) \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42cdc9f-5c29-4904-9428-550052c6d534",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "\\nabla _{\\theta }logP(\\tau ;\\theta ) \n",
    "& = \\nabla _{\\theta }log\\left[ d(s_0)\\prod_{t=0}^{H-1} P(s_{t+1}|s_t, u_t)*\\pi_{\\theta }(u_t|s_t) \\right ]\\\\\n",
    "& = \\nabla _{\\theta }\\left[logd(s_0) + \\sum_{t=0}^{H-1}logP(s_{t+1}|s_t, u_t) + \\sum_{t=0}^{H-1}log\\pi_{\\theta }(u_t|s_t)\\right ]\\\\\n",
    "& = \\underset{0}{ \\underbrace{\\nabla _{\\theta }logd(s_0)}}  + \\underset{0}{\\underbrace{\\nabla _{\\theta }\\sum_{t=0}^{H-1}logP(s_{t+1}|s_t, u_t)}} + \\nabla _{\\theta }\\sum_{t=0}^{H-1}log\\pi_{\\theta }(u_t|s_t)\\\\\n",
    "& = \\nabla _{\\theta }\\sum_{t=0}^{H-1}log\\pi_{\\theta }(u_t|s_t) \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803a00c4-ae4f-467e-a515-bfc87ba44d78",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "取梯度：g & = \\nabla _{\\theta }J(\\theta )\\\\\n",
    "& =  E_{\\tau }\\nabla _{\\theta }logP(\\tau ;\\theta )R(\\tau ) \\\\\n",
    "& = E_{\\tau }\\left[\\nabla _{\\theta }\\sum_{t=0}^{H-1}log\\pi_{\\theta }(u_t|s_t)R(\\tau )\\right ]\\\\\n",
    "& = E_{\\tau }\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t|s_t)\\sum_{t=0}^{H-1}r(s_t, u_t)\\right ]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2012fe80-7107-4643-9779-6fab919edae5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### I.2.2 rewards to go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d5c2b1-8194-4e71-a7fd-add002873b6b",
   "metadata": {},
   "source": [
    "$$g = E_{\\tau }\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t|s_t)\\sum_{t'=t}^{H-1}r(s_{t'}, u_{t'})\\right ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63be9e4-f19d-4333-9b48-68d73ebba08b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### I.2.3 baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accfd6a0-59b4-477b-9d94-c222715e97a9",
   "metadata": {},
   "source": [
    "- 当b是常数时，下式成立：\n",
    "$$\\begin{align}\n",
    "E_{\\tau }[\\nabla _{\\theta }logP(\\tau ;\\theta )]b& = \\nabla _{\\theta } \\sum_{\\tau }P(\\tau ;\\theta )b \\\\\n",
    "& = b\\nabla _{\\theta } \\sum_{\\tau }P(\\tau ;\\theta )\\\\\n",
    "& = b\\nabla _{\\theta }1 = 0\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5666cb-6ecc-48bc-97d6-5cf86667a8ef",
   "metadata": {},
   "source": [
    "- <font color=norange>**即使b是states的函数，只要b(s)不受action的影响，等式仍然成立。注意，这里关键是b(s)与action的关系，而不是b(s)与策略的关系。即使b(s)受策略的影响，比如图取值为状态价值函数V(s)也不改变结果。**</font>见下面分析过程："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9ee64c-8acf-410c-81fa-a22f94371fd6",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "E_{\\tau }\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(a_t|s_t)b(s_t)\\right ]\n",
    "& = E_{\\tau }\\left[ \\sum_{t=0}^{H-1}[\\nabla _{\\theta }log\\pi_{\\theta }(a_t|s_t)]V^{\\pi }(s_t)\\right ]\\\\\n",
    "& = \\underset{s_t\\in\\mathcal{S},a_t\\in \\mathcal{A}}{E}\\left[ \\sum_{t=0}^{H-1}[\\nabla _{\\theta }log\\pi_{\\theta }(a_t|s_t)]V^{\\pi }(s_t)\\right ]\\\\\n",
    "& = \\sum_{t=0}^{H-1}\\left[\\sum_{s_t\\in\\mathcal{S}}P(s_t)\\sum_{a_t\\in \\mathcal{A}} [\\pi_{\\theta }(a_t|s_t)\\nabla _{\\theta }log\\pi_{\\theta }(a_t|s_t)]V^{\\pi }(s_t)\\right ]\\\\\n",
    "& = \\sum_{t=0}^{H-1}\\left[\\sum_{s_t\\in\\mathcal{S}}P(s_t)\\sum_{a_t\\in \\mathcal{A}}[\\nabla _{\\theta }\\pi_{\\theta }(a_t|s_t)]V^{\\pi }(s_t)\\right ],{\\color{green}{因为V(s_t)与a_t无关}}\\\\\n",
    "& = \\sum_{t=0}^{H-1}\\left[\\sum_{s_t\\in\\mathcal{S}}P(s_t)V^{\\pi }(s_t)\\sum_{a_t\\in \\mathcal{A}}\\nabla _{\\theta }\\pi_{\\theta }(a_t|s_t)\\right ]\\\\\n",
    "& = \\sum_{t=0}^{H-1}\\left[\\sum_{s_t\\in\\mathcal{S}}P(s_t)V^{\\pi }(s_t)\\nabla _{\\theta }\\left(\\sum_{a_t\\in \\mathcal{A}}\\pi_{\\theta }(a_t|s_t)\\right)\\right ]\\\\\n",
    "& = \\sum_{t=0}^{H-1}\\left[\\sum_{s_t\\in\\mathcal{S}}P(s_t)V^{\\pi }(s_t)\\nabla _{\\theta }1\\right ]=0\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb59c94-45e5-4274-913e-05173adb38f5",
   "metadata": {},
   "source": [
    "- 为了降低估计量的方差，b(s)的sub-optimal取值为$b(s)=V^{\\pi}(s)$。\n",
    "  - 将action advantage记为$A_t=Q^{\\pi}(s_t,u_t)-V^{\\pi}(s_t)$\n",
    "  - 因为 $Q^{\\pi}(s_t,u_t) = \\sum_{t'=t}^{H-1}E_{\\pi}[r(s_{t'}, u_{t'})|s_t,u_t]$\n",
    "$$\\begin{align}\n",
    "g & =  E_{\\tau }\\nabla _{\\theta }logP(\\tau ;\\theta )[R(\\tau ) - b]\\\\\n",
    "&= E_{\\tau }\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t|s_t)\\left( \\sum_{t'=t}^{H-1}r(s_{t'}, u_{t'})-b(s_t)\\right)\\right ]\\\\\n",
    "&= E_{\\tau }\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t|s_t)\\left( Q(s_{t}, u_{t})-V(s_t)\\right)\\right ]\\\\\n",
    "&= E_{\\tau }\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t|s_t)A_t\\right ]\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1600a745-a729-4bd2-9ae5-b32a20353b21",
   "metadata": {},
   "source": [
    "- 此时，策略梯度的估计量为：\n",
    "$$\\begin{align}\n",
    "\\hat g & = \\frac{1}{N} \\sum _{i=1 }^N\\nabla _{\\theta }logP(\\tau_i;\\theta )[R(\\tau_i) - b]\\\\\n",
    "&= \\frac{1}{N} \\sum _{i=1 }^N\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t^{(i)}|s_t^{(i)})\\left( \\sum_{t'=t}^{H-1}r(s_{t'}^{(i)}, u_{t'}^{(i)})-V^{\\pi}(s_t^{(i)})\\right)\\right ]\\\\\n",
    "&= \\frac{1}{N} \\sum _{i=1 }^N\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t^{(i)}|s_t^{(i)})\\hat A_t\\right ]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b9dfb0-ca3d-4573-8be7-c77724a2a1e1",
   "metadata": {},
   "source": [
    "- <font color=blue>**不同估计方法的bias-varince tradeoff**</font>：\n",
    "  1. 如果用Monte Carol估$R(\\tau_i) - V(s_t^{(i)})$，得到的估计量unbiased，但是varince很大\n",
    "  2. 改用DNN先估 $V(s_t)或者Q(s_t)$，然后得到Advantage function的估计量的方法能降低variance，但是此时不再满足unbiased条件。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee18dfb1-6eaa-4267-bc07-9c5cf0f1f4ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### I.3 <font color=gray>用state function期望值推导梯度</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21df144-7c48-482c-a44f-06f4c0a309eb",
   "metadata": {},
   "source": [
    "- <font color=orange>这种推导方式的假设比用trajectory的假设多了一条：d(s)不受策略影响。而这一点在现实中通常不成立，所以这种方式描述现实的能力不如用trajectory期望值的推导方式。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06e8aa3-0889-4272-9bef-55aba1d20546",
   "metadata": {},
   "source": [
    "- <font color=red>**假如d(s)不受策略影响**</font>：这个假设通常不成立，但是能简化策略梯度的推导\n",
    "$$\\begin{align}\n",
    "g = \\nabla _{\\theta }J(\\theta) & = \\nabla _{\\theta }E_{s\\sim d(s)}[V^{\\pi_{\\theta }}(s)] \\\\\n",
    "& = \\nabla _{\\theta }\\sum_{s\\in \\mathcal{S} }d(s)V^{\\pi_{\\theta }}(s) \\\\\n",
    "& = \\sum_{s\\in \\mathcal{S} }d(s)\\nabla _{\\theta }V^{\\pi_{\\theta }}(s) \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1931b14-5ae1-487f-8849-67e6475bdd17",
   "metadata": {},
   "source": [
    "- 将状态价值函数的梯度展开可以得到一个线性方程组：\n",
    "$$\\begin{align}\n",
    "\\nabla _{\\theta }V^{\\pi_{\\theta }}(s)\n",
    "& = \\nabla _{\\theta }\\left[ \\sum_{a\\in \\mathcal{A}} \\pi(a|s)Q^{\\pi }(s,a)\\right]\\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}} \\left[ Q^{\\pi }(s,a)\\nabla _{\\theta } \\pi(a|s) + \\pi(a|s)\\nabla _{\\theta } Q^{\\pi }(s,a)\\right]\\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}} \\left[ Q^{\\pi }(s,a)\\nabla _{\\theta } \\pi(a|s) + \\pi(a|s)\\nabla _{\\theta } \\left (r(s,a,s') + \\gamma \\sum_{s'\\in \\mathcal{S}}P(s'|s,a) V^{\\pi }(s')\\right )\\right]\\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}} \\left[ Q^{\\pi }(s,a)\\nabla _{\\theta } \\pi(a|s) + \\pi(a|s) \\gamma \\sum_{s'\\in \\mathcal{S}}P(s'|s,a)\\nabla _{\\theta }V^{\\pi }(s')\\right]\\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}}  Q^{\\pi }(s,a)\\nabla _{\\theta } \\pi(a|s) + \\gamma \\sum_{a\\in \\mathcal{A}}\\pi(a|s) \\sum_{s'\\in \\mathcal{S}}P(s'|s,a)\\nabla _{\\theta }V^{\\pi }(s')\\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}}  Q^{\\pi }(s,a)\\nabla _{\\theta } \\pi(a|s) + \\gamma \\sum_{s'\\in \\mathcal{S}}P^{\\pi}(s'|s)\\nabla _{\\theta }V^{\\pi }(s')\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2651b51d-8829-47ef-a2b8-9e6a8d0527e8",
   "metadata": {},
   "source": [
    "- 求解该线性方程组的解为：<font color=brown>[详见math of RL lemma 9.2]</font>\n",
    "$$\\begin{align}\n",
    "\\nabla _{\\theta }V^{\\pi_{\\theta }}(s)\n",
    "& = \\sum_{s'\\in \\mathcal{S}}Pr^{\\pi}(s'|s)\\sum_{a\\in\\mathcal{A}}\\nabla_{\\theta}\\pi(a|s')Q(s',a)\\\\\n",
    "其中：\\\\\n",
    "Pr^{\\pi}(s'|s) & =\\sum_{k=0}^{\\infty}\\gamma^k[P^{\\pi,k }(s'|s)],\\ P^{\\pi,k }(s'|s)是s经k步转向s'的概率\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ccc57e-0a6c-4680-8e68-e0dcafb5d2c2",
   "metadata": {},
   "source": [
    "- 代入策略梯度式：\n",
    "$$\\begin{align}\n",
    "g & = \\sum_{s\\in \\mathcal{S} }d(s)\\nabla _{\\theta }V^{\\pi_{\\theta }}(s) \\\\\n",
    "& = \\sum_{s\\in \\mathcal{S} }d(s)\\sum_{s'\\in \\mathcal{S}}Pr^{\\pi}(s'|s)\\sum_{a\\in\\mathcal{A}}\\nabla_{\\theta}\\pi(a|s')Q(s',a)\\\\\n",
    "& = \\sum_{s'\\in \\mathcal{S}}\\left(\\sum_{s\\in \\mathcal{S} }d(s)Pr^{\\pi}(s'|s)\\right)\\sum_{a\\in\\mathcal{A}}\\nabla_{\\theta}\\pi(a|s')Q(s',a)\\\\\n",
    "& = \\sum_{s'\\in \\mathcal{S}}\\rho (s')\\sum_{a\\in\\mathcal{A}}\\nabla_{\\theta}\\pi(a|s')Q(s',a)\\ ,\\ 更换符号\\\\\n",
    "& = \\sum_{s\\in \\mathcal{S}}\\rho (s)\\sum_{a\\in\\mathcal{A}}\\nabla_{\\theta}\\pi(a|s)Q(s,a) \\\\\n",
    "& = \\sum_{s\\in \\mathcal{S}}\\rho (s)\\sum_{a\\in\\mathcal{A}}\\pi_{\\theta}(a|s)\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a) \\\\\n",
    "& = \\underset{s\\sim \\rho^{\\pi}, a\\sim \\pi}{E}\\nabla_{\\theta}log\\pi_{\\theta }(a|s)Q^{\\pi }(s,a) \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d073bf-0624-41c4-a2fd-e8723f9c6164",
   "metadata": {},
   "source": [
    "## II. Importance sampling形式的目标和梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde1a98a-f037-40d0-ab53-5c7d62b80771",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### II.1 <font color=gray>基于trajectory的期望rewards的Importance sampling</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99efd692-8138-47e5-a6bd-fec0870a93c6",
   "metadata": {},
   "source": [
    "- <font color=red>这种方式推导出来的式子中有累乘，导致直接用它抽样的话，方差极大。所以这里的推导只做分析参考，不会在实践中使用。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f575f18-9f6b-4ea0-9245-5f81e99bbac8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### II.1.1 目标$J(\\theta)$的Importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2878e3ff-9151-47bb-8f45-e2c858351dc8",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "J (\\theta )& = \\underset{\\tau\\sim \\pi _\\theta}{E} [R(\\tau)] = \\underset{\\tau\\sim \\pi_{\\bar \\theta}}{E} \\ \\  [\\frac{P_{\\theta}(\\tau)}{P_{\\bar \\theta}(\\tau)}R(\\tau)] \\\\\n",
    "其中：\\\\\n",
    "\\frac{P_{\\theta}(\\tau)}{P_{\\bar \\theta}(\\tau)} & = \\frac{P(s_0)\\prod_{t=0}^{H-1}\\pi_{\\theta }(a_t|s_t)P(s_{t+1}|s_t, a_t)}{P(s_0)\\prod_{t=0}^{H-1}\\pi_{\\bar \\theta }(a_t|s_t)P(s_{t+1}|s_t, a_t)} \n",
    "= \\frac{\\prod_{t=0}^{H-1}\\pi_{\\theta }(a_t|s_t)}{\\prod_{t=0}^{H-1}\\pi_{\\bar \\theta }(a_t|s_t)} \n",
    "= \\prod_{t=0}^{H-1}\\frac{\\pi_{\\theta }(a_t|s_t)}{\\pi_{\\bar \\theta }(a_t|s_t)}  \\\\\n",
    "代入上式：\\\\\n",
    "J(\\theta) &= \\underset{\\tau\\sim \\pi_{\\bar \\theta}}{E} \\ \\  [\\frac{P_{\\theta}(\\tau)}{P_{\\bar \\theta}(\\tau)}R(\\tau)]= \\underset{\\tau\\sim \\pi_{\\bar \\theta}}{E} \\ \\  [ \\prod_{t=0}^{H-1}\\frac{\\pi_{\\theta }(a_t|s_t)}{\\pi_{\\bar \\theta }(a_t|s_t)} R(\\tau)]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01a78ee-6a2e-4b48-8109-6ec107716cec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### II.1.2 梯度$\\nabla_\\theta J(\\theta)$的Importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4bd951-d864-4d4e-8be4-274950464dcc",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "\\nabla _{\\theta }J(\\theta) & =  \\underset{\\tau\\sim \\pi_{\\theta}}{E} \\ \\  \\nabla _{\\theta }logP(\\tau ;\\theta )R(\\tau ) \\\\\n",
    "& = \\underset{\\tau\\sim \\pi_{\\bar \\theta}}{E} \\ \\  \\frac{P_{\\theta}(\\tau)}{P_{\\bar \\theta}(\\tau)}\\nabla _{\\theta }logP_{\\theta}(\\tau)R(\\tau )  \\\\\n",
    "& = \\underset{\\tau\\sim \\pi_{\\bar \\theta}}{E} \\ \\  \\frac{P_{\\theta}(\\tau)}{P_{\\bar \\theta}(\\tau)}\\nabla _{\\theta }log\\pi _{\\theta}(\\tau)R(\\tau )  \\\\\n",
    "&= \\underset{\\tau\\sim \\pi_{\\bar \\theta}}{E} \\ \\  \\left [\\left( \\prod_{t=0}^{H-1}\\frac{\\pi_{\\theta,t }}{\\pi_{\\bar \\theta, t }} \\right)\\left(\\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta,t }\\right)\\left(\\sum_{t=0}^{H-1}r_t\\right)\\right]\\\\\n",
    "&= \\underset{\\tau\\sim \\pi_{\\bar \\theta}}{E} \\ \\  \\left [\\left( \\prod_{t^{''}=0}^{t}\\frac{\\pi_{\\theta,t^{''} }}{\\pi_{\\bar \\theta,t^{''} }} \\right)\\left( \\prod_{t^{'''}={t+1\n",
    "}}^{H-1}\\frac{\\pi_{\\theta, t^{'''} }}{\\pi_{\\bar \\theta, t^{'''} }} \\right)\\left(\\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta,t }\\sum_{t'=t}^{H-1}r_t\\right)\\right]\\\\\n",
    "&= \\underset{\\tau\\sim \\pi_{\\bar \\theta}}{E} \\ \\  \\left [\\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta,t }(\\prod_{t^{''}=0}^{t}\\frac{\\pi_{\\theta ,t^{''} }}{\\pi_{\\bar \\theta ,t^{''}}})(\\sum_{t'=t}^{H-1}r_t)\\right]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa0287-ea54-4a24-9aad-71a224d774a6",
   "metadata": {},
   "source": [
    "- 实践中不用这个式子，因为上式中：\n",
    "$$\\prod_{t^{''}=0}^{t}\\frac{\\pi_{\\theta ,t^{''} }}{\\pi_{\\bar \\theta ,t^{''}}} \\rightarrow exponential\\ in\\  T，方差极大$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90d93c9-29ed-4f83-9dd0-e718d3c00941",
   "metadata": {},
   "source": [
    "### II.2 IS梯度的近似和它对应的surrogate loss目标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfa1df3-d7c5-458c-8f1e-88b06865c4ca",
   "metadata": {},
   "source": [
    "- <font color=blue>Natural PG、TRPO和PPO的梯度形式和这里用importance sampling推导的近似表达式非常像。但要注意：</font>\n",
    "  1. Natural PG推导它的方式并不是从importance sampling的角度。<font color=brown>[详见Natural PG部分的notes]</font>\n",
    "  2. 在原Natural PG论文中没有使用IS形态的表达式，在TRPO论文中使用了类似IS的表述形式，但仅仅是为了更清晰地描述新旧策略在算法中的角色。<font color=green>**并没有真的使用IS方法抽样数据。**</font><font color=brown>[具体可参考PPO论文part 5公式(13)-公式(14)]</font>\n",
    "  3. 这里就是用原梯度的importance sampling形式推导该近似表达式，实际得到的表达式和上述算法中实际使用的表达式有一点差异。<font color=red>这里每个trajectory对应的效用$Q(s_t,a_t)$是新策略条件下的效用，而上述算法中使用的梯度表达式中，trajectory的效用是旧策略条件下得到的。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa92c13f-0ba2-4e62-9c53-28313fd1e281",
   "metadata": {},
   "source": [
    "#### II.2.1 用state-action的的边际分布得到Importance sampling梯度的近似"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52bdaf6-7ce0-4641-935e-83945b1d62c3",
   "metadata": {},
   "source": [
    "- 另一种推导方式：假设$(s_t, a_t)\\sim P_{\\theta}(s_t, a_t)$是state-action的marginal distribution。\n",
    "- 这种形式的优点是没有累乘，所以没有前一种推导方式中方差太大的问题。代价是得到的是策略梯度的近似估计量。实践中可用的原因是，尽管是近似估计量，但它导致的估计误差有明确的上限。<font color=brown>[详见后文]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95229604-c62c-4a96-aa6d-db942fef0c03",
   "metadata": {},
   "source": [
    "- 推导式：\n",
    "$$\\begin{align}\n",
    "g & = \\underset{\\tau \\sim \\pi_{\\theta }}{E} \\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(a_t|s_t)\\sum_{t'=t}^{H-1}r(s_{t'}, a_{t'})\\right ]\\\\\n",
    " & = \\underset{\\tau \\sim \\pi_{\\theta }}{E} \\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(a_t|s_t)Q^{\\pi_{\\theta }}(s_t,a_t)\\right ]\\\\\n",
    "& = \\sum_{t=0}^{H-1}\\underset {(a_t, s_t)\\sim P_{\\theta }(a_t, s_t)}{E} \\nabla _{\\theta }log\\pi_{\\theta }(a_t|s_t)Q^{\\pi_{\\theta }}(s_t,a_t)\\\\\n",
    "& = \\sum_{t=0}^{H-1}\\sum_{a_t\\in\\mathcal{A},s_t\\in\\mathcal{S}} P_{\\theta }(a_t,s_t)\\nabla _{\\theta }log\\pi_{\\theta }(a_t|s_t)Q^{\\pi_{\\theta }}(s_t,a_t)\\\\\n",
    "& = \\sum_{t=0}^{H-1}\\sum_{a_t\\in\\mathcal{A},s_t\\in\\mathcal{S}} P_{\\bar \\theta }(a_t,s_t)\\frac{P_{\\theta }(a_t,s_t)}{P_{\\bar \\theta }(a_t,s_t)}\\nabla _{\\theta }log\\pi_{\\theta }(a_t|s_t)Q^{\\pi_{\\theta }}(s_t,a_t)\\\\\n",
    "& = \\sum_{t=0}^{H-1}\\underset {(a_t, s_t)\\sim P_{\\bar \\theta }(a_t, s_t)}{E}\\frac{P_{\\theta }(a_t,s_t)}{P_{\\bar \\theta }(a_t,s_t)}\\nabla _{\\theta }log\\pi_{\\theta }(a_t|s_t)Q^{\\pi_{\\theta }}(s_t,a_t) \\\\\n",
    "& = \\sum_{t=0}^{H-1}\\underset {(a_t, s_t)\\sim P_{\\bar \\theta }(a_t, s_t)}{E}\\frac{P_{\\theta }(s_t)\\pi_{\\theta }(a_t|s_t)}{P_{\\bar \\theta }(s_t)\\pi_{\\bar\\theta }(a_t|s_t)}\\nabla _{\\theta }log\\pi_{\\theta }(a_t|s_t)Q^{\\pi_{\\theta }}(s_t,a_t) \\\\\n",
    "& \\approx \\sum_{t=0}^{H-1}\\underset {(a_t, s_t)\\sim P_{\\bar \\theta }(a_t, s_t)}{E}\\frac{\\pi_{\\theta }(a_t|s_t)}{\\pi_{\\bar\\theta }(a_t|s_t)}\\nabla _{\\theta }log\\pi_{\\theta }(a_t|s_t)Q^{\\pi_{\\theta }}(s_t,a_t) \\\\\n",
    "& = \\underset {\\tau \\sim \\pi_{\\bar\\theta }}{E}\\sum_{t=0}^{H-1}\\frac{\\pi_{\\theta }(a_t|s_t)}{\\pi_{\\bar\\theta }(a_t|s_t)}\\nabla _{\\theta }log\\pi_{\\theta }(a_t|s_t)\\sum_{t'=t}^{H-1}r(s_{t'}, a_{t'})\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b569bf0c-544d-4c49-bd39-f39f04a1d4ea",
   "metadata": {},
   "source": [
    "- 上面推导过程的说明：\n",
    "  - 第二个等式成立是因为：将$\\sum_{t'=t}^{H-1}r(s_{t'}, a_{t'})$替换为$Q^{\\pi_{\\theta }}(s_t,a_t)$在trajectory的期望条件下是成立的。\n",
    "  - 第三个等式成立是因为：$\\nabla _{\\theta }log\\pi_{\\theta }(a_t|s_t)Q^{\\pi_{\\theta }}(s_t,a_t)$ 是$(s_t,a_t)$为变量的函数。\n",
    "    1. $\\pi_{\\theta }(a_t|s_t)$ 是$(s_t,a_t)$为变量的函数\n",
    "    2. $Q^{\\pi_{\\theta }}(s_t,a_t)$的展开式中虽然有$t'>t$的所有time-steps的影响，但是所有的$a_{t'},s_{t'}$的不确定性都被期望积掉了。\n",
    "  - 约等式中直接去掉了$\\frac{P_{\\theta}(s_t)}{P_{\\bar \\theta}(s_t)}$，这是导致最终近似估计式简化的原因，也是导致估计量近似而不是等式的原因。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6f54d6-409c-41b7-ab1d-882a2a42d490",
   "metadata": {},
   "source": [
    "#### II.2.2 surrogate loss目标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e99c403-e8e0-4934-8204-770c3c811114",
   "metadata": {},
   "source": [
    "- 取梯度为IS梯度近似表达式时，为了利用现成的深度学习框架，可以用它反推一个surrogate loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6ed315-c3de-4919-bb5d-ccf3bb342d30",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "当梯度取下式时：\\\\\n",
    "g & = \\underset {\\tau \\sim \\pi_{\\bar\\theta }}{E}\\sum_{t=0}^{H-1}\\frac{\\pi_{\\theta }(a_t|s_t)}{\\pi_{\\bar\\theta }(a_t|s_t)}\\nabla _{\\theta }log\\pi_{\\theta }(a_t|s_t)\\sum_{t'=t}^{H-1}r(s_{t'}, a_{t'})\\\\\n",
    "对应的目标可以用：\\\\\n",
    "L^{IS}(\\theta ) & = \\underset {\\tau \\sim \\pi_{\\bar\\theta }}{E}\\sum_{t=0}^{H-1}\\frac{\\pi_{\\theta }(a_t|s_t)}{\\pi_{\\bar\\theta }(a_t|s_t)}\\sum_{t'=t}^{H-1}r(s_{t'}, a_{t'})\\\\\n",
    "对L(\\theta)求梯度很容易得到：\\\\\n",
    "& \\nabla_{\\theta }L^{IS}(\\theta ) = g\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc80bb5-a0da-45fe-9204-aa8fae059fcc",
   "metadata": {},
   "source": [
    "- 用梯度反推得到的surrogate loss可以从importance sampling角度来理解其含义：\n",
    "  - 严格地说，这里的surrogate loss不算真正的importance sampling loss，因为trajectory的rewards如果要取importance sampling的期望值的话，应该用$(s_t, a_t)$的边际分布来计算importance weight，但这里用的是$\\pi(a_t|s_t)$，此外还有r采样在实践中应该来自旧策略，而IS中的推导是新策略这点差异。\n",
    "  - 但一般会将这个loss用importance sampling来标记，这是因为它是真正的importance sampling loss的近似值，并且很多地方都没有区分r采样的所使用的策略差异，直接把IS中的reward改成旧策略的采样，然后标记为IS loss。\n",
    "$$\\begin{align}\n",
    "L^{IS}(\\theta ) & = \\underset {\\tau \\sim \\pi_{\\bar\\theta }}{E}\\sum_{t=0}^{H-1}\\frac{\\pi_{\\theta }(a_t|s_t)}{\\pi_{\\bar\\theta }(a_t|s_t)}\\sum_{t'=t}^{H-1}r(s_{t'}, a_{t'})\\\\\n",
    "& = \\sum_{t=0}^{H-1}E_{s_t\\sim P_{\\bar \\theta }(s_t)}E_{a_t\\sim {\\pi_{\\bar \\theta }}}\\frac{\\pi_{\\theta }(a_t|s_t)}{\\pi_{\\bar\\theta }(a_t|s_t)}\\sum_{t'=t}^{H-1}r(s_{t'}, a_{t'})\\\\\n",
    "PPO等算法实际上用的是：\\\\\n",
    "& = \\sum_{t=0}^{H-1}E_{s_t\\sim P_{\\bar \\theta }(s_t)}E_{a_t\\sim {\\pi_{\\bar \\theta }}}\\frac{\\pi_{\\theta }(a_t|s_t)}{\\pi_{\\bar\\theta }(a_t|s_t)}{\\color{red}{Q^{\\pi_{\\bar \\theta}}}}(s_t,a_t)\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022cd89a-8b89-471b-b30e-2f22a2946a02",
   "metadata": {},
   "source": [
    "## III. Importance Sampling要点 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0635328f-9193-4931-88e7-0209248fdfcc",
   "metadata": {},
   "source": [
    "- 思路：如果没办法直接从 $p(x)$ 中采样数据，可以从 $q(x)$ 中采样数据，$x^i\\sim q(x)$\n",
    "$$\n",
    "\\begin{split}\n",
    "E_{x\\sim p}[f(x)]&=\\int f(x)p(x)dx=\\int f(x)\\underbrace{\\frac{p(x)}{q(x)}}_{\\text{importance weight}}q(x)dx\\\\\n",
    "&=E_{x\\sim q}[f(x)\\frac{p(x)}{q(x)}]\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a54918b-f305-4a26-b9ec-ef799fbe8417",
   "metadata": {},
   "source": [
    "- 期望相同\n",
    "$$E_{x\\sim p}[f(x)]=E_{x\\sim q}[f(x)\\frac{p(x)}{q(x)}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab85bde7-a9a4-42d2-ad35-459a9cf8cc45",
   "metadata": {},
   "source": [
    "- 方差不同： q的分布与p的分布越接近，方差越小。如果两个分布差异很大，则估计的结果方差大，也就是说大多数时候估计得到的结果偏差很大。\n",
    "$$\\begin{split}\n",
    "    \\text{Var}_{x\\sim q}[f(x)\\frac{p(x)}{q(x)}]&=E_{x\\sim q}[(f(x)\\frac{p(x)}{q(x)})^2]-(E_{x\\sim q}[f(x)\\frac{p(x)}{q(x)}])^2\\\\\n",
    "    &=E_{x\\sim p}[f^2(x)\\frac{p(x)}{q(x)}] - (E_{x\\sim p}[f(x)])^2\n",
    "\\end{split}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
