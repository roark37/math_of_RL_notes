{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c2c4393-c05a-41b2-8f0c-e61698899001",
   "metadata": {},
   "source": [
    "# TRPO and PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a1b52a-361e-4fe4-874e-5c2d9d1a71e8",
   "metadata": {},
   "source": [
    "- <font color=blue>**从Natural PG到TRPO，再到PPO的solution**</font>：\n",
    "  1. vanilla policy gradient使用的迭代式训练的效果非常不稳定，\n",
    "  2. Natrual PG从policy iteration出发，推导出了带KL divergence约束的<font color=blue>**Importance Sampling形态的**</font>Policy Gradient优化目标，但是求解的方法相对复杂。\n",
    "  3. TRPO使用共轭梯度简化了Natural PG的优化目标。\n",
    "  4. 而PPO为了充分利用深度学习现成的工具，从梯度迭代式反过来思考。找到了与迭代式对应的surrogate loss来构建优化问题。并在此基础上进一步优化了step size的设计思路。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e50555-5188-462e-9e0d-e010b433448b",
   "metadata": {},
   "source": [
    "- <font color=brown>[notes of UCB RL bootcamp(2021) lec5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11227e24-235a-45d9-b115-a483b8242916",
   "metadata": {},
   "source": [
    "## I. TRPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3969ddd-d95e-4e9f-92dd-cf7c3e87afa9",
   "metadata": {},
   "source": [
    "### I.1 出发点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad08ad6c-ee93-4874-93e2-97a0c4348d9c",
   "metadata": {},
   "source": [
    "#### I.1.1 背景知识：RL和DL样本分布的差异"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a4cbab-27ce-40e1-9a61-9002c23a7456",
   "metadata": {},
   "source": [
    "- 一般的DL中样本是iid的，也就是说通常可以假设样本都是来自同一个分布。但是这个假设在RL中不成立。\n",
    "- RL中有两个因素导致iid不成立：\n",
    "  1. 如果策略稳定，并且状态s的分布达到了稳定的d(s)。按照episodes来收集数据集，每个episode(trajectory)上，连续的time-steps中，前一步的状态必然影响后一步的状态，所以每个step上收集到的数据是相关的。\n",
    "  2. 在算法迭代过程中，策略在不断更新，所以状态s是non-stationary distribution，基本不会处于稳定的d(s)上。也就是说，收集到的数据并不是来自相同的分布。这里还有一个因素让策略的更新过程并不smooth，也就是一次迭代产生的两个策略之间差别可能很大：\n",
    "     - 算法迭代的参数是$\\pi(\\theta)中的\\theta$，<font color=orange>NN中可以控制步幅让DL中的参数迭代量很小，这只能让参数的变化幅度小，但是从结果来看，策略本身的变化却可能很大。</font>\n",
    "- 上述RL中数据分布的相关性和不稳定性会让RL算法迭代过程中reward波动很大。常见的现象是，迭代过程中很长时间reward都很小，然后过了很久才突然有一个很大的提升。它不会像supervised DL中，指标会逐步稳定上升。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43137d38-498f-47e5-b07e-8f95c677f832",
   "metadata": {},
   "source": [
    "#### I.1.2 出发点1：AC算法的两大问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2dafbe-54aa-4739-8120-aae413f9df40",
   "metadata": {},
   "source": [
    "  1. 很难选择stepsize：这个问题在RL中比在深度学习中带来的后果更糟糕。\n",
    "     - 由于数据分布不稳定，很难像DL中那样找到一个大小合适的stepsize，并且在整个训练过程中几乎不用太调整这个stepsize。在DL中，可以用normalization的方法来稳定让网络中的变量分布，然后用更好的优化器（如Adam）来拟合二阶优化，达到控制参数不同维度的步幅效果。这样就能让迭代过程稳定。但如前文分析，即使这样做也只能稳定网络参数的变化，却无法稳定策略本身，所以需要有办法稳定策略本身的变化。\n",
    "     - 而且错误的stepsize带来的后果也更糟糕。如果stepsize略大，policy gradient得到的策略可能是一个非常差的策略。和DL中不一样，此时新的数据就会是用这个糟糕的策略生成的，这会进入无法恢复的恶性循环。\n",
    "  2. sample efficiency很低，同样因为是on-policy算法，一个batch的trajectory用一次就扔掉。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72efb692-977a-4ecf-88fd-63a834a72506",
   "metadata": {},
   "source": [
    "- **TRPO提出的solution**：\n",
    "  1. 用trust region来减缓stepsize的问题。trust region不是控制参数$\\theta$的变化幅度，而是直接控制一次迭代导致的策略变化差异。这就针对性的解决了前文中提到的策略波动导致的问题。\n",
    "  2. 更有效地使用样本，尽管仍然是on-policy algorithm，仍然用一次就扔掉，但样本使用效率提高了。\n",
    "     - trust region让得到的糟糕样本更少"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2921a9e-3086-4620-bf99-7198c7ba49d0",
   "metadata": {},
   "source": [
    "#### I.1.3 出发点2：参考DL，将RL处理成优化问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaddb13-a47b-4415-8658-92a27a91009d",
   "metadata": {},
   "source": [
    "- 深度学习中，learning问题被转化为numerical optimization problem。\n",
    "  - 典型例子是：Supervised learning的优化目标就是minimize training error\n",
    "- 强化学习的目标是：如何利用所有搜集到的data来找到最优策略？\n",
    "  - <font color=blue>**Q-learning** </font>：通过off-policy做到了**能够利用所有截至目前所有搜集到的data**，但是优化的目标是Bellman Error，不是真正的目标：best policy\n",
    "  - <font color=blue>**vanilla policy gradient**</font>：用了stochastic gradient，但是并没有将问题转化为优化问题。<font color=orange>[rk's note]: </font>PG的目标是$max U(\\theta)$，用TD方法来求解，理论上这也是优化问题。但是实际求解过程中，经过rewards to go和增加baseline等操作，最终得到的算法已经没有一个对应的优化目标了。而John Schulman这里想做的，是像DL中一样，以一个loss function为目标，通过直接求目标梯度来求解最优策略。<font color=green>**这还有一个非常好的优点，就是所有的DL framework等工具就能直接用在RL算法上了。**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5604d09b-3055-4b6f-899b-1a51621f2653",
   "metadata": {},
   "source": [
    "### I.2 surrogate loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b5bb5f-f5c7-4198-b13b-e32e364f7864",
   "metadata": {},
   "source": [
    "#### I.2.1 以vanilla PG为迭代式的loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aa7b32-2d42-4ed1-ba44-51ef5b438d3d",
   "metadata": {},
   "source": [
    "- PG迭代式：\n",
    "$$\\hat g = \\hat E_t \\left[ \\nabla_{\\theta}log\\pi_{\\theta} (a_t|s_t)\\hat A_t \\right]$$\n",
    "  - <font color=orange>E表示多个time-steps的均值的估计</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dacca9-784a-467b-9d0b-b9eb066165ec",
   "metadata": {},
   "source": [
    "- 直接与之对应的loss形式：\n",
    "$$max L^{PG} (\\theta)= \\hat E_t \\left[ log\\pi_{\\theta} (a_t|s_t)\\hat A_t \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f179f46-5115-4ab6-84b2-50be4719256e",
   "metadata": {},
   "source": [
    "- 这个loss形式实践中不会直接使用，因为：\n",
    "  - 当$\\hat A_t>0$时，用这个式子为目标会让$\\pi(a_t|s_t)$增加，往1走；反之，当$\\hat A_t<0$时，用这个式子为目标会让$\\pi(a_t|s_t)$减少，往0走。\n",
    "  - 但实际迭代过程中有一个问题，$\\hat A_t$的估计是非常noisy的。当noisy导致advantage的符号发生频繁改变，此时梯度的variance很大，基于advantage的估计量调整的策略会发生radically change。\n",
    "  - 如果要用的话，一定要很大的batch size，但调参特别是调learning rate会很难，用adaptive step size的优化器比如adam，普通优化器比如SGD更难work。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8466bdca-f75d-4d26-a6ba-380ee9148b88",
   "metadata": {},
   "source": [
    "#### I.2.2 另一种仍以policy gradient为迭代式的loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78f18b1-3b3c-4ba9-81c8-5e77e5ceb17b",
   "metadata": {},
   "source": [
    "- <font color=green>**思路：先将梯度转换成一种等价的梯度形式，然后再用该等价的梯度形式构造loss函数。**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53444c73-7a55-488d-831d-7be62459ce6e",
   "metadata": {},
   "source": [
    "- 两种等价的梯度形式：\n",
    "$$\\nabla_{\\theta}log f(\\theta)|_{\\theta=\\bar \\theta}=\\frac{\\nabla_{\\theta}f(\\theta)|_{\\theta=\\bar \\theta}}{f(\\bar \\theta)} = \\nabla_{\\theta}\\left(\\frac{f(\\theta)}{f(\\bar \\theta)}\\right)|_{\\theta=\\bar \\theta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4feba7b-58de-4a1c-9b01-7ee86030c27b",
   "metadata": {},
   "source": [
    "- 从上式可知，policy gradient的迭代式可以转换为等价的梯度迭代式："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf96bfb5-1db7-4522-b808-c24120590435",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "\\hat g & = \\hat E_t \\left[ \\nabla_{\\theta}log\\pi_{\\theta} (a_t|s_t)\\hat A_t \\right] \\\\\n",
    "& = \\sum_{t=0}^{H-1}\\left( \\nabla_{\\theta }log\\pi_{\\theta}(a_t|s_t)\\hat A_t |{\\color{Red} {\\theta_t}}  \\right )\\\\\n",
    "& = \\sum_{t=0}^{H-1}\\left( \\frac{\\nabla_{\\theta }\\pi_{\\theta}(a_t|s_t) |{\\color{Red} {\\theta_t} }}{\\pi_{{\\color{Red} {\\theta_t} }}(a_t|s_t)}{\\color{Red} {\\hat A_t}}  \\right )\\\\\n",
    "&  = \\hat E_t \\left[ \\frac{\\nabla_{\\theta }\\pi_{\\theta}(a_t|s_t) |{\\color{Red} {\\theta_t}} }{\\pi_{{\\color{Red} {\\theta_t} }}(a_t|s_t)}{\\color{Red} {\\hat A_t}}  \\right] \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c326696-aa24-40c2-be86-d659dc349c64",
   "metadata": {},
   "source": [
    "- 上式对应的loss形态：\n",
    "$$L(\\theta )= \\hat E_t \\left[ \\frac{\\pi_{\\theta}(a_t|s_t)  }{\\pi_{\\color{Red} {\\theta_t} }(a_t|s_t)}{\\color{Red} {\\hat A_t}}  \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7933ce-5a1c-4a6e-b2e5-85132411d2a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### I.2.3 importance sampling的要点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14decbb-98fe-4a9b-b0c1-55bae079996b",
   "metadata": {},
   "source": [
    "- 思路：如果没办法直接从 $p(x)$ 中采样数据，可以从 $q(x)$ 中采样数据，$x^i\\sim q(x)$\n",
    "$$\n",
    "\\begin{split}\n",
    "E_{x\\sim p}[f(x)]&=\\int f(x)p(x)dx=\\int f(x)\\underbrace{\\frac{p(x)}{q(x)}}_{\\text{importance weight}}q(x)dx\\\\\n",
    "&=E_{x\\sim q}[f(x)\\frac{p(x)}{q(x)}]\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f747b3-d85c-4b08-96e8-6ed97464c23f",
   "metadata": {},
   "source": [
    "- 期望相同\n",
    "$$E_{x\\sim p}[f(x)]=E_{x\\sim q}[f(x)\\frac{p(x)}{q(x)}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eefe78-3925-4c17-82b9-1af22e52ea99",
   "metadata": {},
   "source": [
    "- 方差不同： q的分布与p的分布越接近，方差越小。如果两个分布差异很大，则估计的结果方差大，也就是说大多数时候估计得到的结果偏差很大。\n",
    "$$\\begin{split}\n",
    "    \\text{Var}_{x\\sim q}[f(x)\\frac{p(x)}{q(x)}]&=E_{x\\sim q}[(f(x)\\frac{p(x)}{q(x)})^2]-(E_{x\\sim q}[f(x)\\frac{p(x)}{q(x)}])^2\\\\\n",
    "    &=E_{x\\sim p}[f^2(x)\\frac{p(x)}{q(x)}] - (E_{x\\sim p}[f(x)])^2\n",
    "\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e49378-98e8-4ced-9537-230242b24c47",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "- 在梯度上使用importance sampling的例子：\n",
    "$$\\begin{align}\n",
    "\\nabla U(\\theta )& = \\nabla \\underset{\\tau\\sim \\pi _\\theta}{E} [R(\\tau)]\\\\\n",
    "& = \\nabla \\underset{\\tau\\sim \\pi_{\\theta_{old}}}{E} \\ \\  [\\frac{P_{\\theta}(\\tau)}{P_{\\theta_{old}}(\\tau)}R(\\tau)]\\\\\n",
    "& =  \\underset{\\tau\\sim \\pi_{\\theta_{old}}}{E} [\\frac{\\nabla P_{\\theta}(\\tau)}{P_{\\theta_{old}}(\\tau)}R(\\tau)]\\\\\n",
    "\\end{align}$$\n",
    "  - <font color=red>但这样直接用importance sampling的话，梯度展开难以求解。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c234100b-576e-464f-8184-6424edf5cbcf",
   "metadata": {},
   "source": [
    "#### I.2.3 理解surrogate loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bcc0c9-0c99-40a3-8aa0-a6061b40b468",
   "metadata": {},
   "source": [
    "$$L^{IS}(\\theta )= \\hat E_t \\left[ \\frac{\\pi_{\\theta}(a_t|s_t)  }{\\pi_{{\\color{Red} {\\theta_t}} }(a_t|s_t)}{\\color{Red} {\\hat A_t}}  \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9264bfb9-9eca-4f5e-9460-de5af52d09e5",
   "metadata": {},
   "source": [
    "- <font color=norange>**从importance sampling角度理解这个loss形态：**</font>\n",
    "  - 所有作为样本点的state-action pair都是用$\\pi_{\\theta_t}$策略获得的，advantage也是用这些data估计的。\n",
    "  - 因此，从IS的角度来看，这个loss是用 $\\pi_{\\theta_t}$ 时的策略分布在估计Advantage在策略为$\\pi(\\theta)$时的均值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f44b1bd-0d9b-41b7-abee-e036286e4be1",
   "metadata": {},
   "source": [
    "- Importance sampling角度重新拆解该loss："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdf46b4-c512-4f83-b788-71873df9ee5e",
   "metadata": {},
   "source": [
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520b6b68-d406-4d25-af01-b317df1f6b63",
   "metadata": {},
   "source": [
    "### I.3 TRPO算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304d8a02-b291-4327-a35e-d9e9a945665f",
   "metadata": {},
   "source": [
    "- surrogate loss + stepsize constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219654f5-9205-4272-87b0-18fdb5284de0",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "& \\underset{\\theta }{max} \\ \\hat E_t \\left[ \\frac{\\pi_{\\theta}(a_t|s_t)  }{\\pi_{ \\theta_{old}}(a_t|s_t)} \\hat A_t  \\right] \\\\\n",
    "& s.t. \\ \\hat E_t [KL(\\pi_{\\theta_{old}}(·|s_t) |\\pi_{\\theta}(·|s_t) )] \\le \\delta \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bc4f18-0903-4d8c-b5dc-d8e177a151b0",
   "metadata": {},
   "source": [
    "- 求解优化问题的方法：\n",
    "  - 要用到conjugate gradient，其中涉及Hessian matrix的逆，复杂度较高。[略]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c7c54f-23e7-4010-a0f4-007e437a41c8",
   "metadata": {},
   "source": [
    "- 算法伪码：<font color=brown>[ref: spinning up]</font>\n",
    "  - <img src='pics/trpo.png' width='70%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5232992a-4472-429c-8536-b1df6954389b",
   "metadata": {},
   "source": [
    "## II. PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd6892b-3e23-49c6-8163-c5e3f18e5b70",
   "metadata": {},
   "source": [
    "### II.1 出发点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518fe08f-10c0-42bc-be57-cb8750962b85",
   "metadata": {},
   "source": [
    "- TRPO算法仍然存在的问题：\n",
    "  1. trust region constraints很难直接融入neural network的设计\n",
    "  2. 为了计算二阶梯度条件而使用的conjugate gradient计算起来很复杂\n",
    "  - John Schulman当时想找到一种不需要二阶梯度的方法来求解TRPO中的优化问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee64e45-439c-4112-86e4-bae6d508810e",
   "metadata": {},
   "source": [
    "- PPO的solution：将TRPO中的优化问题转化为等价的没有constraint的优化问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2c5fc4-bc22-41be-b610-e48b40abbc66",
   "metadata": {},
   "source": [
    "### II.2 目标函数形式优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72308a8b-59b1-4ab6-b84f-9d0e9b6caf91",
   "metadata": {},
   "source": [
    "#### II.2.1 有约束优化问题直接转无约束优化问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d687e3-d2c0-48fd-b8d5-4da7612d560b",
   "metadata": {},
   "source": [
    "- 将TRPO中的有约束的优化问题转化为无约束优化问题：\n",
    "$$\\begin{align}\n",
    "& \\underset{\\theta }{max} \\ \\hat E_t \\left[ \\frac{\\pi_{\\theta}(a_t|s_t)  }{\\pi_{ \\theta_{old}}(a_t|s_t)} \\hat A_t  \\right] \\\\\n",
    "& s.t. \\ \\hat E_t [KL(\\pi_{\\theta_{old}}(·|s_t) |\\pi_{\\theta}(·|s_t) )] \\le \\delta  \\\\\n",
    "等价问题：\\\\\n",
    "& \\underset{\\theta }{max} \\ \\hat E_t \\left[ \\frac{\\pi_{\\theta}(a_t|s_t)  }{\\pi_{ \\theta_{old}}(a_t|s_t)} \\hat A_t - \\beta KL(\\pi_{\\theta_{old}}(·|s_t) |\\pi_{\\theta}(·|s_t) )\\right]  \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dc239b-bed5-434f-8772-61d56d44c181",
   "metadata": {},
   "source": [
    "- 带来新的问题：\n",
    "  1. 虽然达到了转为无约束优化问题的目标，现在可以直接使用DL的方法解RL问题。但$\\beta$很难取到合适的值。\n",
    "  2. 这个形式下，无法达到TRPO迭代中的一个优势：monotonic improvement。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a115fd-1c97-4bc0-989a-5d329c5c0b3c",
   "metadata": {},
   "source": [
    "#### II.2.2 将优化问题改为Clip形式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d019c630-45f1-4ca8-b6e3-d07d3219381f",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "r_t(\\theta ) & = \\frac{\\pi_{\\theta}(a_t|s_t)  }{\\pi_{ \\theta_{old}}(a_t|s_t)} \\\\\n",
    "L^{CLIP}(\\theta ) & = \\hat E_t \\left[ min[r_t(\\theta )\\hat A_t, clip(r_t(\\theta ), 1-\\epsilon, 1+\\epsilon )\\hat A_t]\\right] \\\\ \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5198983f-e871-4785-b747-17aa0ea716f7",
   "metadata": {},
   "source": [
    "- 一种更好理解的写法：\n",
    "$$\\begin{align}\n",
    "& L^{CLIP}(\\theta )  = \\hat E_t \\left[ min[r_t(\\theta )\\hat A_t, g(\\epsilon ,\\hat A_t)]\\right] \\\\\n",
    "& g(\\epsilon ,\\hat A_t) = \\left\\{\\begin{matrix}\n",
    " (1-\\epsilon ) \\hat A_t& A\\ge 0\\\\\n",
    " (1+\\epsilon ) \\hat A_t & A\\le 0\n",
    "\\end{matrix}\\right. , \\ \\ \\ r_t(\\theta ) = \\frac{\\pi_{\\theta}(a_t|s_t)  }{\\pi_{ \\theta_{old}}(a_t|s_t)} \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8441d165-2f2c-4a74-b410-c1fa1974826e",
   "metadata": {},
   "source": [
    "#### II.2.3 Clip目标函数形式的理解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86533f45-e31b-4c19-a76f-7f98f648a7cc",
   "metadata": {},
   "source": [
    "1. $clip(r_t(\\theta ), 1-\\epsilon, 1+\\epsilon )\\hat A_t$的作用：当新旧策略一模一样的时候，r值为1，而clip截断新旧策略分布的比例，使它保持在$(1-\\epsilon, 1+\\epsilon)$范围内。实际上就是让新策略相对旧策略变化不要太大。\n",
    "   - 实现方式：如果$A>0，r>1+\\epsilon$，此时clip part取值为$(1+\\epsilon)A$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7448070f-7d3c-456d-aea8-8c9cbce0c735",
   "metadata": {},
   "source": [
    "？？？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fe6bd7-1cee-443a-a10b-caa8d83c21c3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc87b028-202e-440a-9873-d19997660ac8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e240a62-b55a-47e0-829e-acfad08e34ac",
   "metadata": {},
   "source": [
    "### II.3 算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e60d57-d35f-4a02-96a4-b7139a4382d3",
   "metadata": {},
   "source": [
    "- 算法伪码：<font color=brown>[ref: spinning up]</font>\n",
    "  - <img src='pics/ppo.png' width='70%'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
