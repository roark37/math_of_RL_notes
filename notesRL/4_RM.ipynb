{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c29d61d5",
   "metadata": {},
   "source": [
    "# Robins-Monro algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a268d9e-c4a2-4594-9753-bcf8677d2047",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Motivating example: Mean estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b259d6-feb5-4c27-8b58-bafefaf202ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 1. 两种空间复杂度$O(1)$的均值计算方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbdc137-d4aa-4fbb-aba9-f16e4b659075",
   "metadata": {},
   "source": [
    "- 目标：计算$\\frac{1}{n}\\sum_{i=1}^nx_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bf9e66-4cc9-40d8-aea5-9c80f382306c",
   "metadata": {},
   "source": [
    "- 方法一：\n",
    "$$\\begin{align}\n",
    "init: & k = 0 \\\\\n",
    "iterate: &\\\\\n",
    "& acc = acc + x_k \\\\\n",
    "& k = k + 1 \\\\\n",
    "& avg = acc / k\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db92aeb-a252-4375-b732-d978f651edfd",
   "metadata": {},
   "source": [
    "- 方法二：\n",
    "$$\\begin{align}\n",
    "init: & k = 0, avg=x_0 \\\\\n",
    "iterate: &\\\\\n",
    "& k = k + 1 \\\\\n",
    "& avg = avg + \\frac{1}{k}(x_{k-1}-avg)  \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c831bc61-a556-4e50-89fa-0d94e7a2f0e8",
   "metadata": {},
   "source": [
    "- 方法二可以写成另一种等价形式：[为了与math of RL书上的表述一致]\n",
    "$$\\begin{align}\n",
    "init: & k = 1, w_1=x_1 \\\\\n",
    "iterate: &\\\\\n",
    "& w_{k+1} = w_k - \\frac{1}{k}(w_k-x_{k})  \\\\\n",
    "& k = k + 1 \\\\\n",
    "\\end{align}$$\n",
    "  - 迭代过程：\n",
    "    - <img src='pics/mean_accumulation.png' width='50%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6186374f-d1d8-484d-b7aa-931fce9680fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2. 一种泛化的均值迭代计算方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cf824c-f1eb-4942-a33b-47592f34d634",
   "metadata": {},
   "source": [
    "- 前述方法二可以归纳为一个迭代表达式：\n",
    "$$w_{k+1} = w_k - \\frac{1}{k}(w_k-x_{k})$$\n",
    "- 该迭代表达式可以进一步一般化为：\n",
    "$$w_{k+1} = w_k - \\alpha_k(w_k-x_{k})$$\n",
    "  - 当$\\alpha_k$满足一定条件时，可以证明当$k\\rightarrow \\infty时，w_k \\rightarrow E(X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33acb9fd-b676-4ab1-bdf6-27f1330e5917",
   "metadata": {},
   "source": [
    "## I. RM算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f447b766-da2b-4e8e-b74b-0f1d9b87a9ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### I.1 stochastic approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28478ca-98ad-4cab-9a22-6d8bb32565f4",
   "metadata": {},
   "source": [
    "- RM算法是stochastic approximation算法中的一种。\n",
    "- <font color=blue>**stochastic approximation算法**</font>：是一类通过使用迭代法求解优化问题和求根问题的方法。和其他解决优化和求根问题的方法，如梯度下降法不同，stochastic approximation算法的优点在于不需要知道目标函数的或者其梯度的表达式，只需要有足够的函数样本就能使用。\n",
    "  - <font color=deeppink>**优化问题在一定条件下可以转换为求根问题**</font>：\n",
    "    - 如果函数是凸函数，那么求最小值的优化问题就是求解$\\nabla f(x)=0$的根。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac25f13",
   "metadata": {},
   "source": [
    "### I.2 RM算法内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3389393-9941-4e48-9c3b-2278782501b8",
   "metadata": {},
   "source": [
    "- 问题：求$w^*$，使其满足$g(w^*)=0$，g(w)的表达式未知，但是有很多含噪声的{w，g(w)}样本pairs。\n",
    "   - 比如一个NN，没有对应的方程表达式，但是有输入和输出样本。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59562436-5c91-4b78-89eb-fa97023d6e9b",
   "metadata": {},
   "source": [
    "- 方法："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63adb09-3f93-41b1-bbb3-666769e8ae08",
   "metadata": {},
   "source": [
    "$$迭代求解：w_{k+1}=w_{k} - a_k * \\tilde g(w_k,\\ \\eta_k)，k=1, 2, 3, ...$$\n",
    "- 说明：\n",
    "  1. $w_k$是$g(w)=0$函数根的第k次估计\n",
    "  2. $\\tilde g(w_k,\\eta_k)是$g(w)$的第k个样本观测值$。$\\tilde g(w_k, \\eta_k)  = g(w_k) + \\eta$\n",
    "  3. $a_k$是一个positive coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91c174a-14cb-4c41-83ae-369539f9dbbe",
   "metadata": {},
   "source": [
    "### I.3 RM算法收敛性：almost surely convergence(strong consistent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c3f38a-8f4c-431a-b982-41fc31b7d148",
   "metadata": {},
   "source": [
    "- 注意两个概念：\n",
    "  1. strong consistent = almost surely convergence\n",
    "  2. weak consistent = converges in probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247f9942-aee2-470c-9173-e43d9d0c3a4d",
   "metadata": {},
   "source": [
    "- 下面三个条件满足时，$w_k$迭代<font color=green>**almost sure**</font>收敛到$w^*$ ：\n",
    "  1. <font color=norange>$\\mathbf{0<c_1\\le \\nabla _wg(w)\\le c_2}$</font>\n",
    "  2. <font color=norange>$\\mathbf{\\sum^{\\infty}_{k=1}a_k=\\infty,且\\sum^{\\infty}_{k=1}a_k^2<\\infty}$</font>\n",
    "  3. <font color=norange>$\\mathbf{E(\\eta_k|W_k)=0, E(\\eta^2_k|W_k)<\\infty$，其中，$W_k={w_k, w_{k-1}, ..., }}$</font>\n",
    "     - 当$\\eta_k与W_k$无关时，第三个条件简化为$E(\\eta_k)=0, E(\\eta^2_k)<\\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7100d3-424a-4176-b986-ab6fef2ee383",
   "metadata": {},
   "source": [
    "- **条件1的说明**：<font color=norange>$\\mathbf{0<c_1\\le \\nabla _wg(w)\\le c_2}$</font>\n",
    "     - <font color=orange>下限不能取0是保证g(w)=0必然有解的关键。</font>因为此时函数g(w)单调递增，有且只有一个0解。\n",
    "     - 上限的存在确保了单次迭代后w的小幅变化不会让函数值f(w)发生巨大的变化，导致f(w_k)在0值上下振动。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cfde23-1ec1-4521-9b53-28e822efa9a4",
   "metadata": {},
   "source": [
    "- **条件2的说明**：<font color=norange>$\\mathbf{\\sum^{\\infty}_{k=1}a_k=\\infty,且\\sum^{\\infty}_{k=1}a_k^2<\\infty}$</font>\n",
    "     - $\\sum^{\\infty}_{k=1}a_k=\\infty$：确保遍历范围达到所有可能的取值，不管初始$w_0$距离$w^*$有多远，都能收敛到最优值。\n",
    "       - 说明：\n",
    "         - $w_1-w_{\\infty}=\\sum_{k=1}^{\\infty}a_k*\\tilde g(w_k, \\eta_k)$，该式左边表示整个迭代过程中w所能覆盖的最远范围。如果没有$\\sum_{k=1}^{\\infty}a_k\\rightarrow \\infty$这个条件，上式右边值有限，就意味着遍历所能达到的最远范围有限。\n",
    "     - $\\sum^{\\infty}_{k=1}a_k^2<\\infty意味着当k\\rightarrow \\infty时，\\alpha_k\\rightarrow 0$：确保当迭代次数足够大，到达最优值附近的时候，$w_k$不要大幅抖动。\n",
    "       - 说明：\n",
    "         - $w_{k+1}-w_k=-\\alpha_k * \\tilde g(w_k,\\ \\eta_k)$，如果$当k\\rightarrow \\infty时，\\alpha_k\\rightarrow 0$不成立，则$w_{k+1}-w_k$不会逼近0，也就是说，$w_k$在迭代次数达到无穷大后，仍然会振动。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fb4afa-b9ef-49ba-85ae-c3d5b9d139f6",
   "metadata": {},
   "source": [
    "- 实践：\n",
    "  - 满足上述条件的典型例子：$a_k=\\frac{1}{k}$。<font color=orange>虽然满足上面两个条件，但也有明显的缺点。当迭代次数k越来越大以后，$\\frac{1}{k}$逼近0的速度很快，导致后面的样本对w更新的影响很小，所以不常用。</font>\n",
    "  - 实践中常见取$\\alpha_k$为一个很小的常数，此时条件2中$\\sum^{\\infty}_{k=1}a_k^2<\\infty$不满足，但是如果允许一定的最小误差，影响不大。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be43a132-3d54-4e44-a730-4e322224b3b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### I.4 无偏性分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1cc398-0670-4cc4-82a5-e6e744d0dde2",
   "metadata": {},
   "source": [
    "- <font color=red>**RM算法的收敛能保证convergence，但不能保证unbias。**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3d8204-0fff-4845-a81f-c1e105290c0e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### I.4.1 概念: asymptotically unbiased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fc5a32-b284-4247-9bb0-34885dc3d599",
   "metadata": {},
   "source": [
    "- **定义**：\n",
    "  - 当如果估计量样本量趋于无穷大时是无偏估计，就称该估计量是asymptotically unbiased\n",
    "$$\\lim_{n \\to \\infty} E\\hat \\theta_n = \\theta$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79f6b13-8243-4ae0-a1c4-92775f3c0a6e",
   "metadata": {},
   "source": [
    "- <font color=blue>consistent是比asymptotically unbiased更强的性质。asymptotically unbiased的估计量不一定是consistent的。但consistent估计量一定是asymptotically unbiased。</font>\n",
    "  - 比如：i.i.d.样本{x1,x2,...,xn},EX=μ,Var(X)=σ2.如果用x1+1/n(注意不是xi,是第1个样本x1)作为μ的估计量，那么该估计量有bias 1/n，但是满足asymptotically unbiased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de388a5-ed97-4fcf-9751-5adedc12bc04",
   "metadata": {},
   "source": [
    "#### I.4.2 在求函数期望值$Ef(x)$时，RM算法得到的估计量满足asymptotically unbiased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9451048-7c8b-4e1e-962a-58c3d8c8bed2",
   "metadata": {},
   "source": [
    "- 迭代式：$\\theta_{k+1}=\\theta_k -\\alpha_k(\\theta_k - f(x_k))$\n",
    "- 无偏性证明：\n",
    "$$\\begin{align}\n",
    "\\alpha_k & = \\frac{E\\theta_{k+1}-E\\theta_k}{Ef(x_k)-E\\theta _k}  = \\frac{E\\theta_{k+1}-E\\theta_k}{\\mu-E\\theta _k}\\\\\n",
    "1- \\alpha_k & = 1- \\frac{E\\theta_{k+1}-E\\theta_k}{\\mu-E\\theta _k}=\\frac{\\mu-E\\theta_{k+1}}{\\mu-E\\theta _k}\\\\\n",
    "\\prod_{i=1}^k (1-\\alpha_i)& = \\frac{\\mu-E\\theta_{k+1}}{\\mu-E\\theta _1}\\\\\n",
    "&\\because \\lim_{k \\to \\infty} \\prod_{i=1}^k (1-\\alpha_i) = 0, then\\ \\lim_{k \\to \\infty} (\\mu-E\\theta_{k+1})=0\n",
    "\\end{align}$$\n",
    "  - <font color=green>这里无偏性的分析是因为Ef(x)为目标时构造的$\\bar g(\\theta, \\eta)$正好可以分解成$\\theta - f(x)$的形式。但一般化的RM算法中，这个特征并不普遍存在，也因此通常没有这里的无偏性质。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87672d5d-37e9-477a-858b-1950b1aa6c8d",
   "metadata": {},
   "source": [
    "- <font color=blue>**收敛过程中的bias特征：随着迭代次数的增加，C会越来越小，此时bias也会越来越小**</font>\n",
    "  - 分析：\n",
    "$$\\begin{align}\n",
    "&取C=\\prod_{i=1}^k (1-\\alpha_i),有：\\\\\n",
    "&\\mu-E\\theta_{k+1} = (\\mu-E\\theta _1)\\prod_{i=1}^k (1-\\alpha_i)= C(\\mu-E\\theta _1)\\\\\n",
    "\\end{align}$$\n",
    "    - 因为 $1-\\alpha_k<1$，所以随着迭代次数的增加，C会越来越小，此时bias也会越来越小。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053d813b-5fac-4a79-967e-7281b975af28",
   "metadata": {},
   "source": [
    "#### I.4.3 RM算法得到的估计量is biased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b8db25-201b-4334-ae69-37a69a90ee30",
   "metadata": {},
   "source": [
    "- <font color=brown>即使在特定场景下，如求$Ef(x)$时可以证明RM迭代得到的估计量是asymptotically unbiased。但其他分析中，上述性质并不普遍存在。RM算法估计量通常情况下是有偏的。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92112c5-25f5-452e-8c39-7d050c1ddfee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### I.5 算法的intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8a8d6e-04dd-4a86-ab0e-833dd58ff0b7",
   "metadata": {},
   "source": [
    "- 当满足收敛的三个条件时，函数图形可以简单如下图所示：\n",
    "  - <img src='pics/rm_intuition.png' width='40%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9def96a4-77c8-4a26-83fb-c8cf5224381c",
   "metadata": {},
   "source": [
    "- 简化假设样本中的噪音较小，接近于0，有：\n",
    "  - $w_k > w^*时，g(w)>0，w_{k+1}=w_k-\\alpha_k * g(w_k) < w_k, 只要\\alpha_k * g(w_k)足够小，则w^*<w_{k+1}<w_k，w_{k+1}比w_k更接近w^*$\n",
    "  - $w_k < w^*时，g(w)<0，w_{k+1}=w_k-\\alpha_k * g(w_k) > w_k, 同样，w_{k+1}比w_k更接近w^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb238fc-e433-4d6e-9360-9d95354cb9ae",
   "metadata": {},
   "source": [
    "### I.6 RM算法收敛性的证明"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99159c99-8798-44fc-a5d1-3fd77592edcb",
   "metadata": {},
   "source": [
    "#### I.6.1 Dvorotzky's theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5493a9-8142-420c-9821-98e4c575ce49",
   "metadata": {},
   "source": [
    "- 随机过程:\n",
    "$$\\Delta_{k+1}=(1-\\alpha_k)*\\Delta_k+\\beta_k*\\eta_k$$\n",
    "  - 其中$\\{\\alpha_k\\}_{k=1}^{\\infty}, \\{\\beta_k\\}_{k=1}^{\\infty}, \\{\\eta_k\\}_{k=1}^{\\infty}$都是随机序列，且$\\alpha_k \\ge 0, \\beta_k \\ge 0$。\n",
    "  - 在满足以下条件时，$\\Delta_k$ almost surely converge to zero:\n",
    "    1. $\\sum^{\\infty}_{k=1}a_k=\\infty,\\ \\sum^{\\infty}_{k=1}a_k^2<\\infty,且\\sum^{\\infty}_{k=1}\\beta _k^2<\\infty$ uniformly almost surely\n",
    "    2. $E(\\eta_k|H_k)=0, E(\\eta^2_k|H_k)<C$ almost surely.$H_k = \\{\\Delta_k,\\Delta_{k-1}, ..., \\alpha_k, \\alpha_{k-1},...,\\beta_k, \\beta_{k-1} \\}$\n",
    "- <font color=brown>[证明见math of RL,6.3.1节 page119]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12956f3-671f-4c76-9e72-fe5213d5b8be",
   "metadata": {},
   "source": [
    "#### I.6.2 证明RM算法满足Dvorotzky's theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcabe2d8-88be-4bf6-90d0-04b2613d3eda",
   "metadata": {},
   "source": [
    "- 简单分析，详细条件见math of RL,6.3.3节\n",
    "$$\\begin{align}\n",
    "w_{k+1} & = w_{k} - a_k * \\tilde g(w_k,\\ \\eta_k) \\\\\n",
    "& = w_{k} - a_k * [g(w_k)+\\eta_k] \\\\\n",
    "w_{k+1} - w^* & = w_{k} - w^* - a_k * [g(w_k)+\\eta_k] \\\\\n",
    "& =  w_{k} - w^* - a_k * [g(w_k) - g(w^*)+\\eta_k] \\\\\n",
    "& =  w_{k} - w^* - a_k * [\\nabla g(w_k')( w_{k} - w^*)+\\eta_k] \\\\\n",
    "\\\\\n",
    "\\Delta_{k+1} & = \\Delta_k - a_k[\\nabla g(w_k')\\Delta_k +\\eta_k] \\\\\n",
    "& = (1-a_k\\nabla g(w_k'))\\Delta_k+a_k(-\\eta_k) \\ , 取\\alpha_k=1-a_k\\nabla g(w_k') \\\\\n",
    "& = \\alpha_k\\Delta_k+a_k(-\\eta_k)\n",
    "\\end{align}$$\n",
    "  - <font color=red>注意，在$ \\tilde g(w_k,\\ \\eta_k)=g(w_k)+\\eta_k$的分解中，所有的随机性都包含在$\\eta_k$中</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e156b752-b531-4971-ba9e-0efe21d257cb",
   "metadata": {},
   "source": [
    "#### I.6.3 Dvorotzky's theorem的一种扩展形式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8a4b7c-2026-4ac4-86fa-be9b8133f6cf",
   "metadata": {},
   "source": [
    "- <font color=norange>在TD类算法的收敛性就是基于Dvorotzky's theorem的这种扩展形式。证明的要点就在于，虽然迭代过程中$V(s_k)需要来自V(s_{k+1})$的信息，但是$V(s_{k+1})$可以放到$\\eta(s_k)_k$中将其视为对状态$s_k$的不确定性，只要整个$\\eta(s_k)$满足条件，就不影响整个迭代的收敛性。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bd30cf-8ca4-46f6-a622-b4298fd2fa23",
   "metadata": {},
   "source": [
    "- <img src='pics/dvorotzky_extension.png' width='70%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218c1edc-13d3-4b8e-b306-a7c4da7f3ee9",
   "metadata": {},
   "source": [
    "## II. RM算法的两大典型应用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acfdde2-4556-4c92-a268-8ae71dd4a44c",
   "metadata": {},
   "source": [
    "1. 尽管算法本身是解决g(w)=0的问题，但$g(w)=E\\tilde g(w, \\eta)$，可见RM算法本质上是一种通过样本取值估计均值的问题。所以RM算法的第一类应用就是估计$Ef(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5ae71a-a65d-4d55-859c-b81280a92fc6",
   "metadata": {},
   "source": [
    "2. 根据前文所述，凸优化与求根问题之间有等价关系。因此，算法的第类典型应用就是求解优化问题。此时，得到的迭代式结果与SGD算法一致。\n",
    "   - 当$J(\\theta)=Ef(x;\\theta)$时:\n",
    "$$\\begin{align}\n",
    "\\underset{\\theta }{min} J(\\theta) =\\underset{\\theta }{min} E_xf(\\theta, x) & \\Leftrightarrow \\nabla_\\theta E_xf(\\theta, x)  = 0 \\\\\n",
    "& \\Leftrightarrow E_x\\nabla_\\theta f(\\theta, x)  = 0 \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e698642e-a2c7-489c-b1a3-9c4143144864",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### II.1 RM算法求均值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c293efa-f778-4267-8951-357798667da3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 1. RM算法求解函数均值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f029732a",
   "metadata": {},
   "source": [
    "- 原问题：已知样本$\\{(x_i, f(x_i))\\}_{i=1}^N$，求$Ef(X)=w$\n",
    "- 转化为RM问题：$g(w)=w-Ef(X)=0$<font color=red> [注：不能是$g(w)=Ef(X)-w=0$，否则不满足RM算法的第一个条件]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af20b15-e9af-436b-9929-ac4f161ea6a6",
   "metadata": {},
   "source": [
    "- 构造迭代要素：\n",
    "$$\n",
    "\\begin{align} \n",
    "\\tilde g(w, \\eta) & = w-f(x) \\\\\n",
    "\\eta & = \\tilde g(w, \\eta) - g(w) \\\\\n",
    "& = [w-f(x)]-[w-Ef(X)] \\\\\n",
    "& = Ef(X)-f(x)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cf74b9-8e4f-4229-bb83-6286f5dea793",
   "metadata": {},
   "source": [
    "- 得到迭代式\n",
    "$$\n",
    "\\begin{align} \n",
    "w_{k+1} \n",
    "& = w_k - \\alpha_k\\tilde g(w_k, \\eta) \\\\\n",
    "& = w_k - \\alpha_k(w_k-f(x_k)) \\\\\n",
    "本质：\\\\\n",
    "新估计 & = 旧估计 - 学习速度*[旧估计 - 新样本值]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fca1f1-11d9-47ea-834c-959d6c778e66",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2. RM算法求样本均值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be83d4b6-01bc-4928-848e-1f466d8e12f5",
   "metadata": {},
   "source": [
    "- 当取f(x)=x时，上式等价于均值估计：\n",
    "$$w_{k+1} = w_{k}-a_k(w_k-x_k)$$\n",
    "$$取a_k=\\frac{1}{k}，w_{k+1} = w_{k}-\\frac{1}{k}(w_k-x_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c401e595-d7d1-4d0d-b5b5-17d459efa93d",
   "metadata": {},
   "source": [
    "### II.2 RM算法求函数最小值：SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c001b7a-5708-419e-8bc6-e1ecc30936af",
   "metadata": {},
   "source": [
    "#### II.2.1 用RM算法推导SGD迭代式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43de2ebe",
   "metadata": {},
   "source": [
    "1. **RM算法求解函数期望值的最小化问题**： $\\underset{w}{min}Ef(w, x)$\n",
    "   - 原问题：$\\underset{\\theta}{min} J(\\theta)  =\\underset{\\theta}{min}  E[f(\\theta, X)]$\n",
    "   - 等价问题：$E_x\\nabla_\\theta f(\\theta, x)  = 0$\n",
    "   - 转化为RM算法可解问题：$g(\\theta) = E[\\nabla_{\\theta}f(\\theta, X)]$\n",
    "   - <font color=blue>**用RM算法得到和SGD相同的迭代形式**</font>：\n",
    "$$\n",
    "\\begin{align}\n",
    "g(\\theta) & = E[\\nabla_{\\theta}f(\\theta, x)]\\\\\n",
    "\\tilde{g}(\\theta,\\eta ) &= \\nabla_{\\theta}f(\\theta, x) \\\\\n",
    "\\eta &= \\tilde{g}(\\theta,\\eta ) - g(\\theta)\\\\&= \\nabla_{\\theta}f(\\theta, x)-E[\\nabla_{\\theta}f(\\theta, x)]\\\\\n",
    "\\\\\n",
    "迭代形式：\n",
    "\\theta_{k+1} & = \\theta_k -a_k*\\nabla_{\\theta}f(\\theta_k, x_k)\n",
    "\\end{align}\n",
    "$$\n",
    "   - <font color=brown>[符号说明：$\\theta和w$表示相同含义]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0286e373-6a82-41e0-b881-ff66334df038",
   "metadata": {},
   "source": [
    "2. <font color=green>**RM算法也可以推导解决样本均值最小化问题(非期望形态)的SGD**：</font>\n",
    "   - 原问题：$\\underset{w}{min} \\frac{1}{n}\\sum_i f(w, x_i)$\n",
    "   - 解决思路：将deterministic value的n个x取值集合${x_1, x_2, ..., x_n}$转化为分布$P(X=x_i)=\\frac{1}{n}$。此时有：\n",
    "$$\\frac{1}{n}\\sum_i f(w, x_i)=Ef(w, x)$$\n",
    "   - 转化为等价的期望值最小化问题：$\\underset{w}{min} \\frac{1}{n}\\sum_i f(w, x_i)=\\underset{w}{min}Ef(w, x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987d17c1-6169-4878-abba-36b392eae28b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### II.2.2 SGD算法的收敛性质"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d229b2",
   "metadata": {},
   "source": [
    "1. SGD的收敛条件：<font color=green>**对应RM算法的3个条件**</font>\n",
    "   - (1) $0<c_1\\le \\nabla ^2 _wf(w,X)\\le c_2$。自然有$0<c_1\\le E[\\nabla ^2 _wf(w,X)]\\le c_2$。且此时f(w,X)相对w是凸函数。\n",
    "   - (2) $\\sum^{\\infty}_{k=1}a_k=\\infty,且\\sum^{\\infty}_{k=1}a_k^2<\\infty$\n",
    "   - (3) $\\{x_k\\}_{k=1}^{\\infty}是iid的$。此时可以证明$E\\eta=0$，用$|\\nabla f(w_k,x)|<\\infty$可以证明$E\\eta^2<\\infty$\n",
    "$$\n",
    "\\begin{align}\n",
    "E\\eta & = E\\left (  \\nabla_wf(w, x)-E[\\nabla_wf(w, X)]  \\right )\\\\\n",
    "& = E[\\nabla_wf(w, x)]-E[\\nabla_wf(w, X)]，\\because \\{x_i\\}是iid的 \\\\\n",
    "& = 0\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "   - 满足这3个条件后，$w_k$almost surely converge to $\\nabla_wEf(w, X)=0$的解。<font color=brown>[证明见math of RM的6.4.5节]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d3242e",
   "metadata": {},
   "source": [
    "2. 定义一个用于衡量收敛结果的指标：relative error\n",
    "$$\\delta_k = \\frac{\\left | \\nabla_wf(w_k, x_k)-E[\\nabla_wf(w_k, X)] \\right | }{\\left | E[\\nabla_wf(w_k, X)] \\right | } $$\n",
    "   - **含义**：<font color=blue>分子是stochastic gradient$\\nabla_wf(w_k, x_k)$与true gradient$E[\\nabla_wf(w_k, X)]$的距离，整个式子表示stochastic gradient与true gradient的相对距离。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5e6828",
   "metadata": {},
   "source": [
    "3. relative error有性质：\n",
    "$$\n",
    "\\delta_k \\le \\frac{\\left | \\nabla_wf(w_k, x_k)-E[\\nabla_wf(w_k, X)] \\right | }{c\\left | w_k-w^* \\right | }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67ec643",
   "metadata": {},
   "source": [
    "<font color=grey>\n",
    "- 证明：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\because  E[\\nabla_wf(w^*, X)]& =0 \\\\\n",
    "\\therefore  E[\\nabla_wf(w_k, X)] & = E[\\nabla_wf(w_k, X)]-E[\\nabla_wf(w^*, X)] \\\\\n",
    " 根据中值定理：& \\exists \\tilde w_k \\in [w_k,w^*]，使得：\\\\\n",
    "\\left | E[\\nabla_wf(w_k, X)] \\right |  & = \\left |E[\\nabla_wf(w_k, X)]-E[\\nabla_wf(w^*, X)] \\right |\\\\\n",
    "& =\\left |(w_k-w^*)E[\\nabla^2_wf(\\tilde w_k, X)]\\right | \\\\\n",
    "& \\ge c\\left |w_k-w^*\\right |，\\because 当SGD满足收敛条件，就有\\nabla^2_wf(w, X) \\ge c_1 > 0  \\\\\n",
    "代入上面的定义即可得证\n",
    "\\end{align}\n",
    "$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c55b9f",
   "metadata": {},
   "source": [
    "4. **SGD的convergence pattern**\n",
    "   - 上述不等式的分母代表了$w_k$与最优值$w_*$的距离，relative error与之成反比。**结论：<font color=green>当$w_k$距离最优值很远的时候，分母大，relative error很小，所以stochastic gradient与true gradient的差异就很小。这时候使用SGD与BGD的效果接近。反之，当$w_k$逼近真实值之后，分母变大，这时候relative error很可能较大，SGD在这时候与BGD的差异很大，此时convergence轨迹会出现更多randomness。</font>**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
