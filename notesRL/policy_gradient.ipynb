{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f6db28e-ab4b-4cbd-b3ba-104ec91dc37a",
   "metadata": {},
   "source": [
    "# Policy Gradient的推导"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce413bf5-2a0b-4a9a-98c3-f9af3c90fb6b",
   "metadata": {},
   "source": [
    "- <font color=brown>符号说明：文中 $a_t和u_t$ 混用，都表示t时刻的action。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3935311-9d64-4aff-b2a0-325eee035f2e",
   "metadata": {},
   "source": [
    "## I. 原目标：最大化状态的效用期望值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7444f12b-d9fd-4306-aec7-37d803c29b43",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### I.1 两种常见的目标展开方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a65a52f-ca53-4d85-88a5-5059ebedc45a",
   "metadata": {},
   "source": [
    "1. trajectory的期望rewards：\n",
    "$$U(\\theta)  = E_{\\tau }R(\\tau|\\pi_{\\theta}) = \\sum_{\\tau }P(\\tau ;\\theta )R(\\tau )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1982c6b1-225f-4ea3-9576-f072edd8525d",
   "metadata": {},
   "source": [
    "2. 不同状态价值的均值：\n",
    "$$\\begin{align}\n",
    "严格说应该是初始状态的价值期望值：\\\\\n",
    "U(\\theta) & = E_{s\\sim d(s_0)}[V^{\\pi_{\\theta }}(s)]=\\sum_{s_0\\in \\mathcal{S} }d(s_0)V^{\\pi_{\\theta }}(s_0)\\\\\n",
    "也可以取s处于stationary\\ distribution时的分布：\\\\\n",
    "U(\\theta) & = E_{s\\sim d(s)}[V^{\\pi_{\\theta }}(s)]=\\sum_{s\\in \\mathcal{S} }d(s)V^{\\pi_{\\theta }}(s)\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ebabcd-44ab-4eeb-b7ef-0c1b6f514bf1",
   "metadata": {},
   "source": [
    "### I.2 目标展开式暗含的假设条件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924eccb8-c109-4253-8773-4bfd6b64b7fd",
   "metadata": {},
   "source": [
    "- 期望值合理的前提是随机变量的分布稳定。也就是说，trajectory展开式中，假设了trajectory的分布稳定；状态价值展开式中，假设了状态s的分布稳定。<font color=red>但这两个条件在现实条件下都不成立。</font>\n",
    "- 在状态价值展开式中，理论上，如果agent与环境交互的次数极大的条件下，$d(s)$可以用策略对应的stationary distribution。但是这个条件在真实环境中通常也很难满足。\n",
    "- 尽管他们的分布处于不稳定状态，实践中还是会用monte carol抽样法直接估计期望的方式来得到估计量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffd7646-fc5b-436c-836e-0779dc70b683",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## II. 直接用目标推导梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aef1420-a783-42c2-9a63-44869644fac5",
   "metadata": {},
   "source": [
    "### II.1 用trajectory的期望rewards推导"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977c9981-d37b-4d84-ac7d-c54302233a62",
   "metadata": {},
   "source": [
    "#### II.1.1 梯度的基本公式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42cdc9f-5c29-4904-9428-550052c6d534",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "\\nabla _{\\theta }U(\\theta ) \n",
    "& = \\nabla _{\\theta } \\sum_{\\tau }P(\\tau ;\\theta )R(\\tau ) \\\\\n",
    "& =  \\sum_{\\tau }\\nabla _{\\theta }P(\\tau ;\\theta )R(\\tau ), {\\color{green}{这步是因为\\theta带给\\tau的随机性都体现在P(\\tau;\\theta)里面}} \\\\\n",
    "& =  \\sum_{\\tau }P(\\tau ;\\theta )\\frac{\\nabla _{\\theta }P(\\tau ;\\theta )}{P(\\tau ;\\theta )}R(\\tau ) \\\\\n",
    "& =  E_{\\tau }\\nabla _{\\theta }logP(\\tau ;\\theta )R(\\tau ) \\\\\n",
    "\\\\\n",
    "\\nabla _{\\theta }logP(\\tau ;\\theta ) \n",
    "& = \\nabla _{\\theta }log\\left[ d(s_0)\\prod_{t=0}^{H-1} P(s_{t+1}|s_t, u_t)*\\pi_{\\theta }(u_t|s_t) \\right ]\\\\\n",
    "& = \\nabla _{\\theta }\\left[logd(s_0) + \\sum_{t=0}^{H-1}logP(s_{t+1}|s_t, u_t) + \\sum_{t=0}^{H-1}log\\pi_{\\theta }(u_t|s_t)\\right ]\\\\\n",
    "& = \\underset{0}{ \\underbrace{\\nabla _{\\theta }logd(s_0)}}  + \\underset{0}{\\underbrace{\\nabla _{\\theta }\\sum_{t=0}^{H-1}logP(s_{t+1}|s_t, u_t)}} + \\nabla _{\\theta }\\sum_{t=0}^{H-1}log\\pi_{\\theta }(u_t|s_t)\\\\\n",
    "& = \\nabla _{\\theta }\\sum_{t=0}^{H-1}log\\pi_{\\theta }(u_t|s_t) \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803a00c4-ae4f-467e-a515-bfc87ba44d78",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "取梯度：g & = \\nabla _{\\theta }U(\\theta )\\\\\n",
    "& =  E_{\\tau }\\nabla _{\\theta }logP(\\tau ;\\theta )R(\\tau ) \\\\\n",
    "& = E_{\\tau }\\left[\\nabla _{\\theta }\\sum_{t=0}^{H-1}log\\pi_{\\theta }(u_t|s_t)R(\\tau )\\right ]\\\\\n",
    "& = E_{\\tau }\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t|s_t)\\sum_{t=0}^{H-1}r(s_t, u_t)\\right ]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2012fe80-7107-4643-9779-6fab919edae5",
   "metadata": {},
   "source": [
    "#### II.1.2 rewards to go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d5c2b1-8194-4e71-a7fd-add002873b6b",
   "metadata": {},
   "source": [
    "$$g = E_{\\tau }\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t|s_t)\\sum_{t'=t}^{H-1}r(s_{t'}, u_{t'})\\right ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63be9e4-f19d-4333-9b48-68d73ebba08b",
   "metadata": {},
   "source": [
    "#### II.1.3 baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accfd6a0-59b4-477b-9d94-c222715e97a9",
   "metadata": {},
   "source": [
    "- 当b是常数时，下式成立：\n",
    "$$\\begin{align}\n",
    "E_{\\tau }[\\nabla _{\\theta }logP(\\tau ;\\theta )]b& = \\nabla _{\\theta } \\sum_{\\tau }P(\\tau ;\\theta )b \\\\\n",
    "& = b\\nabla _{\\theta } \\sum_{\\tau }P(\\tau ;\\theta )\\\\\n",
    "& = b\\nabla _{\\theta }1 = 0\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5666cb-6ecc-48bc-97d6-5cf86667a8ef",
   "metadata": {},
   "source": [
    "- <font color=norange>**即使b是states的函数，只要b(s)不受action的影响，等式仍然成立。注意，这里关键是b(s)与action的关系，而不是b(s)与策略的关系。即使b(s)受策略的影响，比如图取值为状态价值函数V(s)也不改变结果。**</font>见下面分析过程："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9ee64c-8acf-410c-81fa-a22f94371fd6",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "E_{\\tau }\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(a_t|s_t)b(s_t)\\right ]\n",
    "& = E_{\\tau }\\left[ \\sum_{t=0}^{H-1}[\\nabla _{\\theta }log\\pi_{\\theta }(a_t|s_t)]V^{\\pi }(s_t)\\right ]\\\\\n",
    "& = \\underset{s_t\\in\\mathcal{S},a_t\\in \\mathcal{A}}{E}\\left[ \\sum_{t=0}^{H-1}[\\nabla _{\\theta }log\\pi_{\\theta }(a_t|s_t)]V^{\\pi }(s_t)\\right ]\\\\\n",
    "& = \\sum_{t=0}^{H-1}\\left[\\sum_{s_t\\in\\mathcal{S}}P(s_t)\\sum_{a_t\\in \\mathcal{A}} [\\pi_{\\theta }(a_t|s_t)\\nabla _{\\theta }log\\pi_{\\theta }(a_t|s_t)]V^{\\pi }(s_t)\\right ]\\\\\n",
    "& = \\sum_{t=0}^{H-1}\\left[\\sum_{s_t\\in\\mathcal{S}}P(s_t)\\sum_{a_t\\in \\mathcal{A}}[\\nabla _{\\theta }\\pi_{\\theta }(a_t|s_t)]V^{\\pi }(s_t)\\right ],{\\color{green}{因为V(s_t)与a_t无关}}\\\\\n",
    "& = \\sum_{t=0}^{H-1}\\left[\\sum_{s_t\\in\\mathcal{S}}P(s_t)V^{\\pi }(s_t)\\sum_{a_t\\in \\mathcal{A}}\\nabla _{\\theta }\\pi_{\\theta }(a_t|s_t)\\right ]\\\\\n",
    "& = \\sum_{t=0}^{H-1}\\left[\\sum_{s_t\\in\\mathcal{S}}P(s_t)V^{\\pi }(s_t)\\nabla _{\\theta }\\left(\\sum_{a_t\\in \\mathcal{A}}\\pi_{\\theta }(a_t|s_t)\\right)\\right ]\\\\\n",
    "& = \\sum_{t=0}^{H-1}\\left[\\sum_{s_t\\in\\mathcal{S}}P(s_t)V^{\\pi }(s_t)\\nabla _{\\theta }1\\right ]=0\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb59c94-45e5-4274-913e-05173adb38f5",
   "metadata": {},
   "source": [
    "- 为了降低估计量的方差，b(s)的sub-optimal取值为$b(s)=V^{\\pi}(s)$。\n",
    "  - 将action advantage记为$A_t=Q^{\\pi}(s_t,u_t)-V^{\\pi}(s_t)$\n",
    "  - 因为 $Q^{\\pi}(s_t,u_t) = \\sum_{t'=t}^{H-1}E_{\\pi}[r(s_{t'}, u_{t'})|s_t,u_t]$\n",
    "$$\\begin{align}\n",
    "g & =  E_{\\tau }\\nabla _{\\theta }logP(\\tau ;\\theta )[R(\\tau ) - b]\\\\\n",
    "&= E_{\\tau }\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t|s_t)\\left( \\sum_{t'=t}^{H-1}r(s_{t'}, u_{t'})-b(s_t)\\right)\\right ]\\\\\n",
    "&= E_{\\tau }\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t|s_t)\\left( Q(s_{t}, u_{t})-V(s_t)\\right)\\right ]\\\\\n",
    "&= E_{\\tau }\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t|s_t)A_t\\right ]\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1600a745-a729-4bd2-9ae5-b32a20353b21",
   "metadata": {},
   "source": [
    "- 此时，策略梯度的估计量为：\n",
    "$$\\begin{align}\n",
    "\\hat g & = \\frac{1}{N} \\sum _{i=1 }^N\\nabla _{\\theta }logP(\\tau_i;\\theta )[R(\\tau_i) - b]\\\\\n",
    "&= \\frac{1}{N} \\sum _{i=1 }^N\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t^{(i)}|s_t^{(i)})\\left( \\sum_{t'=t}^{H-1}r(s_{t'}^{(i)}, u_{t'}^{(i)})-V^{\\pi}(s_t^{(i)})\\right)\\right ]\\\\\n",
    "&= \\frac{1}{N} \\sum _{i=1 }^N\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t^{(i)}|s_t^{(i)})\\hat A_t\\right ]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b9dfb0-ca3d-4573-8be7-c77724a2a1e1",
   "metadata": {},
   "source": [
    "- <font color=blue>**不同估计方法的bias-varince tradeoff**</font>：\n",
    "  1. 如果用Monte Carol估$R(\\tau_i) - V(s_t^{(i)})$，得到的估计量unbiased，但是varince很大\n",
    "  2. 改用DNN先估 $V(s_t)或者Q(s_t)$，然后得到Advantage function的估计量的方法能降低variance，但是此时不再满足unbiased条件。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee18dfb1-6eaa-4267-bc07-9c5cf0f1f4ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### II.2 用state function期望值推导"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21df144-7c48-482c-a44f-06f4c0a309eb",
   "metadata": {},
   "source": [
    "- <font color=orange>这种推导方式的假设比用trajectory的假设多了一条：d(s)不受策略影响。而这一点在现实中通常不成立，所以这种方式描述现实的能力不如用trajectory期望值的推导方式。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06e8aa3-0889-4272-9bef-55aba1d20546",
   "metadata": {},
   "source": [
    "- <font color=red>**假如d(s)不受策略影响**</font>：这个假设通常不成立，但是能简化策略梯度的推导\n",
    "$$\\begin{align}\n",
    "g = \\nabla _{\\theta }U(\\theta) & = \\nabla _{\\theta }E_{s\\sim d(s)}[V^{\\pi_{\\theta }}(s)] \\\\\n",
    "& = \\nabla _{\\theta }\\sum_{s\\in \\mathcal{S} }d(s)V^{\\pi_{\\theta }}(s) \\\\\n",
    "& = \\sum_{s\\in \\mathcal{S} }d(s)\\nabla _{\\theta }V^{\\pi_{\\theta }}(s) \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1931b14-5ae1-487f-8849-67e6475bdd17",
   "metadata": {},
   "source": [
    "- 将状态价值函数的梯度展开可以得到一个线性方程组：\n",
    "$$\\begin{align}\n",
    "\\nabla _{\\theta }V^{\\pi_{\\theta }}(s)\n",
    "& = \\nabla _{\\theta }\\left[ \\sum_{a\\in \\mathcal{A}} \\pi(a|s)Q^{\\pi }(s,a)\\right]\\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}} \\left[ Q^{\\pi }(s,a)\\nabla _{\\theta } \\pi(a|s) + \\pi(a|s)\\nabla _{\\theta } Q^{\\pi }(s,a)\\right]\\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}} \\left[ Q^{\\pi }(s,a)\\nabla _{\\theta } \\pi(a|s) + \\pi(a|s)\\nabla _{\\theta } \\left (r(s,a,s') + \\gamma \\sum_{s'\\in \\mathcal{S}}P(s'|s,a) V^{\\pi }(s')\\right )\\right]\\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}} \\left[ Q^{\\pi }(s,a)\\nabla _{\\theta } \\pi(a|s) + \\pi(a|s) \\gamma \\sum_{s'\\in \\mathcal{S}}P(s'|s,a)\\nabla _{\\theta }V^{\\pi }(s')\\right]\\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}}  Q^{\\pi }(s,a)\\nabla _{\\theta } \\pi(a|s) + \\gamma \\sum_{a\\in \\mathcal{A}}\\pi(a|s) \\sum_{s'\\in \\mathcal{S}}P(s'|s,a)\\nabla _{\\theta }V^{\\pi }(s')\\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}}  Q^{\\pi }(s,a)\\nabla _{\\theta } \\pi(a|s) + \\gamma \\sum_{s'\\in \\mathcal{S}}P^{\\pi}(s'|s)\\nabla _{\\theta }V^{\\pi }(s')\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2651b51d-8829-47ef-a2b8-9e6a8d0527e8",
   "metadata": {},
   "source": [
    "- 求解该线性方程组的解为：<font color=brown>[详见math of RL lemma 9.2]</font>\n",
    "$$\\begin{align}\n",
    "\\nabla _{\\theta }V^{\\pi_{\\theta }}(s)\n",
    "& = \\sum_{s'\\in \\mathcal{S}}Pr^{\\pi}(s'|s)\\sum_{a\\in\\mathcal{A}}\\nabla_{\\theta}\\pi(a|s')Q(s',a)\\\\\n",
    "其中：\\\\\n",
    "Pr^{\\pi}(s'|s) & =\\sum_{k=0}^{\\infty}\\gamma^k[P^{\\pi,k }(s'|s)],\\ P^{\\pi,k }(s'|s)是s经k步转向s'的概率\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ccc57e-0a6c-4680-8e68-e0dcafb5d2c2",
   "metadata": {},
   "source": [
    "- 代入策略梯度式：\n",
    "$$\\begin{align}\n",
    "g & = \\sum_{s\\in \\mathcal{S} }d(s)\\nabla _{\\theta }V^{\\pi_{\\theta }}(s) \\\\\n",
    "& = \\sum_{s\\in \\mathcal{S} }d(s)\\sum_{s'\\in \\mathcal{S}}Pr^{\\pi}(s'|s)\\sum_{a\\in\\mathcal{A}}\\nabla_{\\theta}\\pi(a|s')Q(s',a)\\\\\n",
    "& = \\sum_{s'\\in \\mathcal{S}}\\left(\\sum_{s\\in \\mathcal{S} }d(s)Pr^{\\pi}(s'|s)\\right)\\sum_{a\\in\\mathcal{A}}\\nabla_{\\theta}\\pi(a|s')Q(s',a)\\\\\n",
    "& = \\sum_{s'\\in \\mathcal{S}}\\rho (s')\\sum_{a\\in\\mathcal{A}}\\nabla_{\\theta}\\pi(a|s')Q(s',a)\\ ,\\ 更换符号\\\\\n",
    "& = \\sum_{s\\in \\mathcal{S}}\\rho (s)\\sum_{a\\in\\mathcal{A}}\\nabla_{\\theta}\\pi(a|s)Q(s,a) \\\\\n",
    "& = \\sum_{s\\in \\mathcal{S}}\\rho (s)\\sum_{a\\in\\mathcal{A}}\\pi_{\\theta}(a|s)\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a) \\\\\n",
    "& = \\underset{s\\sim \\rho^{\\pi}, a\\sim \\pi}{E}\\nabla_{\\theta}log\\pi_{\\theta }(a|s)Q^{\\pi }(s,a) \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d073bf-0624-41c4-a2fd-e8723f9c6164",
   "metadata": {},
   "source": [
    "## III. Importance sampling形式的目标和梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde1a98a-f037-40d0-ab53-5c7d62b80771",
   "metadata": {},
   "source": [
    "### III.1 基于trajectory的期望rewards的Importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f575f18-9f6b-4ea0-9245-5f81e99bbac8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### III.1.1 目标$J(\\theta)$的Importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2878e3ff-9151-47bb-8f45-e2c858351dc8",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "J (\\theta )& = \\underset{\\tau\\sim \\pi _\\theta}{E} [R(\\tau)] = \\underset{\\tau\\sim \\pi_{\\bar \\theta}}{E} \\ \\  [\\frac{P_{\\theta}(\\tau)}{P_{\\bar \\theta}(\\tau)}R(\\tau)] \\\\\n",
    "其中：\\\\\n",
    "\\frac{P_{\\theta}(\\tau)}{P_{\\bar \\theta}(\\tau)} & = \\frac{P(s_0)\\prod_{t=0}^{H-1}\\pi_{\\theta }(a_t|s_t)P(s_{t+1}|s_t, a_t)}{P(s_0)\\prod_{t=0}^{H-1}\\pi_{\\bar \\theta }(a_t|s_t)P(s_{t+1}|s_t, a_t)} \n",
    "= \\frac{\\prod_{t=0}^{H-1}\\pi_{\\theta }(a_t|s_t)}{\\prod_{t=0}^{H-1}\\pi_{\\bar \\theta }(a_t|s_t)} \n",
    "= \\prod_{t=0}^{H-1}\\frac{\\pi_{\\theta }(a_t|s_t)}{\\pi_{\\bar \\theta }(a_t|s_t)}  \\\\\n",
    "代入上式：\\\\\n",
    "J(\\theta) &= \\underset{\\tau\\sim \\pi_{\\bar \\theta}}{E} \\ \\  [\\frac{P_{\\theta}(\\tau)}{P_{\\bar \\theta}(\\tau)}R(\\tau)]= \\underset{\\tau\\sim \\pi_{\\bar \\theta}}{E} \\ \\  [ \\prod_{t=0}^{H-1}\\frac{\\pi_{\\theta }(a_t|s_t)}{\\pi_{\\bar \\theta }(a_t|s_t)} R(\\tau)]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01a78ee-6a2e-4b48-8109-6ec107716cec",
   "metadata": {},
   "source": [
    "#### III.1.2 梯度$\\nabla_\\theta J(\\theta)$的Importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4bd951-d864-4d4e-8be4-274950464dcc",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "\\nabla _{\\theta }J(\\theta) & =  \\underset{\\tau\\sim \\pi_{\\theta}}{E} \\ \\  \\nabla _{\\theta }logP(\\tau ;\\theta )R(\\tau ) \\\\\n",
    "& = \\underset{\\tau\\sim \\pi_{\\bar \\theta}}{E} \\ \\  \\frac{P_{\\theta}(\\tau)}{P_{\\bar \\theta}(\\tau)}\\nabla _{\\theta }logP_{\\theta}(\\tau)R(\\tau )  \\\\\n",
    "& = \\underset{\\tau\\sim \\pi_{\\bar \\theta}}{E} \\ \\  \\frac{P_{\\theta}(\\tau)}{P_{\\bar \\theta}(\\tau)}\\nabla _{\\theta }log\\pi _{\\theta}(\\tau)R(\\tau )  \\\\\n",
    "&= \\underset{\\tau\\sim \\pi_{\\bar \\theta}}{E} \\ \\  \\left [\\left( \\prod_{t=0}^{H-1}\\frac{\\pi_{\\theta,t }}{\\pi_{\\bar \\theta, t }} \\right)\\left(\\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta,t }\\right)\\left(\\sum_{t=0}^{H-1}r_t\\right)\\right]\\\\\n",
    "&= \\underset{\\tau\\sim \\pi_{\\bar \\theta}}{E} \\ \\  \\left [\\left( \\prod_{t^{''}=0}^{t}\\frac{\\pi_{\\theta,t^{''} }}{\\pi_{\\bar \\theta,t^{''} }} \\right)\\left( \\prod_{t^{'''}={t+1\n",
    "}}^{H-1}\\frac{\\pi_{\\theta, t^{'''} }}{\\pi_{\\bar \\theta, t^{'''} }} \\right)\\left(\\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta,t }\\sum_{t'=t}^{H-1}r_t\\right)\\right]\\\\\n",
    "&= \\underset{\\tau\\sim \\pi_{\\bar \\theta}}{E} \\ \\  \\left [\\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta,t }(\\prod_{t^{''}=0}^{t}\\frac{\\pi_{\\theta ,t^{''} }}{\\pi_{\\bar \\theta ,t^{''}}})(\\sum_{t'=t}^{H-1}r_t)\\right]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa0287-ea54-4a24-9aad-71a224d774a6",
   "metadata": {},
   "source": [
    "- 实践中不用这个式子，因为上式中：\n",
    "$$\\prod_{t^{''}=0}^{t}\\frac{\\pi_{\\theta ,t^{''} }}{\\pi_{\\bar \\theta ,t^{''}}} \\rightarrow exponential\\ in\\  T，方差极大$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90d93c9-29ed-4f83-9dd0-e718d3c00941",
   "metadata": {},
   "source": [
    "### III.2 IS梯度的近似和它的surrogate loss目标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa92c13f-0ba2-4e62-9c53-28313fd1e281",
   "metadata": {},
   "source": [
    "#### III.2.1 用state-action的的边际分布得到Importance sampling梯度的近似"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52bdaf6-7ce0-4641-935e-83945b1d62c3",
   "metadata": {},
   "source": [
    "- 另一种推导方式：假设$(s_t, a_t)\\sim P_{\\theta}(s_t, a_t)$是state-action的marginal distribution。\n",
    "- 这种形式的优点是没有累乘，所以没有前一种推导方式中方差太大的问题。代价是得到的是策略梯度的近似估计量。实践中可用的原因是，尽管是近似估计量，但它导致的估计误差有明确的上限。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95229604-c62c-4a96-aa6d-db942fef0c03",
   "metadata": {},
   "source": [
    "- 推导式：\n",
    "$$\\begin{align}\n",
    "g & = \\underset{\\tau \\sim \\pi_{\\theta }}{E} \\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(a_t|s_t)\\sum_{t'=t}^{H-1}r(s_{t'}, a_{t'})\\right ]\\\\\n",
    " & = \\underset{\\tau \\sim \\pi_{\\theta }}{E} \\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(a_t|s_t)Q^{\\pi_{\\theta }}(s_t,a_t)\\right ]\\\\\n",
    "& = \\sum_{t=0}^{H-1}\\underset {(a_t, s_t)\\sim P_{\\theta }(a_t, s_t)}{E} \\nabla _{\\theta }log\\pi_{\\theta }(a_t|s_t)Q^{\\pi_{\\theta }}(s_t,a_t)\\\\\n",
    "& = \\sum_{t=0}^{H-1}\\sum_{a_t\\in\\mathcal{A},s_t\\in\\mathcal{S}} P_{\\theta }(a_t,s_t)\\nabla _{\\theta }log\\pi_{\\theta }(a_t|s_t)Q^{\\pi_{\\theta }}(s_t,a_t)\\\\\n",
    "& = \\sum_{t=0}^{H-1}\\sum_{a_t\\in\\mathcal{A},s_t\\in\\mathcal{S}} P_{\\bar \\theta }(a_t,s_t)\\frac{P_{\\theta }(a_t,s_t)}{P_{\\bar \\theta }(a_t,s_t)}\\nabla _{\\theta }log\\pi_{\\theta }(a_t|s_t)Q^{\\pi_{\\theta }}(s_t,a_t)\\\\\n",
    "& = \\sum_{t=0}^{H-1}\\underset {(a_t, s_t)\\sim P_{\\bar \\theta }(a_t, s_t)}{E}\\frac{P_{\\theta }(a_t,s_t)}{P_{\\bar \\theta }(a_t,s_t)}\\nabla _{\\theta }log\\pi_{\\theta }(a_t|s_t)Q^{\\pi_{\\theta }}(s_t,a_t) \\\\\n",
    "& = \\sum_{t=0}^{H-1}\\underset {(a_t, s_t)\\sim P_{\\bar \\theta }(a_t, s_t)}{E}\\frac{P_{\\theta }(s_t)\\pi_{\\theta }(a_t|s_t)}{P_{\\bar \\theta }(s_t)\\pi_{\\bar\\theta }(a_t|s_t)}\\nabla _{\\theta }log\\pi_{\\theta }(a_t|s_t)Q^{\\pi_{\\theta }}(s_t,a_t) \\\\\n",
    "& \\approx \\sum_{t=0}^{H-1}\\underset {(a_t, s_t)\\sim P_{\\bar \\theta }(a_t, s_t)}{E}\\frac{\\pi_{\\theta }(a_t|s_t)}{\\pi_{\\bar\\theta }(a_t|s_t)}\\nabla _{\\theta }log\\pi_{\\theta }(a_t|s_t)Q^{\\pi_{\\theta }}(s_t,a_t) \\\\\n",
    "& = \\underset {\\tau \\sim \\pi_{\\bar\\theta }}{E}\\sum_{t=0}^{H-1}\\frac{\\pi_{\\theta }(a_t|s_t)}{\\pi_{\\bar\\theta }(a_t|s_t)}\\nabla _{\\theta }log\\pi_{\\theta }(a_t|s_t)\\sum_{t'=t}^{H-1}r(s_{t'}, a_{t'})\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b569bf0c-544d-4c49-bd39-f39f04a1d4ea",
   "metadata": {},
   "source": [
    "- 上面推导过程的说明：\n",
    "  - 第二个等式成立是因为：将$\\sum_{t'=t}^{H-1}r(s_{t'}, a_{t'})$替换为$Q^{\\pi_{\\theta }}(s_t,a_t)$在trajectory的期望条件下是成立的。\n",
    "  - 第三个等式成立是因为：$\\nabla _{\\theta }log\\pi_{\\theta }(a_t|s_t)Q^{\\pi_{\\theta }}(s_t,a_t)$ 是$(s_t,a_t)$为变量的函数。\n",
    "    1. $\\pi_{\\theta }(a_t|s_t)$ 是$(s_t,a_t)$为变量的函数\n",
    "    2. $Q^{\\pi_{\\theta }}(s_t,a_t)$的展开式中虽然有$t'>t$的所有time-steps的影响，但是所有的$a_{t'},s_{t'}$的不确定性都被期望积掉了。\n",
    "  - 约等式中直接去掉了$\\frac{P_{\\theta}(s_t)}{P_{\\bar \\theta}(s_t)}$，这是导致最终近似估计式简化的原因，也是导致估计量近似而不是等式的原因。\n",
    "  - <font color=blue>**最后一个等式中，均值条件从 $(a_t, s_t)\\sim P_{\\bar \\theta }(a_t, s_t)换成\\tau\\sim\\pi_{\\bar\\theta}$ 之后，action function也恢复成rewards累加形式。这里的细微差异是，当取action function形式时，它是策略$\\pi_{\\theta}$ 条件下的action function；但恢复成rewards形态后，就是直接用每个time-steps上的reward加和来计算。后者只是trajectory上的state-action pair的函数，不受策略影响，因此，做importance sampling抽样估计时就可以用策略$\\pi_{\\bar \\theta}$抽样的样本来计算了。要注意这一点细微的差异。**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6f54d6-409c-41b7-ab1d-882a2a42d490",
   "metadata": {},
   "source": [
    "#### III.2.2 surrogate loss目标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e99c403-e8e0-4934-8204-770c3c811114",
   "metadata": {},
   "source": [
    "- 取梯度为IS梯度近似表达式时，为了利用现成的深度学习框架，可以用它反推一个surrogate loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6ed315-c3de-4919-bb5d-ccf3bb342d30",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "当梯度取下式时：\\\\\n",
    "g & = \\underset {\\tau \\sim \\pi_{\\bar\\theta }}{E}\\sum_{t=0}^{H-1}\\frac{\\pi_{\\theta }(a_t|s_t)}{\\pi_{\\bar\\theta }(a_t|s_t)}\\nabla _{\\theta }log\\pi_{\\theta }(a_t|s_t)\\sum_{t'=t}^{H-1}r(s_{t'}, a_{t'})\\\\\n",
    "对应的目标可以用：\\\\\n",
    "L^{IS}(\\theta ) & = \\underset {\\tau \\sim \\pi_{\\bar\\theta }}{E}\\sum_{t=0}^{H-1}\\frac{\\pi_{\\theta }(a_t|s_t)}{\\pi_{\\bar\\theta }(a_t|s_t)}\\sum_{t'=t}^{H-1}r(s_{t'}, a_{t'})\\\\\n",
    "对L(\\theta)求梯度很容易得到：\\\\\n",
    "& \\nabla_{\\theta }L^{IS}(\\theta ) = g\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc80bb5-a0da-45fe-9204-aa8fae059fcc",
   "metadata": {},
   "source": [
    "- 用梯度反推得到的surrogate loss可以从importance sampling角度来理解其含义：\n",
    "  - 严格地说，这里的surrogate loss不算真正的importance sampling loss，因为trajectory的rewards如果要取importance sampling的期望值的话，应该用$(s_t, a_t)$的边际分布来计算importance weight，但这里用的是$\\pi(a_t|s_t)$。\n",
    "  - 但一般会将这个loss用importance sampling来标记，这是因为它是真正的importance sampling loss的近似值，实践中PG使用的loss也是它。<font color=brown>[详见后文：'用Policy Gradient近似policy iteration的误差分析'部分]</font>\n",
    "$$\\begin{align}\n",
    "L^{IS}(\\theta ) & = \\underset {\\tau \\sim \\pi_{\\bar\\theta }}{E}\\sum_{t=0}^{H-1}\\frac{\\pi_{\\theta }(a_t|s_t)}{\\pi_{\\bar\\theta }(a_t|s_t)}\\sum_{t'=t}^{H-1}r(s_{t'}, a_{t'})\\\\\n",
    "& = \\sum_{t=0}^{H-1}E_{s_t\\sim P_{\\bar \\theta }(s_t)}E_{a_t\\sim {\\pi_{\\bar \\theta }}}\\frac{\\pi_{\\theta }(a_t|s_t)}{\\pi_{\\bar\\theta }(a_t|s_t)}\\sum_{t'=t}^{H-1}r(s_{t'}, a_{t'})\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a3a121-ff34-46db-9e44-61216a5aca36",
   "metadata": {},
   "source": [
    "## IV. 从policy iteration角度理解policy gradient(PG)：为什么PG能improve policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ded4b4d-3359-42eb-be03-8c4664518964",
   "metadata": {},
   "source": [
    "- <font color=blue>**思路**：</font> <font color=green>先证明policy iteration是在优化正确的RL目标。然后证明policy gradient的目标和policy iteration的目标非常接近，在一定条件下这两个目标的差异可以控制在一个明确的bound之内。既然policy iteration能让策略在迭代过程中improvement，那么与之近似的policy gradient也应该在优化正确的目标。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7f4e80-ba56-4ca0-a3e5-b5d7945022ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### III.4.1 优化目标的重新表述"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba30d1e2-062c-4a3e-a848-be9b79c76d12",
   "metadata": {},
   "source": [
    "- **claim**: \n",
    "  - 符号：$\\bar\\theta$表示new policy参数\n",
    "$$J(\\bar \\theta)-J(\\theta)=\\underset{\\tau\\sim P_{\\bar \\theta}(\\tau)}{E}\\left[\\sum_t\\gamma^tA^{\\pi_{\\theta}}(s_t,a_t) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48982ace-9ee9-4204-8360-1371962505b0",
   "metadata": {},
   "source": [
    "- 证明：\n",
    "$$\\begin{align}\n",
    "J(\\bar \\theta)-J(\\theta) & = J(\\bar \\theta)- E_{s_0\\sim P(s_0)}[V^{\\pi_{\\theta }}(s_0)]，这里P(s_0)不取决于\\theta \\\\\n",
    "& = J(\\bar \\theta)- E_{\\tau\\sim P_{\\bar\\theta}(\\tau)}[V^{\\pi_{\\theta }}(s_0)]，\\tau的分布不改变s_0的边际分布，所以\\tau可以改取任意分布，只要它的初始状态满足P(s_0)就行\\\\\n",
    "& = J(\\bar \\theta)- E_{\\tau\\sim P_{\\bar\\theta}(\\tau)}\\left[\\sum_{t=0}^{\\infty}\\gamma^tV^{\\pi_{\\theta }}(s_t)-\\sum_{t=1}^{\\infty}\\gamma^tV^{\\pi_{\\theta }}(s_t)\\right]\\\\\n",
    "& = J(\\bar \\theta)- E_{\\tau\\sim P_{\\bar\\theta}(\\tau)}\\left[\\sum_{t=0}^{\\infty}\\left(\\gamma^tV^{\\pi_{\\theta }}(s_t)-\\gamma^{t+1}V^{\\pi_{\\theta }}(s_{t+1})\\right)\\right]\\\\\n",
    "& = J(\\bar \\theta)+ E_{\\tau\\sim P_{\\bar\\theta}(\\tau)}\\left[\\sum_{t=0}^{\\infty}\\gamma^t\\left(\\gamma V^{\\pi_{\\theta }}(s_{t+1})-V^{\\pi_{\\theta }}(s_t)\\right)\\right]\\\\\n",
    "&= E_{\\tau\\sim P_{\\bar\\theta}(\\tau)}\\left[\\sum_{t=0}^{\\infty }\\gamma^tr(s_t,a_t)\\right]+ E_{\\tau\\sim P_{\\bar\\theta}(\\tau)}\\left[\\sum_{t=0}^{\\infty}\\gamma^t\\left(\\gamma V^{\\pi_{\\theta }}(s_{t+1})-V^{\\pi_{\\theta }}(s_t)\\right)\\right]\\\\\n",
    "&= E_{\\tau\\sim P_{\\bar\\theta}(\\tau)}\\left[\\sum_{t=0}^{\\infty }\\gamma^t\\left(r(s_t,a_t)+\\gamma V^{\\pi_{\\theta }}(s_{t+1})-V^{\\pi_{\\theta }}(s_t)\\right)\\right]\\\\\n",
    "& = E_{\\tau\\sim P_{\\bar\\theta}(\\tau)}\\left[\\sum_{t=0}^{\\infty }\\gamma^tA^{\\pi_{\\theta}}(s_t,a_t) \\right]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83813b69-8208-4cf6-94e3-64141310677e",
   "metadata": {},
   "source": [
    "- <font color=red>说明：上面第二个等式成立是因为$P(s_0)$是环境决定的，不受策略影响。因此期望值可以取任意分布，只要满足该分布下$s_0$的边际分布是$P(s_0)$就行。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c42643-32de-4cda-920b-280c41c8360f",
   "metadata": {},
   "source": [
    "- 该等式的意义：说明policy iteration带来的策略更新$\\bar \\theta$能最大化RL的优化目标$J(\\bar \\theta)$。\n",
    "  - 用数学表述为：\n",
    "$$\\underset{\\bar \\theta }{max}\\ E_{\\tau\\sim P_{\\bar\\theta}(\\tau)}\\left[\\sum_{t=0}^{\\infty }\\gamma^tA^{\\pi_{\\theta}}(s_t,a_t) \\right]\n",
    "\\overset{等价于}{\\Longrightarrow} \\underset{\\bar \\theta }{max}J(\\bar \\theta)-J(\\theta)\\overset{等价于}{\\Longrightarrow} \\underset{\\bar \\theta }{max}J(\\bar \\theta)$$\n",
    "  - 从而证明了，policy iteration做了正确的事情。因为policy iteration有两步：\n",
    "     1. policy evaluation是在求当前策略下的Advantage\n",
    "     2. policy improvement是用第一步得到的Advantage来更新得到新的策略\n",
    "     - <font color=norange>**这两步合起来对应的数学表达就是**：？？？为什么数学上是这个形式，再理解policy iteration在做什么</font>\n",
    "$$\\underset{\\bar \\theta }{max}\\ E_{\\tau\\sim P_{\\bar\\theta}(\\tau)}\\left[\\sum_{t=0}^{\\infty }\\gamma^tA^{\\pi_{\\theta}}(s_t,a_t) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a044b967-3450-4052-b134-d6059683b96e",
   "metadata": {},
   "source": [
    "#### III.4.2 policy gradient的优化目标和上述policy iteration的优化目标近似"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1a1fc3-edd0-4d8d-a0a1-a83fcb1b9c67",
   "metadata": {},
   "source": [
    "$$E_{\\tau\\sim P_{\\bar\\theta}(\\tau)}\\left[\\sum_{t=0}^{\\infty }\\gamma^tA^{\\pi_{\\theta}}(s_t,a_t) \\right]\n",
    "\\approx E_{\\tau\\sim P_{\\color{red}{\\theta} }(\\tau)}\\left[\\sum_t\\frac{\\pi_{\\bar \\theta }(a_t|s_t)}{\\pi_{\\theta }(a_t|s_t)} \\gamma ^tA^{\\pi_{\\theta }}(s_t,a_t)\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25433151-3aff-4224-b2cc-e82b6ca3f110",
   "metadata": {},
   "source": [
    "- 证明：\n",
    "  - 下式中的approximation成立是因为：当新旧策略差异不大的时候，可以认为$P_{\\bar \\theta}\\approx P_{\\theta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183eb5e7-7529-4e88-81bd-1a787837433b",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "E_{\\tau\\sim P_{\\bar\\theta}(\\tau)}\\left[\\sum_{t=0}^{\\infty }\\gamma^tA^{\\pi_{\\theta}}(s_t,a_t) \\right]\n",
    "&= \\sum_tE_{s_t\\sim P_{\\bar \\theta }(s_t)}[E_{a_t\\sim \\pi_{\\bar \\theta }(a_t|s_t)}[\\gamma ^tA^{\\pi_{\\theta }}(s_t,a_t)]]\\\\\n",
    "&= \\sum_tE_{s_t\\sim P_{\\bar \\theta }(s_t)}[E_{a_t\\sim \\pi_{\\theta }(a_t|s_t)}[\\frac{\\pi_{\\bar \\theta }(a_t|s_t)}{\\pi_{\\theta }(a_t|s_t)} \\gamma ^tA^{\\pi_{\\theta }}(s_t,a_t)]], importance\\ sampling\\\\\n",
    "&\\approx \\sum_tE_{s_t\\sim P_{\\color{red}{\\theta} }(s_t)}[E_{a_t\\sim \\pi_{\\theta }(a_t|s_t)}[\\frac{\\pi_{\\bar \\theta }(a_t|s_t)}{\\pi_{\\theta }(a_t|s_t)} \\gamma ^tA^{\\pi_{\\theta }}(s_t,a_t)]]\\\\\n",
    "& = E_{\\tau\\sim P_{\\color{red}{\\theta} }(\\tau)}\\left[\\sum_t\\frac{\\pi_{\\bar \\theta }(a_t|s_t)}{\\pi_{\\theta }(a_t|s_t)} \\gamma ^tA^{\\pi_{\\theta }}(s_t,a_t)\\right]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcd7463-ba50-4db7-8619-67f60ceaef98",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### III.4.3 用policy gradient近似policy iteration的误差分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2e0a07-39d3-4f73-bbb4-bce6c18f0679",
   "metadata": {},
   "source": [
    "- 当策略是deterministic时的bound分析\n",
    "  - <img src='pics/bound_deterministic.png' width='70%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06dca75-f3cf-405a-9d97-318119b56f5d",
   "metadata": {},
   "source": [
    "- 将策略一般化为任意分布时的bound分析\n",
    "  - <img src='pics/bound_general.png' width='75%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27ba96f-d5c5-4c4b-bc70-aaf867a401dd",
   "metadata": {},
   "source": [
    "- 根据下面的分析可知：只要$\\epsilon$的值足够小，那么：\n",
    "$$\\sum_tE_{s_t\\sim P_{\\color{green}{\\bar \\theta}}(s_t)}\\left[E_{a_t\\sim \\pi_{\\theta }(a_t|s_t)}[\\frac{\\pi_{\\bar \\theta }(a_t|s_t)}{\\pi_{\\theta }(a_t|s_t)} \\gamma ^tA^{\\pi_{\\theta }}(s_t,a_t)]\\right] \n",
    "\\approx \\sum_tE_{s_t\\sim P_{\\color{red}{\\theta}}(s_t)}\\left[E_{a_t\\sim \\pi_{\\theta }(a_t|s_t)}[\\frac{\\pi_{\\bar \\theta }(a_t|s_t)}{\\pi_{\\theta }(a_t|s_t)} \\gamma ^tA^{\\pi_{\\theta }}(s_t,a_t)]\\right]\\\\\n",
    "$$\n",
    "    - <img src='pics/policy_gradient_approx.png' width='75%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78f4a1c-cf1d-403f-83d5-ea7ebf0eba28",
   "metadata": {},
   "source": [
    "#### III.4.4 以误差bound为限制条件重写policy gradient的目标表达式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfad6b3-50f4-4494-b9d8-3aa8b86e39e1",
   "metadata": {},
   "source": [
    "- 从上节的误差bound分析可知，当$\\epsilon$很小时，下述有限制的优化目标一定可以improve$J(\\bar\\theta)-J(\\theta)$。也就是说，policy gradient的目标在加上约束之后，就和policy iterate的目标完全一致了。\n",
    "$$\\begin{align}\n",
    "& \\underset{\\bar \\theta}{max} \\sum_tE_{s_t\\sim P_{\\theta}(s_t)}\\left[E_{a_t\\sim \\pi_{\\theta }(a_t|s_t)}[\\frac{\\pi_{\\bar \\theta }(a_t|s_t)}{\\pi_{\\theta }(a_t|s_t)} \\gamma ^tA^{\\pi_{\\theta }}(s_t,a_t)]\\right]\\\\\n",
    "& s.t. |\\pi_{\\bar\\theta }(a_t|s_t) - \\pi_{\\theta }(a_t|s_t)| \\le \\epsilon \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643ea015-f5b9-497a-9307-8ae35a4739dc",
   "metadata": {},
   "source": [
    "### III.5 带约束的policy gradient算法目标优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e3d3bc-78c5-4229-9ce4-f3ef8fe04278",
   "metadata": {},
   "source": [
    "#### III.5.1 Natural policy gradient将约束改为数学上更好处理的形式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84234a7f-4799-47e5-9c8c-0a2de055041a",
   "metadata": {},
   "source": [
    "- 前面有约束的PG优化目标的约束形式会让对应的优化问题不好处理，而natural PG算法提出的约束条件形式在数学上更好处理。\n",
    "- 根据Pinsker's inequality, variational difference与KL divergence之间有下述关系：<font color=brown>[证明见附页，ref deepseek]</font>\n",
    "$$|\\pi_{\\bar \\theta}(a_t|s_t)-\\pi_{\\theta}(a_t|s_t)| \\le \\sqrt{0.5*D_{KL}(\\pi_{\\bar \\theta}(a_t|s_t)||\\pi_{\\theta}(a_t|s_t))}$$\n",
    "  - <font color=green>因此将前述约束条件转化为natural PG使用的KL divergence约束是同样有效的。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43f74fd-0a88-4e6c-8766-8a186218ef2d",
   "metadata": {},
   "source": [
    "- <font color=green>**当$\\epsilon$很小时，下述有限制的优化目标同样能保证improve$J(\\bar\\theta)-J(\\theta)$。**</font>\n",
    "  - 使用KL divergence的好处是，其表达式方便求梯度，而使用绝对值的约束形式无法处处可导，因此这种方式在数学上更好处理。\n",
    "  - 等价的优化目标：\n",
    "$$\\begin{align}\n",
    "& \\underset{\\bar \\theta}{max} \\sum_tE_{s_t\\sim P_{\\theta}(s_t)}\\left[E_{a_t\\sim \\pi_{\\theta }(a_t|s_t)}[\\frac{\\pi_{\\bar \\theta }(a_t|s_t)}{\\pi_{\\theta }(a_t|s_t)} \\gamma ^tA^{\\pi_{\\theta }}(s_t,a_t)]\\right]\\\\\n",
    "& s.t. D_{KL}(\\pi_{\\bar\\theta }(a_t|s_t) || \\pi_{\\theta }(a_t|s_t)) \\le \\epsilon \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9db8f7-5f1f-455e-be9b-8d529b2963a6",
   "metadata": {},
   "source": [
    "#### III.5.2 泰勒一阶展开求解优化问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4c1d4d-4d5a-4420-9d2e-4633da25ce29",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "将优化目标简记为：\\\\\n",
    "\\underset{\\bar \\theta}{max} f(\\bar \\theta ) & = \\underset{\\bar \\theta}{max} \\sum_tE_{s_t\\sim P_{\\theta}(s_t)}\\left[E_{a_t\\sim \\pi_{\\theta }(a_t|s_t)}[\\frac{\\pi_{\\bar \\theta }(a_t|s_t)}{\\pi_{\\theta }(a_t|s_t)} \\gamma ^tA^{\\pi_{\\theta }}(s_t,a_t)]\\right]\\\\\n",
    "f(\\bar \\theta )泰勒一阶展开：\\\\\n",
    "f(\\bar\\theta ) & \\approx f(\\bar\\theta _0) + \\nabla_{\\bar\\theta }f(\\bar\\theta)|_{\\bar\\theta=\\bar\\theta _0}(\\bar\\theta -\\bar\\theta _0)\\\\\n",
    "取\\bar\\theta _0=\\theta,有：\\\\\n",
    "f(\\bar\\theta ) & \\approx f(\\theta) + \\nabla_{\\theta }f(\\theta)(\\bar\\theta -\\theta)\\\\\n",
    "因为f(\\theta )与\\bar\\theta 无关：\\\\\n",
    "\\underset{\\bar \\theta}{max} f(\\bar \\theta ) & \\approx \\underset{\\bar \\theta}{max}\\nabla_{\\theta }f(\\theta)(\\bar\\theta -\\theta)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9410d5-8be0-4faf-a6a5-26e3c3cb5ff0",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "\\nabla_{\\bar\\theta }f(\\bar\\theta) \n",
    "& = \\nabla_{\\bar\\theta }\\sum_tE_{s_t\\sim P_{\\theta}(s_t)}\\left[E_{a_t\\sim \\pi_{\\theta }(a_t|s_t)}[\\frac{\\pi_{\\bar \\theta}(a_t|s_t)}{\\pi_{\\theta }(a_t|s_t)} \\gamma ^tA^{\\pi_{\\theta }}(s_t,a_t)]\\right]\\\\\n",
    "& = \\sum_tE_{s_t\\sim P_{\\theta}(s_t)}\\left[E_{a_t\\sim \\pi_{\\theta }(a_t|s_t)}[\\frac{\\nabla_{\\bar\\theta }\\pi_{\\bar \\theta}(a_t|s_t)}{\\pi_{\\theta }(a_t|s_t)} \\gamma ^tA^{\\pi_{\\theta }}(s_t,a_t)]\\right]\\\\\n",
    "& = \\sum_tE_{s_t\\sim P_{\\theta}(s_t)}\\left[E_{a_t\\sim \\pi_{\\theta }(a_t|s_t)}[\\frac{\\pi_{\\bar \\theta}(a_t|s_t)}{\\pi_{\\theta }(a_t|s_t)} \\nabla_{\\bar\\theta }log\\pi_{\\bar \\theta}(a_t|s_t)\\gamma ^tA^{\\pi_{\\theta }}(s_t,a_t)]\\right]\\\\\n",
    "& {\\color{blue}{正好是Importance\\ sampling形式的策略梯度}}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f58acc8-0563-40e7-ae23-fbc8adf6d58b",
   "metadata": {},
   "source": [
    "- 取$\\bar\\theta _0=\\theta$时,有：\n",
    "$$\n",
    "\\begin{align}\n",
    "f(\\theta ) & =f(\\bar \\theta = \\theta ) \\\\\n",
    "&   = \\sum_tE_{s_t\\sim P_{\\theta}(s_t)}\\left[E_{a_t\\sim \\pi_{\\theta }(a_t|s_t)}[\\frac{\\pi_{\\bar \\theta=\\theta}(a_t|s_t)}{\\pi_{\\theta }(a_t|s_t)} \\gamma ^tA^{\\pi_{\\theta }}(s_t,a_t)]\\right]\\\\\n",
    "& = \\sum_tE_{s_t\\sim P_{\\theta}(s_t)}E_{a_t\\sim \\pi_{\\theta }(a_t|s_t)}[\\gamma ^tA^{\\pi_{\\theta }}(s_t,a_t)]\\\\\n",
    "& = E_{\\tau\\sim P_{\\theta}(\\tau)}\\sum_t[\\gamma ^tA^{\\pi_{\\theta }}(s_t,a_t)]\\\\\n",
    "& = J(\\theta )\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286d15cd-4a8b-4b61-a4ca-d4a82d917f50",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "\\nabla_{\\theta }f(\\theta) & = \\nabla_{\\bar\\theta }f(\\bar\\theta) |_{\\bar\\theta =\\theta } \\\\\n",
    "& = \\sum_tE_{s_t\\sim P_{\\theta}(s_t)}\\left[E_{a_t\\sim \\pi_{\\theta }(a_t|s_t)}[\\frac{\\pi_{\\bar \\theta=\\theta}(a_t|s_t)}{\\pi_{\\theta }(a_t|s_t)} \\nabla_{\\bar\\theta }log\\pi_{\\bar \\theta}(a_t|s_t)|_{\\bar \\theta=\\theta}\\gamma ^tA^{\\pi_{\\theta }}(s_t,a_t)]\\right]\\\\\n",
    "& = \\sum_tE_{s_t\\sim P_{\\theta}(s_t)}\\left[E_{a_t\\sim \\pi_{\\theta }(a_t|s_t)}[ \\nabla_{\\theta }log\\pi_{\\theta}(a_t|s_t)\\gamma ^tA^{\\pi_{\\theta }}(s_t,a_t)]\\right]\\\\\n",
    "& = \\nabla_{\\theta }J(\\theta )\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13cfba6-8280-4b28-be98-dc5529d47751",
   "metadata": {},
   "source": [
    "- 泰勒一阶展开后，原优化问题转化为：\n",
    "$$\\begin{align}\n",
    "& \\underset{\\bar \\theta}{max} f(\\bar \\theta ) \\approx \\underset{\\bar \\theta}{max}\\nabla_{\\theta }f(\\theta)^T(\\bar\\theta -\\theta)=\\nabla_{\\theta }J(\\theta )^T(\\bar\\theta -\\theta)\\\\\n",
    "& s.t. D_{KL}(\\pi_{\\bar\\theta }(a_t|s_t) || \\pi_{\\theta }(a_t|s_t)) \\le \\epsilon \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9262f8-d11b-4fb3-afc8-06f44e60b48a",
   "metadata": {},
   "source": [
    "#### III.5.3 Natrual PG求解方法：用taylor一阶展开求解带约束的优化目标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf85560c-e985-43fd-adf6-505ac06c9667",
   "metadata": {},
   "source": [
    "- <font color=norange>**思路**：</font>\n",
    "  1. gradient ascent方法是有约束的优化问题的solution，并且该优化问题中的约束是用L2 norm表达的: $\\left\\|\\bar \\theta-\\theta\\right\\|^2 =  (\\bar \\theta-\\theta)^TI(\\bar \\theta-\\theta) \\le \\epsilon $\n",
    "  2. KL divergence可以近似表达为Hessian matrix为Fisher-information matrix的矩阵形态：$D_{KL}(\\pi_{\\bar\\theta}||\\pi_{\\theta}) \\approx \\frac{1}{2}(\\bar \\theta-\\theta)^TF(\\bar \\theta-\\theta)$\n",
    "  3. 利用上面两种约束的相似性，可以将KL divergence处理到Gradient Ascend方法的step size中求解目标优化问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372891c4-edf2-41ff-aa67-2f8d33e973c0",
   "metadata": {},
   "source": [
    "1. Gradient Ascent的learning rate控制step size，本质上是给被迭代的参数加上了distance 限制。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565511cf-eab4-49af-85aa-dfebe4a95f64",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "\\underset{\\bar \\theta}{max} L(\\bar \\theta )&=\\nabla_{\\theta }J(\\theta)^T(\\bar \\theta-\\theta)\\\\\n",
    "\\nabla_{\\bar \\theta }L(\\bar \\theta ) &= \\nabla_{\\theta }J(\\theta)\\\\\n",
    "对应的GA迭代式: \\\\\n",
    "\\bar\\theta & = \\theta +\\alpha \\nabla_{\\theta }J(\\theta) \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b8959e-f141-45c5-82b7-995a3cde8b0c",
   "metadata": {},
   "source": [
    "2. <font color=blue>**Gradient Ascent是有约束的优化问题的解**：</font>\n",
    "    - 按照下面方式可以找到对应的有约束优化问题：<font color=brown>[详见附页 ref deepseek]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bb18ba-7c5d-4896-ad94-8e19be7afff4",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "取\\epsilon 与\\alpha 的关系式：\\\\\n",
    "\\alpha &= \\sqrt{\\frac{\\epsilon }{\\left \\| \\nabla_\\theta J(\\theta ) \\right \\| ^2}} \\nabla_\\theta J(\\theta )\\\\\n",
    "迭代式是下面有约束目标问题的解：\\\\\n",
    "\\underset{\\bar \\theta}{max} L(\\bar \\theta )&=\\nabla_{\\theta }J(\\theta)^T(\\bar \\theta-\\theta)\\\\\n",
    "s.t. \\ \\ \\left\\|\\bar \\theta-\\theta\\right\\|^2& \\le \\epsilon \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4cbeae-89b3-41ed-aef1-9bcaea8e7a9e",
   "metadata": {},
   "source": [
    "3. 可以将KL divergence约束转化为类似L2 norm的形态\n",
    "   - $D_{KL}(\\pi_{\\bar\\theta}||\\pi_{\\theta}) \\approx \\frac{1}{2}(\\bar \\theta-\\theta)^TF(\\bar \\theta-\\theta)$  <font color=brown>[证明详见附页 ref deepseek]</font>\n",
    "   - 其中，$F=E_{\\pi_\\theta}[\\nabla_\\theta log \\pi_\\theta (a|s)\\nabla_\\theta log \\pi_\\theta (a|s)^T]$，这个值可以抽样估计"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcd095d-57ee-43a2-845a-6fc12e81a5d4",
   "metadata": {},
   "source": [
    "4. 将原目标转化为Gradient ascent solution对应的目标，及其对应的GA solution为：\n",
    "$$\\begin{align}\n",
    "\\underset{\\bar \\theta}{max}& L(\\bar \\theta )=\\nabla_{\\theta }J(\\theta)^T(\\bar \\theta-\\theta)\\\\\n",
    "s.t. \\ \\ &\\frac{1}{2}(\\bar \\theta-\\theta)^TF(\\bar \\theta-\\theta) \\le \\epsilon \\\\\n",
    "GA\\ solution：\\\\\n",
    "\\theta '& = \\theta +\\alpha F^{-1}\\nabla _\\theta J(\\theta )\\\\\n",
    "\\alpha  & = \\sqrt{\\frac{2\\epsilon }{ \\nabla_\\theta J(\\theta )^TF\\nabla_\\theta J(\\theta ) }} \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e06bc1-4e04-4a3a-995c-49c6a892e78f",
   "metadata": {},
   "source": [
    "#### III.5.4 TRPO提出了更高效的求解该优化问题的方法：共轭梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703a3904-63d2-4938-a9c7-1631cf809479",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86dc5227-d77d-43b2-aa0d-c048d44167a3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "871e8ced-a162-444c-b7e2-8b216fbefefb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec299974-e76b-4383-8c6b-5da46c0777fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d801c3e1-46c1-4c8b-b1cf-ee504471e69a",
   "metadata": {},
   "source": [
    "### III.6 PPO进一步简化优化目标，并用IS求解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3db9bf-7cc9-4aac-b0e7-1d805c49a52d",
   "metadata": {},
   "source": [
    "- 梯度表达式直接对应的‘pseudo-loss’优化目标\n",
    "$$\\begin{align}\n",
    "g = E_{\\tau }\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t|s_t)A_t\\right ] \n",
    "& \\Rightarrow \n",
    "J(\\theta ) = E_{\\tau }\\left[ \\sum_{t=0}^{H-1}log\\pi_{\\theta }(u_t|s_t)A_t\\right ]\\\\\n",
    "\\\\\n",
    "\\hat g= \\frac{1}{N} \\sum _{i=1 }^N\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t^{(i)}|s_t^{(i)})A_t^{(i)}\\right ] \n",
    " & \\Rightarrow \n",
    "\\hat J(\\theta ) = \\frac{1}{N} \\sum _{i=1 }^N\\left[ \\sum_{t=0}^{H-1}log\\pi_{\\theta }(u_t^{(i)}|s_t^{(i)})A_t^{(i)}\\right ]\n",
    "\\end{align}$$\n",
    "  - 实践中不用，因为Advantage波动大导致策略迭代的波动大。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c961b37-6842-48e6-b829-af50e92fb7ac",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38b1964f-2f09-49f7-b647-36712804dce8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eaba845a-38ea-4126-9b98-8d3bae4d90fc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
