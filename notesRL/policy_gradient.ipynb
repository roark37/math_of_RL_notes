{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f6db28e-ab4b-4cbd-b3ba-104ec91dc37a",
   "metadata": {},
   "source": [
    "# Policy Gradient的推导"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce413bf5-2a0b-4a9a-98c3-f9af3c90fb6b",
   "metadata": {},
   "source": [
    "- <font color=brown>符号说明：文中 $a_t和u_t$ 混用，都表示t时刻的action。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3935311-9d64-4aff-b2a0-325eee035f2e",
   "metadata": {},
   "source": [
    "## I. 原目标：最大化状态的效用期望值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7444f12b-d9fd-4306-aec7-37d803c29b43",
   "metadata": {},
   "source": [
    "### I.1 两种常见的目标展开方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a65a52f-ca53-4d85-88a5-5059ebedc45a",
   "metadata": {},
   "source": [
    "1. trajectory的期望rewards：\n",
    "$$U(\\theta)  = E_{\\tau }R(\\tau|\\pi_{\\theta}) = \\sum_{\\tau }P(\\tau ;\\theta )R(\\tau )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1982c6b1-225f-4ea3-9576-f072edd8525d",
   "metadata": {},
   "source": [
    "2. 不同状态价值的均值：\n",
    "$$U(\\theta) = E_{s\\sim d(s)}[V^{\\pi_{\\theta }}(s)]=\\sum_{s\\in \\mathcal{S} }d(s)V^{\\pi_{\\theta }}(s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ebabcd-44ab-4eeb-b7ef-0c1b6f514bf1",
   "metadata": {},
   "source": [
    "### I.2 目标展开式暗含的假设条件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924eccb8-c109-4253-8773-4bfd6b64b7fd",
   "metadata": {},
   "source": [
    "- 期望值合理的前提是随机变量的分布稳定。也就是说，trajectory展开式中，假设了trajectory的分布稳定；状态价值展开式中，假设了状态s的分布稳定。<font color=red>但这两个条件在现实条件下都不成立。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bc03e9-2964-4aa7-8a1f-84a8367a3fec",
   "metadata": {},
   "source": [
    "- 在状态价值展开式中，理论上，如果agent与环境交互的次数极大的条件下，$d(s)$可以用策略对应的stationary distribution。但是这个条件在真实环境中通常也很难满足。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffd7646-fc5b-436c-836e-0779dc70b683",
   "metadata": {},
   "source": [
    "## II. 梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aef1420-a783-42c2-9a63-44869644fac5",
   "metadata": {},
   "source": [
    "### II.1 用trajectory的期望rewards推导"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977c9981-d37b-4d84-ac7d-c54302233a62",
   "metadata": {},
   "source": [
    "#### II.1.1 梯度的基本公式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42cdc9f-5c29-4904-9428-550052c6d534",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "\\nabla _{\\theta }U(\\theta ) \n",
    "& = \\nabla _{\\theta } \\sum_{\\tau }P(\\tau ;\\theta )R(\\tau ) \\\\\n",
    "& =  \\sum_{\\tau }\\nabla _{\\theta }P(\\tau ;\\theta )R(\\tau ), {\\color{green}{这步是因为\\theta带给\\tau的随机性都体现在P(\\tau;\\theta)里面}} \\\\\n",
    "& =  \\sum_{\\tau }P(\\tau ;\\theta )\\frac{\\nabla _{\\theta }P(\\tau ;\\theta )}{P(\\tau ;\\theta )}R(\\tau ) \\\\\n",
    "& =  E_{\\tau }\\nabla _{\\theta }logP(\\tau ;\\theta )R(\\tau ) \\\\\n",
    "\\\\\n",
    "\\nabla _{\\theta }logP(\\tau ;\\theta ) \n",
    "& = \\nabla _{\\theta }log\\left[ d(s_0)\\prod_{t=0}^{H-1} P(s_{t+1}|s_t, u_t)*\\pi_{\\theta }(u_t|s_t) \\right ]\\\\\n",
    "& = \\nabla _{\\theta }\\left[logd(s_0) + \\sum_{t=0}^{H-1}logP(s_{t+1}|s_t, u_t) + \\sum_{t=0}^{H-1}log\\pi_{\\theta }(u_t|s_t)\\right ]\\\\\n",
    "& = \\underset{0}{ \\underbrace{\\nabla _{\\theta }logd(s_0)}}  + \\underset{0}{\\underbrace{\\nabla _{\\theta }\\sum_{t=0}^{H-1}logP(s_{t+1}|s_t, u_t)}} + \\nabla _{\\theta }\\sum_{t=0}^{H-1}log\\pi_{\\theta }(u_t|s_t)\\\\\n",
    "& = \\nabla _{\\theta }\\sum_{t=0}^{H-1}log\\pi_{\\theta }(u_t|s_t) \\\\\n",
    "\\\\\n",
    "取梯度：g & = \\nabla _{\\theta }U(\\theta )\\\\\n",
    "& =  E_{\\tau }\\nabla _{\\theta }logP(\\tau ;\\theta )R(\\tau ) \\\\\n",
    "& = E_{\\tau }\\left[\\nabla _{\\theta }\\sum_{t=0}^{H-1}log\\pi_{\\theta }(u_t|s_t)R(\\tau )\\right ]\\\\\n",
    "& = E_{\\tau }\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t|s_t)\\sum_{t=0}^{H-1}r(s_t, u_t)\\right ]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2012fe80-7107-4643-9779-6fab919edae5",
   "metadata": {},
   "source": [
    "#### II.1.2 rewards to go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d5c2b1-8194-4e71-a7fd-add002873b6b",
   "metadata": {},
   "source": [
    "$$g = E_{\\tau }\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t|s_t)\\sum_{t'=t}^{H-1}r(s_{t'}, u_{t'})\\right ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63be9e4-f19d-4333-9b48-68d73ebba08b",
   "metadata": {},
   "source": [
    "#### II.1.3 baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accfd6a0-59b4-477b-9d94-c222715e97a9",
   "metadata": {},
   "source": [
    "- 只要b与$\\theta$无关，则下式成立：\n",
    "$$\\begin{align}\n",
    "E_{\\tau }\\nabla _{\\theta }logP(\\tau ;\\theta )b& = \\nabla _{\\theta } \\sum_{\\tau }P(\\tau ;\\theta )b \\\\\n",
    "& = b\\nabla _{\\theta } \\sum_{\\tau }P(\\tau ;\\theta )\\\\\n",
    "& = b\\nabla _{\\theta }1 = 0\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9ee64c-8acf-410c-81fa-a22f94371fd6",
   "metadata": {},
   "source": [
    "- 此时，将b代入策略梯度有：\n",
    "$$\\begin{align}\n",
    "g & =  E_{\\tau }\\nabla _{\\theta }logP(\\tau ;\\theta )[R(\\tau ) - b]\\\\\n",
    "&= E_{\\tau }\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t|s_t)\\left( \\sum_{t'=t}^{H-1}r(s_{t'}, u_{t'})-b(s_t)\\right)\\right ]\n",
    "\\end{align}$$\n",
    "  - 即使b是states的函数，等式仍然成立，只要b(s)不受参数的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb59c94-45e5-4274-913e-05173adb38f5",
   "metadata": {},
   "source": [
    "- 为了降低估计量的方差，b(s)的sub-optimal取值为$b(s)=V^{\\pi}(s)$。将action advantage记为$A_t=Q^{\\pi}(s_t,u_t)-V^{\\pi}(s_t)$\n",
    "$$\\begin{align}\n",
    "g & =  E_{\\tau }\\nabla _{\\theta }logP(\\tau ;\\theta )[R(\\tau ) - b]\\\\\n",
    "&= E_{\\tau }\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t|s_t)\\left( \\sum_{t'=t}^{H-1}r(s_{t'}, u_{t'})-b(s_t)\\right)\\right ]\\\\\n",
    "&= E_{\\tau }\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t|s_t)A_t\\right ]\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1600a745-a729-4bd2-9ae5-b32a20353b21",
   "metadata": {},
   "source": [
    "- <font color=red>此时，策略梯度的估计量虽然降低了方差，但由于b的取值受策略的影响，从而失去了无偏性。</font>\n",
    "$$\\begin{align}\n",
    "\\hat g & = \\frac{1}{N} \\sum _{i=1 }^N\\nabla _{\\theta }logP(\\tau_i;\\theta )[R(\\tau_i) - b]\\\\\n",
    "&= \\frac{1}{N} \\sum _{i=1 }^N\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t^{(i)}|s_t^{(i)})\\left( \\sum_{t'=t}^{H-1}r(s_{t'}^{(i)}, u_{t'}^{(i)})-V^{\\pi}(s_t^{(i)})\\right)\\right ]\\\\\n",
    "&= \\frac{1}{N} \\sum _{i=1 }^N\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t^{(i)}|s_t^{(i)})\\hat A_t\\right ]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee18dfb1-6eaa-4267-bc07-9c5cf0f1f4ae",
   "metadata": {},
   "source": [
    "### II.2 用起始状态的state function期望值推导"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21df144-7c48-482c-a44f-06f4c0a309eb",
   "metadata": {},
   "source": [
    "- 参考math of RL page210"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06e8aa3-0889-4272-9bef-55aba1d20546",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1931b14-5ae1-487f-8849-67e6475bdd17",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2651b51d-8829-47ef-a2b8-9e6a8d0527e8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0ccc57e-0a6c-4680-8e68-e0dcafb5d2c2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8435d798-3a4b-434a-8738-61035205804c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1aacd52-a58e-4f6e-9f4e-5b222a4c4643",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adb9099b-d780-47b7-bd94-e1f126555e96",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05d073bf-0624-41c4-a2fd-e8723f9c6164",
   "metadata": {},
   "source": [
    "## III. 优化目标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde1a98a-f037-40d0-ab53-5c7d62b80771",
   "metadata": {},
   "source": [
    "### III.1 Importance sampling形式的优化目标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f575f18-9f6b-4ea0-9245-5f81e99bbac8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### III.1.1 用原目标推导"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2878e3ff-9151-47bb-8f45-e2c858351dc8",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "J (\\theta )& = \\underset{\\tau\\sim \\pi _\\theta}{E} [R(\\tau)] = \\underset{\\tau\\sim \\pi_{\\bar \\theta}}{E} \\ \\  [\\frac{P_{\\theta}(\\tau)}{P_{\\bar \\theta}(\\tau)}R(\\tau)] \\\\\n",
    "其中：\\\\\n",
    "\\frac{P_{\\theta}(\\tau)}{P_{\\bar \\theta}(\\tau)} & = \\frac{P(s_0)\\prod_{t=0}^{H-1}\\pi_{\\theta }(a_t|s_t)P(s_{t+1}|s_t, a_t)}{P(s_0)\\prod_{t=0}^{H-1}\\pi_{\\bar \\theta }(a_t|s_t)P(s_{t+1}|s_t, a_t)} \n",
    "= \\frac{\\prod_{t=0}^{H-1}\\pi_{\\theta }(a_t|s_t)}{\\prod_{t=0}^{H-1}\\pi_{\\bar \\theta }(a_t|s_t)} \n",
    "= \\prod_{t=0}^{H-1}\\frac{\\pi_{\\theta }(a_t|s_t)}{\\pi_{\\bar \\theta }(a_t|s_t)}  \\\\\n",
    "代入上式：\\\\\n",
    "J(\\theta) &= \\underset{\\tau\\sim \\pi_{\\bar \\theta}}{E} \\ \\  [\\frac{P_{\\theta}(\\tau)}{P_{\\bar \\theta}(\\tau)}R(\\tau)]= \\underset{\\tau\\sim \\pi_{\\bar \\theta}}{E} \\ \\  [ \\prod_{t=0}^{H-1}\\frac{\\pi_{\\theta }(a_t|s_t)}{\\pi_{\\bar \\theta }(a_t|s_t)} R(\\tau)]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4bd951-d864-4d4e-8be4-274950464dcc",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "\\nabla _{\\theta }J(\\theta) & =  \\underset{\\tau\\sim \\pi_{\\theta}}{E} \\ \\  \\nabla _{\\theta }logP(\\tau ;\\theta )R(\\tau ) \\\\\n",
    "& = \\underset{\\tau\\sim \\pi_{\\bar \\theta}}{E} \\ \\  \\frac{P_{\\theta}(\\tau)}{P_{\\bar \\theta}(\\tau)}\\nabla _{\\theta }logP_{\\theta}(\\tau)R(\\tau )  \\\\\n",
    "& = \\underset{\\tau\\sim \\pi_{\\bar \\theta}}{E} \\ \\  \\frac{P_{\\theta}(\\tau)}{P_{\\bar \\theta}(\\tau)}\\nabla _{\\theta }log\\pi _{\\theta}(\\tau)R(\\tau )  \\\\\n",
    "&= \\underset{\\tau\\sim \\pi_{\\bar \\theta}}{E} \\ \\  \\left [\\left( \\prod_{t=0}^{H-1}\\frac{\\pi_{\\theta,t }}{\\pi_{\\bar \\theta, t }} \\right)\\left(\\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta,t }\\right)\\left(\\sum_{t=0}^{H-1}r_t\\right)\\right]\\\\\n",
    "&= \\underset{\\tau\\sim \\pi_{\\bar \\theta}}{E} \\ \\  \\left [\\left( \\prod_{t^{''}=0}^{t}\\frac{\\pi_{\\theta,t^{''} }}{\\pi_{\\bar \\theta,t^{''} }} \\right)\\left( \\prod_{t^{'''}={t+1\n",
    "}}^{H-1}\\frac{\\pi_{\\theta, t^{'''} }}{\\pi_{\\bar \\theta, t^{'''} }} \\right)\\left(\\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta,t }\\sum_{t'=t}^{H-1}r_t\\right)\\right]\\\\\n",
    "&= \\underset{\\tau\\sim \\pi_{\\bar \\theta}}{E} \\ \\  \\left [\\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta,t }(\\prod_{t^{''}=0}^{t}\\frac{\\pi_{\\theta ,t^{''} }}{\\pi_{\\bar \\theta ,t^{''}}})(\\sum_{t'=t}^{H-1}r_t)\\right]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa0287-ea54-4a24-9aad-71a224d774a6",
   "metadata": {},
   "source": [
    "- 实践中不用这个式子，因为上式中：\n",
    "$$\\prod_{t^{''}=0}^{t}\\frac{\\pi_{\\theta ,t^{''} }}{\\pi_{\\bar \\theta ,t^{''}}} \\rightarrow exponential\\ in\\  T，方差极大$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94885361-634b-48d6-a91a-8f51a3236012",
   "metadata": {},
   "source": [
    "#### III.1.2 IS目标的一阶近似"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52bdaf6-7ce0-4641-935e-83945b1d62c3",
   "metadata": {},
   "source": [
    "- 另一种推导方式：假设$(s_t, a_t)\\sim P_{\\theta}(s_t, a_t)$是state-action的marginal distribution。\n",
    "- 可以重新将优化目标表示为：\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95229604-c62c-4a96-aa6d-db942fef0c03",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dba13830-203a-48d2-aedd-22e0b760f677",
   "metadata": {},
   "source": [
    "### III.2 梯度表达式直接对应的‘pseudo-loss’优化目标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a243a225-9da5-4051-9e3f-d0cceabbbb15",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "g = E_{\\tau }\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t|s_t)A_t\\right ] \n",
    "& \\Rightarrow \n",
    "J(\\theta ) = E_{\\tau }\\left[ \\sum_{t=0}^{H-1}log\\pi_{\\theta }(u_t|s_t)A_t\\right ]\\\\\n",
    "\\\\\n",
    "\\hat g= \\frac{1}{N} \\sum _{i=1 }^N\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t^{(i)}|s_t^{(i)})A_t^{(i)}\\right ] \n",
    " & \\Rightarrow \n",
    "\\hat J(\\theta ) = \\frac{1}{N} \\sum _{i=1 }^N\\left[ \\sum_{t=0}^{H-1}log\\pi_{\\theta }(u_t^{(i)}|s_t^{(i)})A_t^{(i)}\\right ]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c031cf7-dca3-45b5-8e85-8abf53886162",
   "metadata": {},
   "source": [
    "### III.3 PPO中使用的surrogate loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b569bf0c-544d-4c49-bd39-f39f04a1d4ea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d591e9a-0cfc-4072-a958-75e05d46d1c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c66ae791-a577-426b-8a41-e197387b6bb6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
