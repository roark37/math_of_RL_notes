{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20b64c91",
   "metadata": {},
   "source": [
    "# TD learning+价值函数估计"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b8a82c-579f-4701-9b07-29dd04673989",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## I. <font color=blue>价值函数估计法的出发点：</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965b7ff5-3a3d-4821-bd38-c1d4106f661d",
   "metadata": {},
   "source": [
    "- 到目前为止，前面的方法都将state or action value处理成table，如sarsa、Q-learning都是在估计table中每个位置的state-action对应的action value。<font color=norange>所以他们又称为tabular Sarsa和tabular Q-learning。</font>这些方法仅适用于state-action数量很少的场景。当state-action数量很大时，算法每次迭代只能处理table中单个元素，效率低。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431c925c-e5fd-4b94-974e-d4998ec2a841",
   "metadata": {},
   "source": [
    "- <font color=green>**一种改进思路**</font>：function approximation methods。指的是用函数代替table，通过求解函数中的参数来找到state-action与value之间的稳定关系。\n",
    "$$\\begin{align}\n",
    "V^{\\pi}(s) & = V_{\\theta }^{\\pi}(s) = f(s,\\theta )\\\\\n",
    "Q^{\\pi}(s, a) & = Q_{\\theta }^{\\pi}(s, a) = h(s, a, \\theta ) \\\\\n",
    "\\end{align}$$\n",
    "  - f(·)和h(·)可以是各种mapping关系，包括神经网络。问题求解的目标不再是直接求V(s)和Q(s,a)，而转变成了求函数的参数$\\theta$。\n",
    "  - 提升算法效率的方式：参数的一次调整会影响所有state-action pair的价值，算法的优化效率更高\n",
    "  - 代价是此时函数只是原价值函数的approximation，如果函数的表达力不够，那么不论参数如何优化都不能准确表达原价值函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be18d1cc-a36f-488c-a751-4a8ce298d110",
   "metadata": {},
   "source": [
    "- <font color=green>**方法调整**</font>：\n",
    "  - sarsa和Q-learning等基于RM的TD算法求解的目标是价值函数Q(s, a)。而用function approximation之后，求解的目标变成给定函数求参数$\\theta$。\n",
    "  - 目标问题变化：和前面章节一样，先用state value function来说明思路。\n",
    "$$\\begin{align}\n",
    "原问题：& V^{\\pi}(S)\\\\\n",
    "新问题：\\\\\n",
    "& \\underset{\\theta}{min}J(\\theta) \n",
    " = \\underset{\\theta}{min}E\\left ( V^{\\pi}(S)-\\hat{V}(S,\\theta ) \\right )^2  \\\\\n",
    "\\Leftrightarrow  \\ \\ &\\nabla_{\\theta }E\\left ( V^{\\pi}(S)-\\hat{V}(S,\\theta ) \\right )^2  =0\\\\\n",
    "\\Leftrightarrow  \\ \\ &E\\nabla_{\\theta }\\left ( V^{\\pi}(S)-\\hat{V}(S,\\theta ) \\right )^2=0\\\\\n",
    "\\Leftrightarrow  \\ \\ &E\\left [\\left ( V^{\\pi}(S)-\\hat{V}(S,\\theta ) \\right )\\nabla_{\\theta }\\hat{V}(S,\\theta )\\right ]=0\n",
    "\\end{align}\n",
    "$$\n",
    "  - 仍然是期望形式，因此还是可以用RM算法迭代求解，对应RM的第二种典型应用：SGD形式\n",
    "  - **由于没有$V^{\\pi}$的真实值的样本，所以不能直接用RM算法，和上一章中TD learning的思路一样，改用当前时刻对$V^{\\pi}$的‘最佳估计’。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48acbafe-2d0b-41c7-8b92-1a69fdfb27f6",
   "metadata": {},
   "source": [
    "- <font color=red>**代价：TD learning加上function approximation之后，除非特定条件下，否则不再具备almost surely convergence的性质。**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d765d2",
   "metadata": {},
   "source": [
    "## II. 用function approximation做TD learning of $V^{\\pi}(S)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385b2fbf-3bd2-42e8-88e2-8c6de06b3348",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### II.1 基础方法：function approximation + SGD+MC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33017a3-9c34-42fa-b7e3-428d7a988dea",
   "metadata": {},
   "source": [
    "#### II.1.1 目标函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255d0cad",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\underset{w}{min}J(w) \n",
    "& = \\underset{w}{min}E\\left [\\left ( v^{\\pi}(S)-\\hat{v}(S,w) \\right )^2 \\right ] \\\\\n",
    "& = \\underset{w}{min}\\sum _{s\\in \\mathcal{S}}d^{\\pi}(s)[v^{\\pi}(s)-\\hat{v}(s,w)]^2 \\\\\n",
    "w & = \\underset{w}{argmin}J(w) \\\\\n",
    "  & = \\underset{w}{argmin} E\\left [\\left ( v^{\\pi}(S)-\\hat{v}(S,w)\\right )^2 \\right ]\n",
    "\\end{align}\n",
    "$$\n",
    "- $v^{\\pi}(S)$和$\\hat{v}(S,w)$分别是给定策略$\\pi$时，所有state value的真实值和估计值。\n",
    "- 这里的期望是针对状态$s\\in S$的分布而言。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130e3014-5c28-4463-b4e4-475c10165ec4",
   "metadata": {},
   "source": [
    "#### II.1.2 求解优化问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998ba07b",
   "metadata": {},
   "source": [
    "1. 等价目标函数:\n",
    "$$J(w) = \\frac{1}{2} E\\left [\\left ( v^{\\pi}(S)-\\hat{v}(S,w)\\right )^2 \\right ] $$\n",
    "2. 梯度：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla J(w) \n",
    "& = \\frac{1}{2} \\nabla_w E\\left [\\left ( v^{\\pi}(S)-\\hat{v}(S,w)\\right )^2 \\right ] \\\\\n",
    "& = -E\\left [v^{\\pi}(S)-\\hat{v}(S,w)\\right  ]\\nabla_w \\hat{v}(S,w)\n",
    "\\end{align}\n",
    "$$\n",
    "3. SGD算法：\n",
    "$$\n",
    "\\begin{align}\n",
    "w_{t+1} &  = w_t - \\alpha_t\\nabla_w J(w_t) \\\\\n",
    "&= w_t + \\alpha_t\\left [v^{\\pi}(s_t)-\\hat{v}(s_t,w_t)\\right  ]\\nabla_w \\hat{v}(s_t,w_t)\n",
    "\\end{align}\n",
    "$$\n",
    "4. 问题：<font color=red>**由于$v^{\\pi}(s_t)$未知，因此无法直接使用SGD。**</font>\n",
    "5. 一种简单的解决方法：Monte Carol方法。用episode的信息来估计所有状态$s$的state value，然后代入SGD求解。\n",
    "6. 引出新的问题，MC只能在抽出很多episodes之后才能计算，如果要实时处理样本，需要新的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d035879-0dda-45ea-909b-567dad647451",
   "metadata": {},
   "source": [
    "### II.2 function approximation + TD learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42a473c-b569-45a5-8082-2b4c60cd7b71",
   "metadata": {},
   "source": [
    "- 方法：用一个函数或者DNN来learn状态价值函数。\n",
    "  - 以参数为w的DNN为例，迭代方式为：\n",
    "    1. 随机初始化$w_0$\n",
    "    2. 迭代，直到达到指定的收敛精度：\n",
    "       - 抽取一个time-step样本$(s_t, a_t, r_t)$\n",
    "       - 按照下面迭代式更新网络参数$w$\n",
    "    3. 从得到的价值函数提取策略"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b96cac",
   "metadata": {},
   "source": [
    "- 迭代式为：\n",
    "$$\\color{Blue} {w_{t+1} = w_t + \\alpha_t\\left [r_{t+1}+\\gamma \\hat{v}(s_{t+1,w_t})-\\hat{v}(s_t,w_t)\\right  ]\\nabla_w \\hat{v}(s_t,w_t)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7378f3cd-39af-40ba-9c6e-929523f3805e",
   "metadata": {},
   "source": [
    "## III. 用function approximation做TD learning of $Q^{\\pi}(S,A)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce5bf79-0c21-4761-ac21-d6ee722bca23",
   "metadata": {},
   "source": [
    "### III.1 Sarsa with function approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f84642-3549-4569-97c9-ef12f975cf36",
   "metadata": {},
   "source": [
    "#### III.1.1 迭代过程推导"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa99ef7-ad71-449f-b02d-89f17998c1d5",
   "metadata": {},
   "source": [
    "- 目标问题：\n",
    "$$\\begin{align}\n",
    "&\\underset{\\theta}{min}J(\\theta) \n",
    " = \\underset{\\theta}{min}E\\left ( Q^{\\pi}(S,A)-\\hat{Q}(S,A,\\theta ) \\right )^2  \\\\\n",
    "\\Leftrightarrow  \\ \\ &\\nabla_{\\theta }E\\left ( Q^{\\pi}(S,A)-\\hat{Q}(S,A,\\theta ) \\right )^2  =0\\\\\n",
    "\\Leftrightarrow  \\ \\ &E\\nabla_{\\theta }\\left ( Q^{\\pi}(S,A)-\\hat{Q}(S,A,\\theta ) \\right )^2=0\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7e12f9-c7d9-46c9-bf9a-edce5e2c50a9",
   "metadata": {},
   "source": [
    "- RM变量：\n",
    "$$\\begin{align}\n",
    "g(\\theta ) \n",
    "& = E\\nabla_{\\theta }\\left ( Q^{\\pi}(S,A)-\\hat{Q}(S,A,\\theta ) \\right )^2\\\\\n",
    "& = -E\\left [\\left ( Q^{\\pi}(S,A)-\\hat{Q}(S,A,\\theta ) \\right )\\nabla_{\\theta }\\hat{Q}(S,A,\\theta )\\right ] \\\\\n",
    "\\tilde g(\\theta_t ,\\eta_t ) \n",
    "& = -\\left[ Q^{\\pi}(s_t,a_t)-\\hat{Q}(s_t,a_t,\\theta_t) \\right ]\\nabla_{\\theta }\\hat{Q}(s_t,a_t,\\theta )|_{\\theta =\\theta_t}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4d1ef3-51cc-420c-a89f-fb109ebcec24",
   "metadata": {},
   "source": [
    "- RM迭代式：\n",
    "$$\\begin{align}\n",
    "\\theta_{t+1} \n",
    "& = \\theta_t -\\alpha_t(\\theta_t)\\tilde g(\\theta_t ,\\eta_t ) \\\\\n",
    "& = \\theta_t +\\alpha_t(\\theta_t)\\left [ Q^{\\pi}(s_t,a_t)-\\hat{Q}(s_t,a_t,\\theta_t) \\right ]\\nabla_{\\theta }\\hat{Q}(s_t,a_t,\\theta )|_{\\theta =\\theta_t}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6275ca9e",
   "metadata": {},
   "source": [
    "- 由于$ Q^{\\pi}(s_t,a_t)$未知，和tabular TD中一样，将其替换为当前时刻的最佳估计量得到新的迭代式：\n",
    "$$\\begin{align}\n",
    "\\theta_{t+1} \n",
    "& = \\theta_t +\\alpha_t(\\theta_t)\\left [ Q^{\\pi}(s_t,a_t)-\\hat{Q}(s_t,a_t,\\theta_t) \\right ]\\nabla_{\\theta }\\hat{Q}(s_t,a_t,\\theta )|_{\\theta =\\theta_t} \\\\\n",
    "& \\rightarrow  \\theta_t +\\alpha_t(\\theta_t)\\left [ r_{t+1}+\\gamma Q^{\\pi}(s_{t+1},a_{t+1})-\\hat{Q}(s_t,a_t,\\theta_t) \\right ]\\nabla_{\\theta }\\hat{Q}(s_t,a_t,\\theta )|_{\\theta =\\theta_t} \\\\\n",
    "& \\rightarrow \\theta_t +\\alpha_t(\\theta_t)\\left [ r_{t+1}+\\gamma \\hat Q(s_{t+1},a_{t+1}, \\theta _t)-\\hat{Q}(s_t,a_t,\\theta_t) \\right ]\\nabla_{\\theta }\\hat{Q}(s_t,a_t,\\theta )|_{\\theta =\\theta_t} \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd7bfc1-2460-43b2-834d-6090adac5f93",
   "metadata": {},
   "source": [
    "#### III.1.2 算法伪码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a579ff8",
   "metadata": {},
   "source": [
    "- <img src=\"./pics/sarsaapprox.png\" alt=\"alt text\" width=\"560\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322c30fb-27a2-4116-9f60-a3b2342d86c5",
   "metadata": {},
   "source": [
    "### III.2 Q-learning with function approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2467bc3-0c11-4315-9b24-2da9d46013de",
   "metadata": {},
   "source": [
    "- 迭代式:\n",
    "$$\\color{Blue} {w_{t+1} = w_t + \\alpha_t\\left [r_{t+1}+\\gamma\\underset{a\\in \\mathcal{A}}{max}\\ \\hat{q}(s_{t+1},a,w_t)-\\hat{q}(s_t,a_t,w_t)\\right  ]\\nabla_w \\hat{q}(s_t,a_t,w_t)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b483597",
   "metadata": {},
   "source": [
    "## IV. Deep Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b163780-548c-4862-b564-afb6a46348bd",
   "metadata": {},
   "source": [
    "- 思想：function approximation使用deep neural network\n",
    "- 迭代式：\n",
    "$$\\color{Blue} {w_{t+1} = w_t + \\alpha_t\\left [r_{t+1}+\\gamma\\underset{a\\in \\mathcal{A}}{max}\\ \\hat{q}(s_{t+1},a,w_t)-\\hat{q}(s_t,a_t,w_t)\\right  ]\\nabla_w \\hat{q}(s_t,a_t,w_t)}$$\n",
    "  - 其中$q(s_t,a_t,w_t)$是DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91eac5d",
   "metadata": {},
   "source": [
    "- 算法伪码\n",
    "  - <img src=\"./pics/deepqlearning.png\" alt=\"alt text\" width=\"560\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7ecd75-0ebd-49ac-b132-409ef19c286f",
   "metadata": {},
   "source": [
    "- 算法特征：\n",
    "  1. off-policy：有两个网络，behavior network用来explore environment，获得训练数据。target network用于迭代求目标参数。target network中的参数每次迭代都在改变；隔一段时间behavior network中的参数更新为target network的参数，在此之前，behavior network中的参数不像target network中那样每次迭代都改变。\n",
    "  2. replay buffer：存放behavior network获得的数据。每个{s, a, r, s'}构成一个样本点，behavior network得到的episodes都被拆散成单个的样本点存入replay buffer。每次迭代时都从buffer中取出一个minibatch的样本来做训练。抽样采用以(s,a)值为index的uniform sample。\n",
    "     - <font color=green>这里必须拆开成单个样本点做uniform sample是因为：不拆开的话样本点之间的相关性太强，他们都是基于当前策略连续抽的episodes。拆开后做有放回抽样可以打破这里的相关性，同时当个样本可能多次被抽中也提高了data efficiency。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd5c87d-34ce-494f-a1a7-4f7015ae7c27",
   "metadata": {},
   "source": [
    "### 1. limit distribution的充要性证明"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8242de-ce0e-4180-a985-b672a3aee63c",
   "metadata": {},
   "source": [
    "- 充分性证明：\n",
    "$$\n",
    "\\lim_{k \\to \\infty} d_k^T =d_0^T\\lim_{k \\to \\infty} (P^{\\pi })^k= d_0^T\\mathbb{1}_n(d^{\\pi})^T = (d^{\\pi})^T\n",
    "$$\n",
    "- 必要性证明：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\because &\\ \\lim_{k \\to \\infty} d_k^T  = d_0^T\\lim_{k \\to \\infty} (P^{\\pi })^k， (d^{\\pi})^T  = d_0^T\\mathbb{1}_n(d^{\\pi})^T \n",
    "\\\\\n",
    "if\\ & \\lim_{k \\to \\infty} d_k^T  = (d^{\\pi})^T\\ ,\\  then \\ \\  \\ d_0^T\\lim_{k \\to \\infty} (P^{\\pi })^k  = d_0^T\\mathbb{1}_n(d^{\\pi})^T  \\\\\n",
    "\\Rightarrow & \\ d_0^T \\left [\\lim_{k \\to \\infty} (P^{\\pi })^k - \\mathbb{1}_n(d^{\\pi})^T\\right ] =0 \\\\\n",
    "\\Rightarrow & \\ \\lim_{k \\to \\infty} (P^{\\pi })^k - \\mathbb{1}_n(d^{\\pi})^T =0 \n",
    "\\end{align}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
