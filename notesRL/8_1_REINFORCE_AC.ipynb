{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9c31731",
   "metadata": {},
   "source": [
    "# REINFORCE and AC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f1d537",
   "metadata": {},
   "source": [
    "- 在前面的模型中，都假设给定state s，有最优策略，因此$\\pi(a=a^*|s)=1, \\pi(a\\ne a^*|s)=0$。\n",
    "  - 在两个action的价值相同时，策略可以任选其中一种，或者以某种概率选择两种action。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10720e66",
   "metadata": {},
   "source": [
    "## I. 策略梯度的目标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e5bf9a",
   "metadata": {},
   "source": [
    "### I.1 目标分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29d3bd2-b4c8-4da7-a4cd-0346fb5f7042",
   "metadata": {},
   "source": [
    "#### I.1.1 理论目标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f8cb97-976a-469c-aa68-840e1c5b563a",
   "metadata": {},
   "source": [
    "$$U(\\theta) \n",
    " = E[V(s)|\\pi_{\\theta }]=E_{s\\sim d^{\\pi }(s)}[V^{\\pi }(s)]=\\sum_{s\\in \\mathcal{S} }d(s)V^{\\pi }(s)$$\n",
    "  - <font color=brown>math of RL书中的展开方法</font>\n",
    "  - 其中d(s)是状态s在当前策略下的稳定分布。这里假设该分布存在。\n",
    "  - 但实践中通常不会等策略迭代无穷次达到稳定分布后再执行算法，所以真实环境下d(s)通常不会被考虑在迭代式中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dfca22-1e93-4ccb-bd6f-a5d6a3f7b247",
   "metadata": {},
   "source": [
    "#### I.1.2 实际分析使用的目标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eb4632-f2c9-4485-8b0d-20aeb795387c",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "U(\\theta) = E_{\\tau }R(\\tau|\\pi_{\\theta}) = \\sum_{\\tau }P(\\tau ;\\theta )R(\\tau )=\\left\\{\\begin{matrix}\n",
    " E\\left[ \\sum_{t=0}^{H-1}R(s_t, u_t)|\\pi_{\\theta } \\right]  & , undiscounted\\ version\\\\\n",
    " E\\left[ \\sum_{t=0}^{H} \\gamma^t R(s_t, u_t)|\\pi_{\\theta } \\right] & , discounted\\ version\n",
    "\\end{matrix}\\right.\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fdb07e-e805-40e6-8404-ee1f9954f11c",
   "metadata": {},
   "source": [
    "### I.2 梯度推导"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d774ad-37d6-480a-a1aa-d8b222fb7c6d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### I.2.1 梯度的估计量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a04520f",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "\\nabla _{\\theta }U(\\theta ) \n",
    "& = \\nabla _{\\theta } \\sum_{\\tau }P(\\tau ;\\theta )R(\\tau ) \\\\\n",
    "& =  \\sum_{\\tau }\\nabla _{\\theta }P(\\tau ;\\theta )R(\\tau ), {\\color{green}{这步是因为\\theta带给\\tau的随机性都体现在P(\\tau;\\theta)里面}} \\\\\n",
    "& =  \\sum_{\\tau }P(\\tau ;\\theta )\\frac{\\nabla _{\\theta }P(\\tau ;\\theta )}{P(\\tau ;\\theta )}R(\\tau ) \\\\\n",
    "& =  E_{\\tau }\\nabla _{\\theta }logP(\\tau ;\\theta )R(\\tau ) \\\\\n",
    "\\\\\n",
    "\\nabla _{\\theta }logP(\\tau ;\\theta ) \n",
    "& = \\nabla _{\\theta }log\\left[ d(s_0)\\prod_{t=0}^{H-1} P(s_{t+1}|s_t, u_t)*\\pi_{\\theta }(u_t|s_t) \\right ]\\\\\n",
    "& = \\nabla _{\\theta }\\left[logd(s_0) + \\sum_{t=0}^{H-1}logP(s_{t+1}|s_t, u_t) + \\sum_{t=0}^{H-1}log\\pi_{\\theta }(u_t|s_t)\\right ]\\\\\n",
    "& = \\underset{0}{ \\underbrace{\\nabla _{\\theta }logd(s_0)}}  + \\underset{0}{\\underbrace{\\nabla _{\\theta }\\sum_{t=0}^{H-1}logP(s_{t+1}|s_t, u_t)}} + \\nabla _{\\theta }\\sum_{t=0}^{H-1}log\\pi_{\\theta }(u_t|s_t)\\\\\n",
    "& = \\nabla _{\\theta }\\sum_{t=0}^{H-1}log\\pi_{\\theta }(u_t|s_t) \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071549a3-4662-4a1f-84a1-c33eaee7f67f",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "取梯度：g & = \\nabla _{\\theta }U(\\theta )\\\\\n",
    "& = E_{\\tau }\\left[\\nabla _{\\theta }\\sum_{t=0}^{H-1}log\\pi_{\\theta }(u_t|s_t)R(\\tau )\\right ]\\\\\n",
    "& = E\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t|s_t)\\sum_{t=0}^HR(s_t, u_t)\\right ]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e34525",
   "metadata": {},
   "source": [
    "- <font color=brown>没有用math of RL书中的展开方法，用的是Pieter Abbeel的foundations of deep RL中的展开方法。上式中和Pieter Abbeel原文分析中不同之处在于加上了$\\nabla _{\\theta }logd(s_0)$。这是因为原文中有一个默认的简化，也即假设MDP过程的稳定分布不受策略的影响。math of RL书中有详细分析，这里折衷，将该假设显式化，相当于math of RL书中$\\bar v^0_{\\pi}$的情况。接近Theorem 9.2，但改为undiscounted case。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b813288-8deb-4b84-98d4-e3f6ddaef730",
   "metadata": {},
   "source": [
    "- 因此，可以得到梯度的估计量：\n",
    "$$\\begin{align}\n",
    "\\hat g \n",
    "& = \\frac{1}{m}\\sum_{i=1}^M\\nabla _{\\theta }logP(\\tau^{(i)};\\theta )R(\\tau^{(i)})\\\\\n",
    "& = \\frac{1}{m}\\sum_{i=1}^M \\left[\\nabla _{\\theta }\\sum_{t=0}^{H-1}log\\pi_{\\theta }(u_t^{(i)}|s_t^{(i)})\\right]R(\\tau^{(i)})\\\\\n",
    "& = \\frac{1}{m}\\sum_{i=1}^M \\left[\\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t^{(i)}|s_t^{(i)})\\right]\\sum_{t=0}^{H-1}R(s_t^{(i)},u_t^{(i)})\\\\\n",
    "& = \\frac{1}{m}\\sum_{i=1}^M \\left[\\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t^{(i)}|s_t^{(i)})\\right]Q(s_t^{(i)},u_t^{(i)})\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3b17e9-1bd1-4065-aa86-fadeaede8834",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### I.2.2 理解梯度估计量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21a49be-c4e1-42b7-a50a-3aaf432f2e7b",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "supervised\\ learning中：maximize\\ likelihood\\\\\n",
    "\\nabla_{\\theta }J(\\theta )&= \\frac{1}{n}\\sum_{i=1}^N \\left[\\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(a_t^{(i)}|s_t^{(i)})\\right]\\\\\n",
    "RL中：policy\\ gradient\\\\\n",
    "\\nabla_{\\theta }J(\\theta )&=\\frac{1}{n}\\sum_{i=1}^N \\left[\\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(a_t^{(i)}|s_t^{(i)})\\right]Q(s_t^{(i)},a_t^{(i)})\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093d9a87-707c-42f2-aebf-9b56824c2655",
   "metadata": {},
   "source": [
    "1. 在有监督学习中，比如imitation learning MLE提高的是样本中出现过的$(a_t|s_t)$ pair的概率。借此实现向样本学习。习得的$\\pi_{\\theta}(a_t|s_t)$尽可能与模仿对象的决策策略一样。\n",
    "   - 有监督学习中，样本提供的$(a_t|s_t)$ pair中，给定state s，对应的action a就是学习的target，因为样本提供的action都       是‘好’action。\n",
    "   - <font color=blue>所以MLE中的迭代式会increase the probabilities of all these sample actions under the corresponding states.</font>\n",
    "2. 在策略梯度中，没有‘好的’目标样本可供学习。每次迭代时得到的$(a_t|s_t)$ pair都是用前一次迭代得到的策略与环境交互生成的，这些策略可能好，可能坏。所以迭代的目标应该是增加这些样本action中'好的'action的发生概率，减少‘坏的’action的发生概率。\n",
    "   - <font color=blue>策略梯度算法中，决定这些action好坏的依据是这些action的rewards，也是它们对应的行为价值。直观理解，高价值的行为的概率被提高，低价值行为的概率被减少。 </font>\n",
    "   - <font color=orange>但是这些行为的价值并不知道，只能估计。这里估计有两个限制，一个是只能用当前策略估计，而不是最优策略做估计；二是即使想知道当前策略下的行为价值，也需要用Monte Carlo或者Q-learning来估计。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d608ca94",
   "metadata": {},
   "source": [
    "#### I.2.3 梯度的估计量的性质：无偏但方差大"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3b4191-9a3c-479e-b743-564aa8c522c8",
   "metadata": {},
   "source": [
    "- 无偏性： $E(\\hat g) = \\nabla_{\\theta}U(\\theta)$ [证略]\n",
    "- 方差：因为用抽样法得到trajectory，在s和a的取值空间较大时，抽样结果的方差很大"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89d3293-91ef-42ee-8246-7a903ee67d42",
   "metadata": {},
   "source": [
    "## II. 最简单的Policy gradient算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3ae39f-da76-4413-ba2f-3d8dda47fe7a",
   "metadata": {},
   "source": [
    "### II.1 迭代式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2bdd38-23a3-4fef-a241-9d8659f146a8",
   "metadata": {},
   "source": [
    "- <font color=blue>同样适用TD类的算法：</font>\n",
    "    - 原问题：$max U(\\theta) = max E_{\\tau }R(\\tau|\\pi_{\\theta})$\n",
    "    - 转化问题：$\\nabla_{\\theta} U(\\theta ) = 0  \\Leftrightarrow  E\\nabla _{\\theta }\\sum_{t=0}^{H-1}log\\pi_{\\theta }(u_t|s_t)\\sum_{t=0}^{H-1}R(s_t,u_t) = 0$\n",
    "    - 迭代变量：$\\hat g(\\theta_k, \\eta_k)  = \\left[\\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t^{(k)}|s_t^{(k)})\\right]Q(s_t^{(k)},u_t^{(k)})$\n",
    "      - <font color=red>符号说明：k是index of trajectory, t是单个trajectory上的time steps的index，这里SGD对应的一个样本是一个trajectory而不是一次time step</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17418306-e911-44f0-b657-4f68012fa244",
   "metadata": {},
   "source": [
    "- 迭代式：\n",
    "$$\\begin{align}\n",
    "\\theta_{k+1} \n",
    "& = \\theta_k +\\alpha_k \\hat g(\\theta_k, \\eta_k)  \\\\\n",
    "& = \\theta_k +\\alpha_k\\left[\\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t^{(k)}|s_t^{(k)})Q(s_t^{(k)},u_t^{(k)})\\right]\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40beeefa-8a11-47d7-9212-303095834cdb",
   "metadata": {},
   "source": [
    "- <font color=blue>**Intuition**</font>：\n",
    "  - 迭代算法工作方式：\n",
    "    - 如果样本trajectory上的回报为正，算法会增加该trajectory发生的概率\n",
    "    - 如果样本trajectory上的回报为负，算法会减少它发生的概率\n",
    "  - <font color=red>问题</font>：\n",
    "    - 如果reward是认为设定的函数，可能所有的trajectory的reward都是大于0的，只是有大小之分，那么上面的算法迭代更新的效率就会很低。因为即使一个trajectory A的reward小于平均值，迭代还是会提升它的发生概率，只有靠下次别的更优秀的trajectory的概率得到更大的提升才会平衡掉A带来的影响。\n",
    "    - <font color=green>这也为后续算法优化方法中增加baseline提供了依据。增加baseline后，如果trajectory A的reward小于均值，那么扣掉baseline后得到的reward就是负值，就不存在上面的问题。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de9e985-dec5-4c94-9e89-7dea23b614da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### II.2 REINFORCE算法：MC policy gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d7c135-8334-4954-9ed6-8528f7bc9ae9",
   "metadata": {},
   "source": [
    "- <font color=blue>**REINFORCE的迭代目标和估计式**：</font>\n",
    "$$\\begin{align}\n",
    "g & = \\underset{\\tau\\sim P_{\\theta }(\\tau )}{E}\\left[\\sum_{t=0}^{H-1} \\nabla _{\\theta }log\\pi_{\\theta }(a_t|s_t)\\left(\\sum_{t'=t}^{H-1}R(s_{t'}, a_{t'})-b\\right)\\right ]\\\\\n",
    "\\\\\n",
    "\\hat g & = \\frac{1}{N}\\sum_{i=1}^N \\left[\\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t^{(i)}|s_t^{(i)})\\left(\\sum_{t'=t}^{H-1}r(s_{t'}^{(i)}, a_{t'}^{(i)})-b\\right)\\right]\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768c7823-184e-440d-a997-9e93fb5628cb",
   "metadata": {},
   "source": [
    "- 算法说明：\n",
    "  - 迭代用SGD方法\n",
    "  - Q(s,u)用MC抽样估计"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e1e342-2987-48be-b528-0d0f94e4413d",
   "metadata": {},
   "source": [
    "- 算法伪码：<font color=brown>[ref: 动手学强化学习]</font>\n",
    "  - 初始化参数$\\theta$\n",
    "  - 迭代M个trajectory：\n",
    "    - 采样一个trajectory，轨迹是${s_1,a_1,r_1,s_2,a_2,r_2,...,s_T,a_T,r_T}$\n",
    "    - 计算当前轨迹每个时刻t往后的回报：$\\hat Q(s_t,u_t)=\\sum_{j=t}^T\\ \\gamma^{j-t} r_{j}$\n",
    "    - 按迭代式更新$\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ea995b-78f6-4459-b6f4-738c289b6104",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## III. 降低梯度估计量的方差"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a94d6b-fdf2-4396-b593-b3f09b80377c",
   "metadata": {},
   "source": [
    "- 出发点：在MC policy gradient的基础上降低迭代所使用的梯度的方差\n",
    "- 方法：\n",
    "  1. 用discounted reward(上面算法中已经加上)\n",
    "  2. 只计算'rewards to go'\n",
    "  3. 增加baseline\n",
    "  4. 把MC方法换成TD\n",
    "- 得到的结果就是advantage actor critic算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1fd099-7037-4e43-83aa-f839a6c62158",
   "metadata": {},
   "source": [
    "### III.1 rewards to go和baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8e8aa5-abff-4644-ba3d-0091ad28279b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### III.1.1 对梯度估计量的影响："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6851edc-f753-4eda-8f1b-6117573e7c40",
   "metadata": {},
   "source": [
    "1. rewards to go不改变原梯度期望值。<font color=brown>[证明见附页：spinning up的证明]</font>\n",
    "$$\\begin{align}\n",
    "g & = E\\left[ \\nabla _{\\theta }\\sum_{t=0}^{H-1}log\\pi_{\\theta }(u_t|s_t)\\sum_{t=0}^{H-1}R(s_t, u_t)\\right ] \\\\\n",
    "& = E\\left[ \\nabla _{\\theta }\\sum_{t=0}^{H-1}log\\pi_{\\theta }(u_t|s_t)\\sum_{t'=t}^{H-1}R(s_{t'}, u_{t'})\\right ]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae56c85a-049b-46c6-bbbf-122d71e34391",
   "metadata": {},
   "source": [
    "2. 只要baseline的取值不取决于action，即使它的取值是state的函数也没关系，此时增加baseline不改变policy gradient估计量的无偏性。<font color=brown>[证明见math of RL page225 10.2.1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c800d2b7-41cb-4d4f-af19-9ce00f4104c2",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "g & = E\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t|s_t)\\sum_{t=0}^{H-1}R(s_t, u_t)\\right ] \\\\\n",
    "& = E\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t|s_t)\\sum_{t'=t}^{H-1}R(s_{t'}, u_{t'})\\right ] \\\\\n",
    "& = E\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t|s_t)\\left( \\sum_{t'=t}^{H-1}R(s_{t'}, u_{t'}) - b(s_t)\\right )\\right ]\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5531bb-51f8-4693-b0fc-2fe4d4418ecc",
   "metadata": {},
   "source": [
    "3. 当baseline取$b(s)=V^{\\pi}(s)$时，会减少估计量的方差。\n",
    "   - <font color=red>注意，此时b的取值虽然与策略相关，但并不受t时刻action $a_t$的影响，所以无偏性仍然成立。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d39767-b1f7-4d54-986c-d53853bbd874",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### III.1.2 定义Advantage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729e433a-9e60-4422-a88e-673a9fbe88e4",
   "metadata": {},
   "source": [
    "- 含义：t时刻的action $u_t$带来的reward比平均reward有多少相对优势。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc44efa-6bba-45e0-8a35-4ce1416a4aad",
   "metadata": {},
   "source": [
    "$$A_t= \\sum_{t'=t}^{H-1}R(s_{t'}^{(i)}, u_{t'}^{(i)}) - V^{\\pi }(s_t^{(i)}) = Q(s_{t}^{(i)}, u_{t}^{(i)}) - V^{\\pi }(s_t^{(i)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cebf9d-bf98-42f7-abf0-4163f854c2c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### III.1.3 改造后的梯度估计量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb41d7c7-553d-4f7a-92f8-dae4ba56f887",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "g & = E\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t|s_t)\\left( \\sum_{t'=t}^{H-1}R(s_{t'}, u_{t'}) - b(s_t)\\right )\\right ]\\\\\n",
    "& = E\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t|s_t)\\left( \\sum_{t'=t}^{H-1}R(s_{t'}, u_{t'}) - V^{\\pi }(s_t)\\right )\\right ]\\\\\n",
    "& = E\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t|s_t)A_t\\right ]\n",
    "\\\\\n",
    "\\\\\n",
    "\\hat g \n",
    "& = \\frac{1}{m}\\sum_{i=1}^m\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t^{(i)}|s_t^{(i)})\\left( \\sum_{t'=t}^{H-1}R(s_{t'}^{(i)}, u_{t'}^{(i)}) - V^{\\pi }(s_t^{(i)})\\right )\\right ] \\\\\n",
    "& = \\frac{1}{m}\\sum_{i=1}^m\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t^{(i)}|s_t^{(i)})\\left( Q(s_{t}^{(i)}, u_{t}^{(i)}) - V^{\\pi }(s_t^{(i)})\\right )\\right ] \\\\\n",
    "& = \\frac{1}{m}\\sum_{i=1}^m\\left[ \\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t^{(i)}|s_t^{(i)})\\hat A\\right ]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a7ba99-b59b-42a9-a631-cd99d19fc341",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### III.2 估计advantage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0de0b5-de7d-4311-97d0-118979ca1fe9",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "\\hat A=\\hat Q^{\\pi }(s_{t}^{(i)}, u_{t}^{(i)}) - \\hat V^{\\pi }(s_t^{(i)})\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ee2a58-f0cc-41d1-bd1b-d7435d46e8da",
   "metadata": {},
   "source": [
    "- 不同的方法和利弊：\n",
    "  1. 用MC估计$\\hat Q$，unbiased但variance大\n",
    "  2. 【A2C算法】用TD估计，将上式转化为：$\\hat A_t=r_{t+1} + V^{\\pi}(s_{t+1}^{(i)}) -  V^{\\pi }(s_t^{(i)})$ ，此时variance相对MC小很多，但是biased\n",
    "  3. 【A3C算法】取上面两种情况的折衷：$\\hat A_t=r_{t+1} + r_{t+2}+ r_{t+3}+ V^{\\pi}(s_{t+3}^{(i)}) -  V^{\\pi }(s_t^{(i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145a6af5-9367-4459-b86d-de944fa90b9f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### III.3 A2C(Advantage Actor-Critic)算法：policy gradient by TD + baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6131f87b-c1e6-4d9d-8622-07afab36a13a",
   "metadata": {},
   "source": [
    "#### III.3.1 算法说明"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fde0f45-94b8-41be-be5c-c65e695e8c66",
   "metadata": {},
   "source": [
    "- <font color=blue>**A2C的迭代目标和估计式**：</font>\n",
    "$$\\begin{align}\n",
    "g & = \\underset{\\tau\\sim P_{\\theta }(\\tau )}{E}\\left[ \\nabla _{\\theta }\\sum_{t=0}^{H-1}log\\pi_{\\theta }(a_t|s_t)\\left(\\sum_{t'=t}^{H-1}R(s_{t'}, a_{t'})-V^{\\pi_{\\theta }}(s_t)\\right)\\right ]\\\\\n",
    "\\\\\n",
    "\\hat g & = \\frac{1}{N}\\sum_{i=1}^N \\left[\\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t^{(i)}|s_t^{(i)})\\left(\\sum_{t'=t}^{H-1}r(s_{t'}^{(i)}, a_{t'}^{(i)})-\\hat V^{\\pi_{\\theta }}(s_t^{(i)})\\right)\\right]\\\\\n",
    "& = \\frac{1}{N}\\sum_{i=1}^N \\left[\\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t^{(i)}|s_t^{(i)})\\left[r(s_{t}^{(i)}, a_{t}^{(i)})+\\hat V^{\\pi_{\\theta }}(s_{t+1}^{(i)})-\\hat V^{\\pi_{\\theta }}(s_t^{(i)})\\right]\\right]\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eefbf6-6448-4fc4-8eee-d0a999dc4af1",
   "metadata": {},
   "source": [
    "  - 迭代用SGD方法，增加了baseline（如果没有baseline就是AC算法）\n",
    "  - Q(s,u)用TD估计，另外有一个network用于估计行为价值函数，参数w。\n",
    "    - DQN中的迭代式：$\\color{Blue} {w_{t+1} = w_t + \\alpha_t\\left [r_{t+1}+\\gamma \\hat{v}(s_{t+1,w_t})-\\hat{v}(s_t,w_t)\\right  ]\\nabla_w \\hat{v}(s_t,w_t)}$\n",
    "    - 这里为了统一两个网络的更新方式，做一点变化：\n",
    "$$\\begin{align}\n",
    "& 取\\delta_t = r_{t+1}+\\gamma \\hat{v}(s_{t+1,w_t})-\\hat{v}(s_t,w_t) \\\\\n",
    "& w_{t+1} = w_t + \\alpha_t\\sum_{t=0}^{H-1}\\delta_t\\nabla_wV_w(s_t)\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0978de78-dab3-42e2-9231-9aefa2a09dce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### III.3.2 最基础的A2C：Batch A2C，每次处理一个batch的trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c746d4-5f74-441e-9eeb-4ce5a8e86fc6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "- <font color=brown>[ref: spinning up]</font>\n",
    "  - <img src='pics/batch_vanilla_pg.png' width='75%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e03ffc6-c601-4615-b5a8-7a2c366595fc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### III.3.3 每次处理一个trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686d3c4c-da66-4c0b-9bd2-8dd2e98bc91d",
   "metadata": {},
   "source": [
    "- <font color=brown>[ref: 动手学强化学习]</font>\n",
    "- 算法伪码：\n",
    "    - 初始化参数$\\theta$\n",
    "    - 迭代M个trajectory：\n",
    "      1. 采样一个trajectory，轨迹是${s_1,a_1,r_1,s_2,a_2,r_2,...,s_T,a_T,r_T}$\n",
    "      2. 为每一步计算$\\delta_t = r_{t+1}+\\gamma \\hat{v}(s_{t+1,w_t})-\\hat{v}(s_t,w_t)$\n",
    "      3. 用min MSE(mean squared error)为目标更新状态价值网络参数：$ w_{t+1} = w_t + \\alpha_w\\sum_{t=0}^{H-1}\\delta_t\\nabla_wV_w(s_t)$\n",
    "      4. 更新策略网络参数:$\\theta_{t+1} = \\theta_t + \\alpha_{\\theta}\\sum_{t=0}^{H-1}\\delta_t\\nabla_{\\theta}log\\pi_{\\theta}(u_t|s_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4aa0de-10bd-4734-8f3e-cae7050ecb29",
   "metadata": {},
   "source": [
    "## IV. AC算法在实践中的design choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2071d1e2-41f7-4b00-be49-abdee3291c4f",
   "metadata": {},
   "source": [
    "### IV.1 从off-line到online actor-critic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0c9055-0af7-4725-b684-807f45364c00",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### IV.1.1 基础的online AC：每次处理一个time-step 数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc3aa61-33c8-47ed-a1dc-c7f240dd669d",
   "metadata": {},
   "source": [
    "- <font color=brown>[ref: cs285 slides]</font>\n",
    "- 说明：\n",
    "  1. 更新策略网络和状态价值网络参数的顺序不重要\n",
    "  2. 状态价值网络用DNN，目标都是$min_w L(w) = \\frac{1}{2}\\sum_i\\|\\hat v(s_i)-y_i\\|^2$\n",
    "  3. <font color=orange>动手学强化学习版本和这里的区别，除了这里改成了on-line之外，还有：这里更新策略网络参数时用更新后的状态价值网络重新计算了一次advantage，而前者每次迭代只算一次advantage，用它同时更新两个网络。**这两种方式差异不大**</font>\n",
    "- <font color=blue>**on-policy online算法伪码**</font>：\n",
    "    - 初始化参数$\\theta$\n",
    "    - 迭代：\n",
    "      1. 用当前策略采样一个time-step，$\\{s_t,a_t,r_{t+1},s_{t+1}\\}$\n",
    "      2. 计算$\\hat y_t = r_{t+1}+\\gamma \\hat{v}(s_{t+1,w_t})$\n",
    "      3. 用min MSE(mean squared error)为目标更新状态价值网络参数：$ \\nabla_w L(w) = (\\hat v(s_t,w_t)-y_t)\\nabla_w\\hat v_w(s_t)$\n",
    "      4. 计算$\\hat A_t = r_{t+1}+\\gamma \\hat{v}(s_{t+1,w})-\\hat{v}(s_t,w)$\n",
    "      5. 更新策略网络参数:$\\theta_{t+1} = \\theta_t + \\alpha_{\\theta}\\hat A_t\\nabla_{\\theta}log\\pi_{\\theta}(u_t|s_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b551e8-f0b7-46ae-bf3e-72ab2cbf49a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### IV.1.2 synchronized和asynchronized parallel AC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd823f9-8cf3-4e09-8dc3-7a47953b765f",
   "metadata": {},
   "source": [
    "- <img src='pics/parallel_ac.png' width='80%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cc84b2-66c6-4fc2-bdac-746bb5f225ae",
   "metadata": {},
   "source": [
    "- **synchronized parallel AC**\n",
    "  - 思路：只用单个time-step数据的online AC的variance太大，一种改进方法是同时用N个simulator跑实验，每个跑一个trajectory。这样每个time-step可以收集N个数据构成一个batch，用这个batch作为on-line AC的数据。\n",
    "  - 问题：这种方式的sample效率低，因为每个simulator的每次采样都都要等其他simulator完成后才能进行下一次采样。\n",
    "  - <font color=green>优化：将采样方式改成异步并行</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eec711-9e5f-4b16-93d6-f9c71ad76561",
   "metadata": {},
   "source": [
    "- **asynchronized parallel AC**\n",
    "  - 思路：多个simulator各自按照各自的节奏做实验，simulator的数量不一定为batch size N。所有simulator一共生成N个data就可以构成一个batch。这N个samples不一定是均匀分布在各个simulator上的。取到一个batch之后，算法做一次迭代，更新参数，然后新的参数一次性更新到所有的simulator。\n",
    "  - 问题：由于simulator不用等待彼此的进度，也不用等待算法更新参数，所以很可能取了一个batch后，算法还没有完成一次迭代，simulator中又用旧参数生成了数据。直到算法迭代完后，simulator才会用新的参数继续做实验。这就导致算法下一次取到的batch中含有旧参数生成的samples。\n",
    "  - 实践：只要样本不是too old，一般问题不大\n",
    "  - <font color=green>优化：直接将算法改成off-policy的，这样样本的生成和是取用就没有参数同步的问题</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4499107-41f5-4c98-a8c0-4f8a03a4b8bf",
   "metadata": {},
   "source": [
    "### IV.2 从on-policy到off-policy online AC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e76bd3-4af4-4132-9e7b-7e245f912ab2",
   "metadata": {},
   "source": [
    "#### IV.2.1 迭代式决定AC是on-policy算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c44758a-65ea-432f-abff-f1c908562d40",
   "metadata": {},
   "source": [
    "- REINFORCE的梯度迭代目标和梯度估计量：\n",
    "$$\\begin{align}\n",
    "g & = \\underset{\\tau\\sim P_{\\theta }(\\tau )}{E}\\left[\\sum_{t=0}^{H-1} \\nabla _{\\theta }log\\pi_{\\theta }(a_t|s_t)\\left(\\sum_{t'=t}^{H-1}R(s_{t'}, a_{t'})-b\\right)\\right ]\\\\\n",
    "\\\\\n",
    "\\hat g & = \\frac{1}{N}\\sum_{i=1}^N \\left[\\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(u_t^{(i)}|s_t^{(i)})\\left(\\sum_{t'=t}^{H-1}r(s_{t'}^{(i)}, a_{t'}^{(i)})-b\\right)\\right]\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b91c7d-e068-4b86-920f-f1c7143b5ca6",
   "metadata": {},
   "source": [
    "- A2C的梯度迭代目标和梯度估计量：\n",
    "$$\\begin{align}\n",
    "g & = \\underset{\\tau\\sim P_{\\theta }(\\tau )}{E}\\left[ \\nabla _{\\theta }\\sum_{t=0}^{H-1}log\\pi_{\\theta }(a_t|s_t)\\left(\\sum_{t'=t}^{H-1}R(s_{t'}, a_{t'})-V^{\\pi_{\\theta }}(s_t)\\right)\\right ]\\\\\n",
    "& = \\underset{\\tau\\sim P_{\\theta }(\\tau )}{E}\\left[ \\nabla _{\\theta }\\sum_{t=0}^{H-1}log\\pi_{\\theta }(a_t|s_t)[Q^{\\pi_{\\theta }}(s_{t}, a_{t})-V^{\\pi_{\\theta }}(s_t)]\\right ]\\\\\n",
    "\\\\\n",
    "\\hat g & = \\frac{1}{N}\\sum_{i=1}^N \\left[\\sum_{t=0}^{H-1}\\nabla _{\\theta }log\\pi_{\\theta }(a_t^{(i)}|s_t^{(i)})\\left[r(s_{t}^{(i)}, a_{t}^{(i)})+\\hat V^{\\pi_{\\theta }}(s_{t+1}^{(i)})-\\hat V^{\\pi_{\\theta }}(s_t^{(i)})\\right]\\right]\\\\\n",
    "online\\ AC\\ estimate:\\\\\n",
    "\\hat g & = \\nabla _{\\theta }log\\pi_{\\theta }(a_t^{(i)}|s_t^{(i)})\\left[r(s_{t}^{(i)}, a_{t}^{(i)})+\\hat V^{\\pi_{\\theta }}(s_{t+1}^{(i)})-\\hat V^{\\pi_{\\theta }}(s_t^{(i)})\\right]\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f42f28f-2f1e-42a7-9318-a3ada2abe47d",
   "metadata": {},
   "source": [
    "#### IV.2.2 AC算法中决定on-policy特征的原因分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb2365a-9679-4552-84fc-a04aefd77a12",
   "metadata": {},
   "source": [
    "- **online AC迭代式分析**：\n",
    "  - 估计量式中$\\nabla _{\\theta }\\sum_{t=0}^{H-1}log\\pi_{\\theta }(a_t|s_t)和V^{\\pi_{\\theta }}(s)$都是由当前策略决定的。因此AC的梯度估计量需要on-policy算法.\n",
    "  - <font color=deeppink>**严格地说，这里涉及算法的两个位置**</font>：  <font color=brown>[详见后文算法伪码]</font>\n",
    "    1. 状态价值网络中，要用当前策略得到$a_t=\\pi(a_t|s_t)$，然后通过emulator与环境交互得到的$r(s_t,a_t,s_{t+1})更新V^{\\pi_{\\theta }}(s)$\n",
    "    2. 计算梯度估计量时，同样要用当前策略得到的$a_t来计算\\nabla _{\\theta }\\sum_{t=0}^{H-1}log\\pi_{\\theta }(a_t|s_t)$\n",
    "  - 这两步中，用当前策略得到$a_t=\\pi(a_t|s_t)$即使在off-policy中也可以直接用现有网络通过抽样来做。所以只要解决了上面第一点中的策略依赖性，那么第2点很好处理。因为它不涉及样本，从而不需要考虑样本是否来自当前策略或者旧策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea45548-8e69-41ac-b898-f899396c3245",
   "metadata": {},
   "source": [
    "- 用MSE为目标估状态价值函数和行为价值函数时，<font color=blue>**状态价值函数和行为价值函数从样本获取signal都是通过reward，但是估计量对样本的要求不同**：</font>\n",
    "  1. 状态价值函数估计时，需要当前策略来得到$a_t$，且$s'$分布要用策略来决定，然后才能用state-action pair从环境获取reward。因此,<font color=green> $V(s_t)$的估计是on-policy的</font>。\n",
    "     - $$\\begin{align}\n",
    "V^{\\pi}(s) &= E_{a\\sim \\pi(s)}Q^{\\pi }(s,a)\\\\\n",
    "& = E_{a\\sim \\pi(s)}[r(s,a)+\\gamma V^{\\pi}(s')]\\\\\n",
    "& = r(s,\\pi(s))+\\gamma E_{s'\\sim P(s'|s,\\pi(s))}V^{\\pi}(s'),假设deterministic\\ action\\\\\n",
    "\\hat V^{\\pi_\\theta }(s_t)  \\overset{目标}{\\longrightarrow}y_t & = r(s_t, \\pi(s_t)) + \\gamma \\hat V^{\\pi_\\theta }(s_{t+1})\\\\\n",
    "\\end{align}$$\n",
    "  2. 行为价值函数估计时，state-action pair已知，所以获取reward不需要当前策略，且下一步的状态由环境决定。也因此,<font color=green>  $Q(s_t,a_t)$的估计可以是off-policy的</font>。\n",
    "     - $$\\begin{align}\n",
    "Q^{\\pi}(s,a) & = r(s,a)+\\gamma E_{s'\\sim P(s'|s, a)}V^{\\pi}(s') \\\\\n",
    "& = r(s,a)+\\gamma E_{s'\\sim P(s'|s, a),a'\\sim \\pi(s')}Q^{\\pi}(s',a')\\\\\n",
    "\\hat Q^{\\pi_\\theta }(s_t, a_t) \\overset{目标}{\\longrightarrow} y_t & = r(s_t, a_t, s_{t+1}) + \\gamma \\hat Q^{\\pi_\\theta }(s_{t+1}, a_{t+1})\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df46704-679b-421c-92a4-10d93a86c448",
   "metadata": {},
   "source": [
    "#### IV.2.3 从on-policy调整到off-policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2de8dbd-98e1-4d07-b4db-ce3e0d45fd6b",
   "metadata": {},
   "source": [
    "- on-policy和off-policy算法伪码对比：\n",
    "  - <img src='pics/on_and_off-policy_ac.png' width='95%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2efeca2-d983-4f5b-a4d0-9e6077394d19",
   "metadata": {},
   "source": [
    "- <font color=red>**还有一个容易被忽略的问题**：</font>\n",
    "  - 改成off-policy之后，$s_t\\sim P_{\\theta}(s)$的要求不满足，此时状态实际上都来自旧策略带来的分布。只是实践中这点通常不处理。\n",
    "- 此外，上面右图中off-policy的第第5步在实践中一般不用advantage，而是直接用Q，所以第4步不用做。这样处理的代价是variance大。但是由于此时是off-policy的算法，会一次迭代用N个samples构成的batch，这样可以抵消一部分variance。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d960ce-9e87-46ad-95f1-095ea331dc29",
   "metadata": {},
   "source": [
    "- 调整前后的梯度迭代式的估计量对比：\n",
    "$$\\begin{align}\n",
    "\\hat g_{on-policy} & =  \\nabla _{\\theta }log\\pi_{\\theta }(a_t^{(i)}|s_t^{(i)})\\hat A(s_t,a_t)\\\\\n",
    "& = \\nabla _{\\theta }log\\pi_{\\theta }(a_t^{(i)}|s_t^{(i)})\\left[r(s_{t}^{(i)}, a_{t}^{(i)})+\\gamma\\hat V^{\\pi_{\\theta }}(s_{t+1}^{(i)})-\\hat V^{\\pi_{\\theta }}(s_t^{(i)})\\right]\\\\\n",
    "\\\\\n",
    "\\hat g_{off-policy} & = \\frac{1}{N}\\sum_{i=1}^N\\nabla _{\\theta }log\\pi_{\\theta }(a_t^{(i)}|s_t^{(i)})\\hat Q(s_t^{(i)},a_t^{(i)})\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a9810f-3c34-4d23-9ae0-e11907e6f340",
   "metadata": {},
   "source": [
    "- off-policy AC的梯度迭代式：<font color=orange>这种算法的本质是用off-policy的方法估Q(s,a)，然后代入online AC</font>\n",
    "$$\\begin{align}\n",
    "g & = \\underset{\\tau\\sim P_{\\theta }(\\tau )}{E}\\left[ \\nabla _{\\theta }\\sum_{t=0}^{H-1}log\\pi_{\\theta }(a_t|s_t)Q^{\\pi_{\\theta }}(s_{t}, a_{t})\\right ]\\\\\n",
    "& = \\underset{s_t\\sim P_{\\theta }(s )}{E}\\left[\\underset{a_t\\sim \\pi_{\\theta }(a|s)}{E} \\nabla _{\\theta }\\sum_{t=0}^{H-1}log\\pi_{\\theta }(a_t|s_t)Q^{\\pi_{\\theta }}(s_{t}, a_{t})\\right ]\\\\\n",
    "& = \\underset{s_t\\sim P_{\\theta }(s )}{E}\\left[\\underset{a_t\\sim \\pi_{\\theta }(a|s)}{E} \\nabla _{\\theta }\\sum_{t=0}^{H-1}log\\pi_{\\theta }(a_t|s_t){\\color{red}{\\hat Q^{\\pi_{\\theta_{old}}}(s_{t}, a_{t})}}\\right ]\\\\\n",
    "实际用 & \\approx \\underset{\\color{red}{s_t\\sim P_{\\theta_{old} }}(s )}{E}\\left[\\underset{a_t\\sim \\pi_{\\theta }(a|s)}{E} \\nabla _{\\theta }\\sum_{t=0}^{H-1}log\\pi_{\\theta }(a_t|s_t){\\color{red}{\\hat Q^{\\pi_{\\theta_{old}}}(s_{t}, a_{t})}}\\right ]\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144805df-46d1-48b8-be99-b07b18838e46",
   "metadata": {},
   "source": [
    "### IV.3 A3C到GAE：更好的bias-variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0f08dd-7d5d-40d2-a786-639a56e28ae8",
   "metadata": {},
   "source": [
    "略"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
