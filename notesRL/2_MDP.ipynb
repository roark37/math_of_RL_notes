{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ea3b633",
   "metadata": {},
   "source": [
    "# MDP: markov decision process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beff702",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## I. MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6afefbf",
   "metadata": {},
   "source": [
    "### I.1 定义MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461e09d1",
   "metadata": {},
   "source": [
    "1. **Markov property：**\n",
    "   - ① 指给定当前状态，未来和过去的状态是独立的。\n",
    "   - ② 在MDP中，'Markov'是指action outcome只取决于当前state，与前序states无关。\n",
    "$$\\begin{align}\n",
    "P(S_{t+1} = s^{'}|S_t=s_t,A_t=a_t, S_{t-1},A_{t-1},..., S_0) = P(S_{t+1} = s^{'}|S_t=s_t,A_t=a_t)\\\\\n",
    "\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f19443f",
   "metadata": {},
   "source": [
    "2. **MDP的定义：在完全可观察(fully observable)的、随机的环境中，transition model符合markovian的条件下，用additive and discounted rewards (utility)做序列决策的过程。** \n",
    "   - 注意这个定义中的要素：\n",
    "   - ① <font color=blue>**fully observable environment**</font>: 指transition function和reward function可观察 \n",
    "   - ② <font color=blue>**random environment**</font>: transition function有随机性 \n",
    "   - ③ <font color=red>**markovian transition function**</font>: $T(s, a, s^{'})=P(s^{'}|s, a)$ \n",
    "   - ④ <font color=blue>**additive rewards**</font>: utility可以用rewards的其他函数形式来定义。而additive utility的优点是简单，且满足preference-independent。</font> <font color=orange>[preference independent的说明详见最后的附录]</font> \n",
    "   - ⑤ <font color=blue>**discounted utility function**</font>: 在value iteration和policy iteration的收敛性质中起到的关键作用。<font color=orange>[详见后文的收敛性分析]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066cfafc",
   "metadata": {},
   "source": [
    "3. **用MDP分析问题所需的要素：**\n",
    "   - (1)状态 \n",
    "     - ① 状态集$\\mathcal{S}$，如果状态是离散且有限的，则$\\mathcal{S}=\\{s^{(1)},s^{(2)},...,s^{(K)} \\}$。 \n",
    "     - ② 一个起始状态$s_0$\n",
    "     - ③ 一个或多个终止状态 \n",
    "   - (2)动作：$a\\in \\mathcal{A} $ \n",
    "   - (3)策略：$\\pi$：$\\pi(a|s)=P_{\\pi}(a|s)=P_{\\pi}(A=a|S=s)$ \n",
    "   - (4)回报\n",
    "     - ① 回报函数(reward function)：$R_t=R(s_t, a_t, s_{t+1})$ \n",
    "     - ② 回报的折扣因子：$\\gamma$ \n",
    "   - (5)状态转移函数(transition function)：$T(历史状态, a, 下一状态)$。<font color=blue>**马尔科夫条件下**</font>，状态转移函数可以表示为：$T(s_t, a_t, s_{t+1}) =P(s_{t+1}|s_t, a_t)$ \n",
    "<font color=orange>**图示: movements of an agent through a MDP:**</font> \n",
    "$$\\begin{align} \n",
    "& s_0\\overset{a_0}{\\rightarrow} s_1\\overset{a_1}{\\rightarrow} s_2\\overset{a_2}{\\rightarrow} s_3\\overset{a_3}{\\rightarrow}...\\\\\n",
    "& a_t\\sim P(a_t|s_t)=\\pi(s_t)\\\\ \n",
    "& s_t\\sim P(s_t|s_{t-1}, a_{t-1})=T(s_{t-1}, a_{t-1}, s_t)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06112a13",
   "metadata": {},
   "source": [
    "4. 求解MDP问题的目标：求policy $\\pi(s)$\n",
    "   - ① 策略的质量由该策略导致的一系列环境状态s对应的回报r折现得到的效用的期望值$E(U|\\pi)$决定。（expected utility of the possible environment histories generated by that policy）\n",
    "$$\\begin{align} \n",
    "E(U|\\pi) & = E( {\\textstyle \\sum_{t=0}^{n}} R_t|\\pi) \\\\\n",
    "& = E( {\\textstyle \\sum_{t=0}^{n}} R(s_t, a_t, s_{t+1})|\\pi) \\\\\n",
    "& = E(R_0+\\gamma R_1+\\gamma ^2R_2+...+\\gamma ^nR_n|\\pi )\n",
    "\\end{align}$$\n",
    "   - ② 最优策略：\n",
    "$$\\begin{align} \n",
    "\\pi ^*=\\underset{\\pi}{argmax}\\ E(U|\\pi)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1dc116-1559-4744-8296-c0274dbdf743",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### I.2 决策时限和策略的稳定性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb742fd",
   "metadata": {},
   "source": [
    "MDP问题考察的时间长度可能是finite horizon或者infinite horizon的，两种情况下最优决策会有所不同。 \\\n",
    "<font color=green>finite horizon的意思是说，有一个固定的时间长度n，在这个时间之后的rewards不影响utility。比如，下围棋约定只能各走20子，20子之后，就算胜负扭转也不计入结果。</font>\n",
    "1. 在<font color=orange>**finite horizon**</font>条件下做决策: 最优决策是<font color=blue>**不稳定的(optimal policy is nonstationary)**</font>  \\\n",
    "· 含义：<font color=purple>如果决策时间长度是finite horizon的，那么最优策略不仅取决于状态s，还取决于剩余的time horizon。这里“不稳定”是指最优决策会受time horizon的变化而变化。</font> \\\n",
    "· 例子：当前状态离终点有两条路径，一条路径近但是路上有掉进坑(比如utility为$-100$的state)的风险；另一条路径远，但是路上没有掉进坑的风险。如果time horizon是无限的，那么最优选择是走原路。但如果time horizon短，没有足够时间走远路，那就只能冒风险走又掉进坑的可能性的那条路。\\\n",
    "<font color=red>注：要区分time horizon和agent的步数(number of timesteps)。infinite horizon并不意味着会走无限步。在infinite horizon条件下，agent一旦走到terminal state一样会停止，实际走的步数仍然是有限的。不过，如果$\\gamma=1$，那么可能出现最优决策是不走向terminal state，此时实际步数是无限的。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3f59b0",
   "metadata": {},
   "source": [
    "2. 在<font color=orange>**infinite horizon**</font>条件下做决策: 最优决策是<font color=blue>**稳定的(optimal policy is stationary)**</font> \\\n",
    "· 含义：<font color=purple>如果决策时间长度是infinite horizon的，那么最优action a只取决于当前状态s，与当前所处的time无关，也与起点时刻的state $s_0$无关。</font>这也意味着$\\pi^*$与$s_0$无关。当然具体的action sequence和他们对应的实际得到的utility结果与起点时刻的state有关。 \\\n",
    "$$\\pi^*=\\pi^*_s,\\ \\forall s\\in S$$\n",
    "· 证明：<font color=blue>可以用bellman operator是contraction function的性质证明。参考后面的policy iteration的收敛性证明。</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2fc655-d5fd-4b83-b9d1-fcdd57b7f12b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## II 行为价值函数和状态价值函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b386bd-2e53-42dc-be7c-e8bdd36aa2c2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### II.1 概念定义"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4480935e",
   "metadata": {},
   "source": [
    "1. **行为价值函数**：给定策略$\\pi$，start state为s，start action为a，期望效用：\n",
    "$$\\begin{align} \n",
    "  Q^{\\pi}(s, a) =E(U|\\pi,s_0=s, a_0=a)  = E( {\\textstyle \\sum_{t=0}^{n}} \\gamma ^tR_t|\\pi,s_0=s, a_0=a)\\\\\n",
    "\\\\\n",
    "\\end{align}$$\n",
    "2. **状态价值函数**：给定策略$\\pi$，start state为s，期望效用：\n",
    "$$\\begin{align} \n",
    "V^{\\pi}(s)=E(U|\\pi,s_0=s)  = E_AQ^{\\pi}(s, a)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfd49cf-06cf-4bab-b314-64be5f2dc74c",
   "metadata": {},
   "source": [
    "3. **最优行为价值函数**：start state为s，start action为a，从下一步开始用最优策略达到的期望效用：\n",
    "$$\\begin{align} \n",
    "Q^{*}(s, a)=\\underset{\\pi}{max}\\ Q^{\\pi}(s, a)=\\underset{\\pi}{max}\\ E(U|\\pi, s_0=s, a_0=a)\\\\\n",
    "\\\\\n",
    "\\end{align}$$\n",
    "4. **最优状态价值函数**：start state为s，用最优策略达到的期望效用：\n",
    "$$\\begin{align} \n",
    "V^*(s)=V^{\\pi^*}(s)=\\underset{\\pi}{max}\\ V^{\\pi}(s)=\\underset{\\pi}{max}\\ E(U|\\pi,s_0=s)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73c13e0-b71d-4a32-8c6b-abffe546b7a1",
   "metadata": {},
   "source": [
    "5. **最优策略**：\n",
    "$$\n",
    "\\pi^*=\\underset{\\pi}{argmax}\\ V^{\\pi}(s), \\forall s\\in \\mathcal{S}\n",
    "$$\n",
    "注1：<font color=green>$\\pi^*=\\pi^*_s$表示start state为s时的整体策略，$\\pi^*(s)$表示当前state是s，当前步的最优策略。\n",
    "$$\\begin{align} \n",
    "\\pi^*(s)=\\underset{a}{argmax}Q^{*}(s, a)\\\\\n",
    "\\end{align}$$</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f986316-e16e-4a50-917b-9584c7ccb47c",
   "metadata": {},
   "source": [
    "6. 相互关系：\n",
    "<font color=blue>$$\\begin{matrix}\n",
    " Q^{\\pi}(s, a) & \\overset{\\pi=\\pi^*}{\\rightarrow} & Q^{*}(s, a)\\\\\n",
    "|& & |\\\\\n",
    " {\\scriptsize E_{A\\sim \\pi}Q^{\\pi}(s, A)}   &  & {\\scriptsize E_{A\\sim \\pi^{*}}Q^{*}(s, A)}\\\\\n",
    "|& & |\\\\\n",
    " V^{\\pi}(s) & \\overset{\\pi=\\pi^*}{\\rightarrow} & V^{*}(s)\n",
    "\\end{matrix}$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1deacac-d740-4f5f-aaea-3a5dbe1a7947",
   "metadata": {},
   "source": [
    "### II.2 效用的迭代形式: Reward function和Utility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5cb7f7-ce8e-46bf-ac27-e433b70c48bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### II.2.1 reward function形式表述两种价值函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6701862f",
   "metadata": {},
   "source": [
    "1. 行为价值函数的效用迭代形式：已知(a, s)，此时只考虑下一状态的分布\n",
    "$$\n",
    "\\begin{align}\n",
    "E(R|a, s) & = E_{T}(R|s, a) , \\ T指状态转移函数  \\\\\n",
    "& = E_{P(s^{'}|s, a)}(R|s, a)   \\\\\n",
    "& = \\sum_{s^{'}\\in \\mathcal{S}} P(s^{'}|s, a)R(s, a, s^{'})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff08aaf-37ce-48bf-8fe4-bcb6944cc54f",
   "metadata": {},
   "source": [
    "2. 状态价值函数的效用迭代形式：已知(s)，此时同时考虑action的分布和下一状态的分布 \n",
    "$$\n",
    "\\begin{align}\n",
    "E(R|s) & = E_{a\\sim A}[E(R|s, a)]   \\\\\n",
    "& = E_{a\\sim A}[\\sum_{s^{'}\\in \\mathcal{S}} P(s^{'}|s, a)R(s, a, s^{'})] \\\\\n",
    "& = \\sum _{a\\in \\mathcal{A}}\\pi (a|s)\\sum_{s^{'}\\in \\mathcal{S}} P(s^{'}|s, a)R(s, a, s^{'})\n",
    "\\end{align} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df23589a-bb14-42b8-8f76-9634eb86d3da",
   "metadata": {},
   "source": [
    "#### II.2.2 效用函数的迭代形式和性质"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82016422",
   "metadata": {},
   "source": [
    "1. time horizon有限的条件下\n",
    "$$\n",
    "\\begin{align}\n",
    "U_t\n",
    "& =R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2}+...+\\gamma^{n} R_{t+n} \\\\\n",
    "& =R_t+\\gamma \\left [ R_{t+1}+\\gamma R_{t+2}+...+ \\gamma^{n-1} R_{t+n} \\right ] \\\\\n",
    "& =R_t+\\gamma U_{t+1}\n",
    "\\end{align} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bc6d7e-7d2a-44d8-802d-cb856e3e73d0",
   "metadata": {},
   "source": [
    "2. time horizon无限的条件下\n",
    "$$\n",
    "\\begin{align}\n",
    "U_t\n",
    "& =R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2}+...+\\gamma^{n} R_{t+n}+... \\\\\n",
    "& =R_t+\\gamma \\left [ R_{t+1}+\\gamma R_{t+2}+...+ \\gamma^{n-1} R_{t+n}+...  \\right ] \\\\\n",
    "& =R_t+\\gamma U_{t+1}\n",
    "\\end{align} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e28c79-db4e-40ef-88ab-d96b8ebf3737",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### II.2.3 time horizon无限的条件下，期望效用的性质"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327a2f7b-bc2e-4130-b2c8-97d143cabc11",
   "metadata": {},
   "source": [
    "<font color=orange>**time horizon无限的条件下，期望效用与starting time无关。**</font>\n",
    "$$\n",
    "\\color{orange}  {E(U_{t=i}|s) =E(U_{t=j}|s)=E(U|s) , 其中i\\ne j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bee1d31-6cd8-41da-bb55-e6ae4378a11b",
   "metadata": {},
   "source": [
    "- <font color=grey>证明：只要证明$E(U_{0}|s) =E(U_{t}|s)$即可 \\\n",
    "$$\n",
    "\\begin{align}\n",
    "E(U_0|s)\n",
    "& = E(R_0+\\gamma R_{1}+\\gamma^2 R_{2}+...+\\gamma^{n} R_{n}+...|s) \\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'}) +\\gamma E(U_1|s^{'}) \\right ] \\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)R(s, a, s^{'}) +\\gamma \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)E(U_1|s^{'})  \\\\\n",
    "E(U_t|s)\n",
    "& = E(R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2}+...+\\gamma^{n} R_{t+n}+...|s) \\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma E(U_{t+1}|s^{'}) \\right ] \\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)R(s, a, s^{'}) +\\gamma \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)E(U_{t+1}|s^{'})  \\\\\n",
    "\\end{align} \n",
    "$$\n",
    "上面两个式子中展开的第一项相同，同样的道理继续展开他们各自对应的$E(U_1|s^{'})$和$E(U_{t+1}|s^{'})$，在infinite horizon条件下，只要reward function、策略$\\pi$和状态转移函数相同，那么后面所有展开的结果必然相同。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9562c0-b785-41d5-9af4-a3b1b88c812b",
   "metadata": {},
   "source": [
    "## III. 状态价值函数的Bellman equation形式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57225daa-e740-4e97-87d3-a7d413543253",
   "metadata": {},
   "source": [
    "### III.1 Bellman方程的定义和性质"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc57d95-6cbb-483c-b496-b0d59d39c7ba",
   "metadata": {},
   "source": [
    "#### III.1.1 什么是Bellman equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e8e161-0148-48b7-bebe-4c2a2ae4697a",
   "metadata": {},
   "source": [
    "1. Bellman equation中的基本思想：\n",
    "   - <font color=green>Bellman equation是markov条件下，state value function的一种表达方式。</font>\n",
    "   - 这种表达方式强调：一个状态s的价值可以分解为immediate reward(当前step的reward)和future reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b586f0ba-8475-430e-917d-7932a3660f2e",
   "metadata": {},
   "source": [
    "2. <font color=blue>**Bellman Equation**</font>: <font color=brown>[推导过程见附录2]</font>\n",
    "$$\\begin{align}\n",
    "V^{\\pi}(s) & = E(R_t|s_t=s) + \\gamma E(U_{t+1}|s_t=s)\\\\\n",
    "& = \\underset{inmmediate\\ rewards}{\\underbrace{\\sum_{s'\\in \\mathcal{S}}P(s'|s)R(s, s')}} + \\underset{future\\ rewards}{\\underbrace{\\gamma \\sum_{s'\\in \\mathcal{S}}P(s'|s)V^{\\pi }(s')}} \\\\\n",
    "\\\\\n",
    "& = \\underset{inmmediate\\ rewards}{\\underbrace{\\sum_{a\\in \\mathcal{A} }\\pi(a|s)\\sum_{s'\\in \\mathcal{S}} P(s'|s, a)R(s, a, s')}} + \\underset{future\\ rewards}{\\underbrace{\\gamma \\sum_{a\\in \\mathcal{A} }\\pi(a|s)\\sum_{s'\\in \\mathcal{S}}P(s'|s, a)V^{\\pi }(s')}} \\\\\n",
    "\\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left ( R(s, a, s^{'})+\\gamma V^{\\pi}(s^{'}) \\right )\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33ee255-1a47-4627-9711-efc82289b218",
   "metadata": {},
   "source": [
    "3. <font color=blue>**Bellman Optimal Equation(BOE)是Bellman Equation的最优解形式**</font>：它也是一个contraction\n",
    "$$\\begin{align}\n",
    "V^*(s)& =\\underset{\\pi }{max}V^{\\pi}(s)\\\\\n",
    "& =\\underset{\\pi }{max} \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left ( R(s, a, s^{'})+\\gamma V^{\\pi}(s^{'}) \\right )\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b41cd8d-b0fd-48f6-99ef-2a2c995cbc2b",
   "metadata": {},
   "source": [
    "4. <font color=blue>**Bellman Equation扩展到action value functionn**</font>\n",
    "   - 下式是行为价值函数的Bellman Equation扩展形态，也适用Bellman equation的一些性质。\n",
    "$$Q^{\\pi }(s, a) = E[R+\\gamma Q^{\\pi }(S', A')|(s, a)]$$\n",
    "   - 推导过程：\n",
    "$$\\begin{align} \n",
    "Q^{\\pi }(s, a)\n",
    "& = E[R+\\gamma Q^{\\pi }(S^{'}, A^{'})|(s, a)] \\\\\n",
    "Q^{\\pi }(s, a) \n",
    "& = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ r(s, a, s^{'})+\\gamma V^{\\pi}(s^{'}) \\right ]\\\\\n",
    "& = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ r(s, a, s^{'})+\\gamma \\sum_{a^{'}\\in \\mathcal{A}}\\pi(a^{'}|s^{'})Q^{\\pi }(s^{'}, a^{'}) \\right ]\\\\\n",
    "& = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)r(s, a, s^{'}) + \\gamma \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\sum_{a^{'}\\in \\mathcal{A}}\\pi(a^{'}|s^{'})Q^{\\pi }(s^{'}, a^{'}) \\\\\n",
    "& = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)r(s, a, s^{'}) + \\gamma \\sum_{s^{'}\\in \\mathcal{S},a'\\in \\mathcal{A}}P(s^{'},a^{'}|s, a)Q^{\\pi }(s^{'}, a^{'}) \\\\\n",
    "& = ER(s, a) + \\gamma EQ^{\\pi }(S^{'}, A^{'}|s, a) \\\\\n",
    "& = E[R+\\gamma Q^{\\pi }(S^{'}, A^{'})|(s, a)] \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3df679-7ddb-455f-b9d0-3bff2765aad2",
   "metadata": {},
   "source": [
    "#### III.1.2 为什么需要Bellman equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab11bae-c68b-44ae-ac65-2d660baf430e",
   "metadata": {},
   "source": [
    "- <font color=deeppink>**关键**</font>：Bellman Equation和BOE都是contraction，contraction有迭代收敛的性质。\n",
    "   - <font color=brown>[contraction的说明见一下节]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b732be-f590-452e-aa11-c33ce627958c",
   "metadata": {},
   "source": [
    "- MDP问题的目标是求解最优策略。而最优策略是基于状态价值函数定义的：$\\pi^{*}=\\underset{\\pi}{argmax} V^{\\pi}(s)$。利用迭代法解Bellman equation给求解最优策略提供了高效的算法：<font color=brown>[算法的详细内容详见后文]</font>\n",
    "  1. <font color=blue>**value iteration**</font>:\n",
    "     - 先迭代：用BOE的迭代收敛性质，直接迭代找max V\n",
    "     - 再一次性提取最优策略：找到每种状态的最优value之后，再提取每种状态对应的最优action得到最优策略\n",
    "  2. <font color=blue>**policy iteration**</font>：<font color=deeppink>算法复杂度和value iteration一样，但是实践中收敛更快</font>\n",
    "     - 将迭代分成两步：\n",
    "       - 第一步利用Bellman方程的迭代收敛性质找当前策略的value\n",
    "       - 第二步做policy improvement\n",
    "     - 直到收敛到最优策略\n",
    "  3. <font color=blue>**Q-learning**</font>：\n",
    "     - <font color=green>根据前文的推导，行为价值函数可以写成Bellman equation的扩展形式，可以证明它也是contraction，因此也可以用可前两种算法相似的方法求解。Q-learning就是将value iteration中的状态价值函数改成行为价值函数后再做一些调整得到的算法。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848a050b-f150-4e64-acec-c6b1f289797b",
   "metadata": {},
   "source": [
    "#### III.1.3 contraction的定义和性质"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46dd5c2-b4b0-4c5b-b694-5287cb8c6af1",
   "metadata": {},
   "source": [
    "- **定义**：如果单变量函数f(x)满足:$$distance(f(a), f(b))\\le \\gamma * distance(a, b)，0\\le \\gamma<1$$则称f(x)是一个contraction。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5aa138-5ec3-4904-9855-00e3fcedc782",
   "metadata": {},
   "source": [
    "- **性质**：\n",
    "  1. <font color=brown>contraction最多只有一个固定的收敛点。序列: $x, f(x), f(f(x)), ...$会收敛到该点。</font> \n",
    "     - <font color=gray>简证：如果有两个的话，那么会向两个位置收敛，就无法满足distance缩小的条件了。</font>\n",
    "  2. <font color=brown>当$x=f(x)$时，就达到了收敛点。</font> \n",
    "     - <font color=gray>eg: f(x)=x/2是contraction。当x=0时，收敛点是0，$x=0时，x=f(0)=0$。</font>\n",
    "  3. <font color=brown>如果f(x)是contraction，则它可以用迭代法$x_{k+1}\\leftarrow f(x_k)$求解。</font> \n",
    "  4. <font color=brown>用迭代法求解contraction时，f(x)会以指数级速度向x*收敛，具体速度由$\\gamma$决定。(converge to the unique fixed point at an exponential rate)</font> [math of RL book, page53-55]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6327d5c8",
   "metadata": {},
   "source": [
    "## IV. 求解贝尔曼期望方程的两类方法 [以time horizon无限为条件]\n",
    "<font color=red>**本质：time horizon无限的条件下，求Utility的期望值**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a19497-bb5f-4fb0-aced-6f7ba97bd4fa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### IV.1 矩阵法：以state value function为例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43da36e9",
   "metadata": {},
   "source": [
    "1. 假设一共有K种状态，每种状态的期望效用可以表示为：\n",
    "$$\n",
    "\\begin{align}\n",
    "V^{\\pi}(s^{k}) & = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{i}\\in \\mathcal{S}}P(s^{i}|s, a)\\left [ R(s, a, s^{i})+\\gamma V^{\\pi}(s^{i}) \\right ] \\\\\n",
    "& = ER(s^{k}) + \\gamma \\sum _{s^{i}\\in \\mathcal{S}}P(s^{i}|s)V^{\\pi }(s^{i}) \\\\\n",
    "& k\\in {1, 2, ..., K}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f392fd8-92ee-409a-83a0-966f9acadf36",
   "metadata": {},
   "source": [
    "2. 用线性方程组方法求解：\n",
    "   - 上式可以展开为：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}\n",
    "V(s^{1}) \\\\\n",
    "V(s^{2}) \\\\\n",
    "... \\\\\n",
    "V(s^{K})\n",
    "\\end{bmatrix}=E\\begin{bmatrix}\n",
    "r(s^{1}) \\\\\n",
    "r(s^{2}) \\\\\n",
    "... \\\\\n",
    "r(s^{K})\n",
    "\\end{bmatrix} + \\gamma \\begin{bmatrix}\n",
    "p(s^{1}|s^{1})  & p(s^{2}|s^{1}) & ... & p(s^{K}|s^{1})\\\\\n",
    "p(s^{1}|s^{2})  & p(s^{2}|s^{2}) & ... & p(s^{K}|s^{2})\\\\\n",
    " ... &  &  & \\\\\n",
    "p(s^{1}|s^{K})  & p(s^{2}|s^{K}) & ... & p(s^{K}|s^{K})\\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "V(s^{1})\\\\\n",
    "V(s^{2}) \\\\\n",
    "... \\\\\n",
    "V(s^{K})\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "求解线性方程组：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{V} & = \\mathcal{R} +\\gamma \\mathcal{PV} \\\\\n",
    "(\\mathcal{I-\\gamma P})\\mathcal{V}& =\\mathcal{R}\\\\\n",
    "\\mathcal{V} & = (\\mathcal{I-\\gamma P})^{-1}\\mathcal{R}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7122afb",
   "metadata": {},
   "source": [
    "3. **解的性质**：\n",
    "   - <font color=blue>**根据线性方程组的特征，n个方程求n个未知数，解存在且唯一。**</font>\n",
    "   - <font color=red>**但是求逆的算法复杂度高，通常是$O(N^3)$，所以这种求解方法只适用于状态数量很小的场景。**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176b318e-f42a-4a4b-9722-800991580114",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### IV.2 动态规划类方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d1ab6-9d29-4cc4-9a84-aa5761d9c681",
   "metadata": {},
   "source": [
    "- <font color=green>**由于矩阵法的算法复杂度高，因此实践中，主要的求解方法是动态规划类算法。**</font>\n",
    "- 主要有三种：\n",
    "  1. value iteration\n",
    "  2. policy iteration\n",
    "  3. Q learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114a140a-4f93-4247-bfdf-2e9a86e62ffc",
   "metadata": {},
   "source": [
    "## V. 三种基于Bellman Equations的求解MDP的动态规划算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f133b7fd-020d-44c9-bb27-31bec8ded861",
   "metadata": {},
   "source": [
    "- <font color=blue>**三种方法的总体思路：**</font>  \n",
    "  - Value Iteration: 用Bellman Optimality Equation来迭代更新value functions，直到收敛\n",
    "  - Policy Iteration: 在迭代中先做policy evaluation (这步就是求解Bellman Expectation Equation)，再做policy improvement (基于新得到的value function更新策略)，直到收敛\n",
    "  - Q-Learning: 用Bellman Optimality Equation for Q-values来学习最优策略"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5721193-72fd-42f8-ada6-9fd1696d501f",
   "metadata": {},
   "source": [
    "### V.1 value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0e06d7-caa9-4f6d-bd78-e359f8388300",
   "metadata": {},
   "source": [
    "#### 1. 算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371b4f95",
   "metadata": {},
   "source": [
    "<font color=red>**理解要点：要从数学优化方法的角度来理解value iteration，不要从每一步迭代找到局部最优的角度，因为value iteration每一步迭代得到的$V_{k}(s)$都不是真的state s的value，而是数学优化方法的中间结果。（这一点和policy iteration不同，后者每次policy evaluation的结果都有具体的$\\pi$与之对应。）但迭代收敛后的$V^{*}$是真的最优state value。**</font>\n",
    "1. **value iteration**:\n",
    "   - 已知transition model $T(s,a,s^{'})$，rewards $R(s,a,s^{'})$，收敛精度$\\epsilon$，discount $\\gamma$ \n",
    "   - 初始化$V_0(s)=0$\n",
    "   - 迭代: \n",
    "$$\\begin{align} \n",
    "repeat:&&\\\\\n",
    "&\\delta=0 \\ &\\\\\n",
    "&for\\ s\\ in\\ S:&\\\\\n",
    "& \\ \\ \\ \\ \\ \\ \\ \\ V_{k+1}(s) \\leftarrow \\underset{a\\in A}{max} \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V_k(s^{'})]\\\\\n",
    "& \\ \\ \\ \\ \\ \\ \\ \\ if\\ \\delta < |V_{k+1}(s)-V_{k}(s)|,\\ then \\ \\delta = |V_{k+1}(s)-V_{k}(s)|\\\\\n",
    "until:&\\delta<\\epsilon\\frac{(1-\\gamma )}{\\gamma}&\n",
    "\\end{align}$$\n",
    "<font color=red>**注**：下标表示time horizon，不是迭代次数的index。迭代条件$\\delta < |V_{k+1}(s)-V_{k}(s)|$虽然有下标，但是最终收敛的时候$V^*(s)=V_k(s), k\\rightarrow \\infty$，即在unlimited time horizon条件下，会收敛到$V^*(s)$。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf44113-611c-4e1f-9369-af950133b697",
   "metadata": {},
   "source": [
    "2. <font color=blue>**policy extraction**: 当算法收敛后，得到$V^*(s)$，再用下式得到最优策略$\\pi^*(s)$\n",
    "$$\\begin{align} \n",
    "\\pi^*(s)& =\\underset{a\\in A}{argmax}Q^{*}(s, a)\\\\\n",
    "& =\\underset{a\\in A}{argmax} \\sum_{s^{'}}P(s^{'}|s, a)[R(s, a, s^{'})+\\gamma V^*(s^{'})] \n",
    "\\end{align}$$</font>\n",
    "3. <font color=red>每次iteration</font>的复杂度：$O(|S|^2|A|)$\n",
    "   - 分析：每个for循环内部的一轮计算是$O(|S||A|)$，每次iteration要做$|S|$次for循环。所以每次iteration的复杂度是$O(|S|^2|A|)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7edbda1-e331-4824-9f38-a38e8bdb1a43",
   "metadata": {},
   "source": [
    "#### 2. 收敛性：value iteration会收敛到唯一最优解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b8bd88",
   "metadata": {},
   "source": [
    "<font color=green>思路：利用Bellman equation满足contraction定义的特征，再用contraction的性质得到收敛的结论</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e7a5bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "- **证明1：以max norm为distance measure时，bellman operator是一个contraction。**\n",
    "  - 分析：\n",
    "    - ① max norm：将$V$看做有$|S|$个元素的vector，则其max norm可以表示为：$$\\left \\| V \\right \\|_{\\infty} = \\underset{s\\in S}{max} |V(s)|$$\n",
    "    - ② Bellman update:\n",
    "$$\\begin{align} \n",
    "& V_{k+1}(s) \\leftarrow \\underset{a\\in A}{max} \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V_k(s^{'})]\\\\\n",
    "& 取B(V(s))=\\underset{a\\in A}{max} \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V(s^{'})]\\\\\n",
    "& B(V)=\\begin{pmatrix}\n",
    "B(V(s_1)) \\\\\n",
    "B(V(s_2)) \\\\\n",
    "... \\\\\n",
    "B(V(s_m))\n",
    "\\end{pmatrix}, s\\in S=\\{s_1, s_2, ..., s_m\\}  \\\\\n",
    "& B称为Bellman\\ operator \\\\\n",
    "\\end{align}$$\n",
    "    - ③ 定义Bellman operator: B(V) \n",
    "$$\\begin{align} \n",
    "& 取B(V(s))=\\underset{a\\in A}{max} \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V(s^{'})], s\\in S=\\{s_1, s_2, ..., s_m\\} \\\\\n",
    "& B(V)=\\begin{pmatrix}\n",
    "B(V(s_1)) \\\\\n",
    "B(V(s_2)) \\\\\n",
    "... \\\\\n",
    "B(V(s_m))\n",
    "\\end{pmatrix} =\\begin{pmatrix}\n",
    "\\underset{a\\in A}{max} \\sum_{s^{'}}^{}T(s_1,a,s^{'})[R(s,a,s^{'})+\\gamma V(s^{'})] \\\\\n",
    "\\underset{a\\in A}{max} \\sum_{s^{'}}^{}T(s_2,a,s^{'})[R(s,a,s^{'})+\\gamma V(s^{'})] \\\\\n",
    "... \\\\\n",
    "\\underset{a\\in A}{max} \\sum_{s^{'}}^{}T(s_m,a,s^{'})[R(s,a,s^{'})+\\gamma V(s^{'})]\n",
    "\\end{pmatrix}\n",
    "\\end{align}$$\n",
    "    - ④ 需要证明：\n",
    "$$\\left \\| B(V)-B(\\tilde V) \\right \\|_{\\infty} \\le \\gamma \\left \\| V-\\tilde V \\right \\| _{\\infty}$$\n",
    "<font color=green>思路：$$\\begin{align} \n",
    "& \\because \\left \\| B(V)-B(\\tilde V) \\right \\|_{\\infty}=\\underset{s\\in S}{max} \\left | B(V(s))-B(\\tilde V(s)) \\right | \\\\\n",
    "& \\therefore 只要证明，对\\forall s都有\\left | B(V(s))-B(\\tilde V(s)) \\right |\\le \\gamma \\left \\| V-\\tilde V \\right \\| _{\\infty}即可 \\\\\n",
    "\\end{align}$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236b85d1",
   "metadata": {},
   "source": [
    "- 证明1（续） \n",
    "  - ⑤ 证明过程：\n",
    "       - 利用任意不等式都有的性质：$$|max\\ f(x)-max\\ h(x)|\\le max|f(x)-h(x)|$$\n",
    "$$\\begin{align} \n",
    "& |BV(s)-B\\tilde V(s)| \\\\\n",
    "& =\\left|\\underset{a\\in A}{max} \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V(s^{'})]-\\underset{a\\in A}{max} \\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma \\tilde V(s^{'})]\\right|\\\\\n",
    "& \\le \\underset{a\\in A}{max}\\left |\\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V(s^{'})]-\\sum_{s^{'}}^{}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma \\tilde V(s^{'})]  \\right | \\\\\n",
    "& = \\underset{a\\in A}{max}\\left |\\gamma \\sum_{s^{'}}^{}T(s,a,s^{'})V(s^{'}) - \\gamma \\sum_{s^{'}}^{}T(s,a,s^{'})\\tilde V(s^{'})\\right | \\\\\n",
    "& = \\gamma*\\underset{a\\in A}{max}\\left |\\sum_{s^{'}}^{}T(s,a,s^{'})[V(s^{'})-\\tilde V(s^{'})]  \\right | \\\\\n",
    "& \\le \\gamma*\\underset{a\\in A}{max}\\left |\\sum_{s^{'}}^{}T(s,a,s^{'})\\underset {s^{'}}{max}|V(s^{'})-\\tilde V(s^{'})| \\right | \\\\\n",
    "& = \\gamma*\\underset {s^{'}}{max}\\left | V(s^{'})-\\tilde V(s^{'}) \\right |=\\gamma \\left \\| V-\\tilde V \\right \\|_{\\infty}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0680fe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "3. **证明2：value iteration会收敛到唯一最优解** \n",
    "   - 证明：\n",
    "     - Bellman equation迭代的过程相当于iterated sequence：$x, f(x), f(f(x)), ...$。\n",
    "     - 根据contraction的性质，iterated sequence会收敛到固定值，且当$x=f(x)$时到达该收敛点。\n",
    "     - 因此，$B(V)$会收敛到唯一的固定值：\n",
    "$$B(V)=\\begin{pmatrix}\n",
    "B(V(s_1)) \\\\\n",
    "B(V(s_2)) \\\\\n",
    "... \\\\\n",
    "B(V(s_m))\n",
    "\\end{pmatrix} \\rightarrow \\begin{pmatrix}\n",
    "V^*(s_1) \\\\\n",
    "V^*(s_2) \\\\\n",
    "... \\\\\n",
    "V^*(s_m)\n",
    "\\end{pmatrix}=V^* \\\\\n",
    "且此时，V^*=B(V^*)=\\begin{pmatrix}\n",
    "B(V^*(s_1)) \\\\\n",
    "B(V^*(s_2)) \\\\\n",
    "... \\\\\n",
    "B(V^*(s_m))\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac7c0b8-43b2-454c-aa81-e85a489ab56e",
   "metadata": {},
   "source": [
    "#### 3. 收敛速度：rate of convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a2717",
   "metadata": {},
   "source": [
    "1. <font color=blue>**value iteration converges exponentially fast.**</font> \\\n",
    "分析：$取error_k = \\left \\| B(V_{k})-B(V^*) \\right \\|_{\\infty}$\n",
    "$$\\begin{align} \n",
    "\\because \\left \\| B(V_{k+1})-B(V^*) \\right \\|_{\\infty} & \\le \\gamma *\\left \\| V_{k+1}-V^* \\right \\|_{\\infty} \\\\\n",
    "& = \\gamma *\\left \\| B(V_{k})-V^* \\right \\|_{\\infty} \n",
    "\\end{align}$$\n",
    "$$\\begin{align} \n",
    "\\therefore \\frac{\\left \\| B(V_{k+1})-B(V^*) \\right \\|_{\\infty}}{\\left \\| B(V_{k})-B(V^*) \\right \\|_{\\infty}}\\le \\gamma \\\\\n",
    "\\\\\n",
    "\\end{align}$$\n",
    "2. 要让收敛精度达到$\\epsilon$，则需要的迭代条件是$\\left \\| V_{k+1}-V_k \\right \\|_{\\infty} \\le \\epsilon (1-\\gamma)/\\gamma$。因为： \\\n",
    "$$if\\ \\left \\| V_{k+1}-V_k \\right \\|_{\\infty} \\le \\epsilon (1-\\gamma)/\\gamma,\\ \\ then\\ \\left \\| V_{k+1}-V^* \\right \\|_{\\infty} \\le \\epsilon$$\n",
    "分析：(略)<font color=blue>[详见AI：a modern approach 4th ch17.2 page575] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f063498-dfc0-4324-89e1-ad04f87be9c1",
   "metadata": {},
   "source": [
    "### V.2 policy iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b35264-fe3b-439b-9d67-8c491cf49134",
   "metadata": {},
   "source": [
    "出发点：策略的收敛通常比value收敛发生的早很多。而bellman equation本身的目标就是找到最优策略。因此一个思路是直接求策略，而不是先求optimal value再得到对应的策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aff95a6-792a-4a0b-9888-522dd5e7231f",
   "metadata": {},
   "source": [
    "#### 1. 算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4de5e2",
   "metadata": {},
   "source": [
    "- **思路**: <font color=green>每次迭代先做policy evaluation，再做policy improvement，直到policy evaluation得到的utility没有变化，也即utility收敛到了contraction的收敛点。此时policy extraction得到的就是最优策略。</font>\n",
    "  - **step1**: <font color=blue>**policy evaluation**</font>：给定策略$\\pi_i$，对所有的states s计算value/utility:\n",
    "$$\\begin{align} \n",
    "迭代至收敛：&\\\\\n",
    "& V^{\\pi_i}(s)\\leftarrow \\sum_{s^{'}}^{}T(s,\\pi_i(s),s^{'})[R(s,\\pi_i(s),s^{'})+\\gamma V^{\\pi_i}(s^{'})]\\\\\n",
    "\\end{align}$$\n",
    "    - <font color=red>注：收敛条件不应该表达为下式，因为下标k表示action的time horizon，它只是在迭代没有收敛的阶段标记time horizon。而在unlimited time horizon的环境下，一旦收敛之后，time horizon的变化已经不会带来utility大小的改变。</font>\n",
    "$$V^{\\pi_i}_{k+1}(s)\\leftarrow \\sum_{s^{'}}^{}T(s,\\pi_i(s),s^{'})[R(s,\\pi_i(s),s^{'})+\\gamma V^{\\pi_i}_{k}(s^{'})]$$\n",
    "   - **step2**: <font color=blue>**policy improvement/extraction**</font>：向前看一步，更新策略\n",
    "$$\\begin{align} \n",
    "\\pi_{i+1}(s) & \\leftarrow \\underset{a\\in A}{argmax}Q^{\\pi_i}(s, a)\\\\\n",
    "& =\\underset{a\\in A}{argmax} \\sum_{s^{'}}P(s^{'}|s, a)[R(s, a, s^{'})+\\gamma V^{\\pi_i}(s^{'})] \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf1610e-a219-41c3-8452-33fcdcc0f7a2",
   "metadata": {},
   "source": [
    "- **伪码**：\n",
    "  - 已知transition model $T(s,a,s^{'})$，rewards $R(s,a,s^{'})$，收敛精度$\\epsilon$，discount $\\gamma$ \n",
    "  - 初始化：$V_0^{\\pi}(s)=0$，任意初始化$\\pi_0$ \n",
    "  - 迭代: \n",
    "$$\\begin{align} \n",
    "repeat:&\\\\\n",
    "& \\#\\ policy\\ evaluation \\\\\n",
    "&for\\ s\\ in\\ S: \\\\\n",
    "& \\ \\ \\ \\ \\ \\ \\ \\ V^{\\pi_i}(s)\\leftarrow \\sum_{s^{'}}^{}T(s,\\pi_i(s),s^{'})[R(s,\\pi_i(s),s^{'})+\\gamma V^{\\pi_i}(s^{'})] \\\\\n",
    "& \\#\\ policy\\ improvement/extraction \\\\\n",
    "& for\\ s\\ in\\ S: \\\\\n",
    "& \\ \\ \\ \\ \\ \\ \\ \\ \\pi_{i+1}(s) \\leftarrow\\underset{a\\in A}{argmax} \\sum_{s^{'}}P(s^{'}|s, a)[R(s, a, s^{'})+\\gamma V^{\\pi_i}(s^{'})]\\\\\n",
    "until:&\\  \\pi\\ stable\\\\\n",
    "return:&\\ \\pi\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d405da0c-e678-4333-96c5-9c8c89c327a2",
   "metadata": {},
   "source": [
    "#### 2. 算法复杂度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6407f830",
   "metadata": {},
   "source": [
    "- **policy evaluation** \n",
    "  - ① 精确求解(实践中很少用)：直接求解线性方程组，那么正常情况下是$O(|S|^3)$。如果利用状态转移函数的稀疏性，可以大幅降低复杂度。\n",
    "  - ② 近似求解：用迭代法得到$V^{\\pi}$的近似估计值\n",
    "$$V^{\\pi}(s)\\leftarrow \\sum_{s^{'}}^{}T(s,\\pi(s),s^{'})[R(s,\\pi(s),s^{'})+\\gamma V^{\\pi}(s^{'})]$$\n",
    "单次迭代的复杂度是$O(|S|^2)$ \n",
    "- **policy improvement**: $O(|S|^2|A|)$\n",
    "  - 分析：每个for循环内部的一轮计算是$O(|S||A|)$，每次iteration要做$|S|$次for循环。所以每次iteration的复杂度是$O(|S|^2|A|)$\n",
    "  - <font color=red>分析：直接看算法复杂度时，policy iteration的每次迭代复杂度是$O(|S|^2|A|)=O(|S|^2|A|)+O(|S|^2)$。从单次迭代的复杂度来看，它和value iteration在同一水平上。但policy iteration需要的总的迭代次数更少。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99759dba-29f7-45e1-ace4-51fe951156f6",
   "metadata": {},
   "source": [
    "#### 3. 收敛性：收敛到唯一最优解\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4c24f6",
   "metadata": {},
   "source": [
    "step1：**证明：迭代法做policy evaluation可以收敛，且会收敛到$V^{\\pi}(s)$的唯一解。** \n",
    "  - (1)value iteration of fixed policy对应的bellman operator是一个contraction。 \n",
    "$$V^{\\pi}(s)\\leftarrow \\sum_{s^{'}}^{}T(s,\\pi(s),s^{'})[R(s,\\pi(s),s^{'})+\\gamma V^{\\pi}(s^{'})]$$\n",
    "<font color=red>注，下面$BV(s)$是$BV^{\\pi}(s)$的简写，指固定策略后的value，其它同理。</font>\n",
    "$$\\begin{align} \n",
    "& |BV(s)-B\\tilde {V}(s)|， \\\\\n",
    "& = \\left |\\sum_{s^{'}}^{}T(s,\\pi(s),s^{'})[R(s,\\pi(s),s^{'})+\\gamma V(s^{'})]-\\sum_{s^{'}}^{}T(s,\\pi(s),s^{'})[R(s,\\pi(s),s^{'})+\\gamma \\tilde V(s^{'})]  \\right | \\\\\n",
    "& = \\left |\\gamma \\sum_{s^{'}}^{}T(s,\\pi(s),s^{'})V(s^{'}) - \\gamma \\sum_{s^{'}}^{}T(s,\\pi(s),s^{'})\\tilde V(s^{'})\\right | \\\\\n",
    "& = \\gamma*\\left |\\sum_{s^{'}}^{}T(s,\\pi(s),s^{'})[V(s^{'})-\\tilde V(s^{'})]  \\right | \\\\\n",
    "& \\le \\gamma*\\left |\\sum_{s^{'}}^{}T(s,\\pi(s),s^{'})\\underset {s^{'}}{max}|V(s^{'})-\\tilde V(s^{'})| \\right | \\\\\n",
    "& = \\gamma*\\underset {s^{'}}{max}\\left | V(s^{'})-\\tilde V(s^{'}) \\right |   \\\\\n",
    "& =\\gamma \\left \\| V-\\tilde V \\right \\|_{\\infty}\n",
    "\\end{align} $$\n",
    "  - (2)证明：value iteration of fixed policy会收敛到唯一解。 \n",
    "    - 直接用contraction性质。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1d425a",
   "metadata": {},
   "source": [
    "step2： **每次policy improvement之后，新得到的policy一定比之前的policy有更好的value。**\n",
    "$$\n",
    "V^{\\pi^{'}}(s)\\ge V^{\\pi}(s), \\forall s\\in \\mathcal{S}\n",
    "$$\n",
    "  - 证明[详见西湖书4.2.1 page73]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abdf8b7-cc70-43ac-bfdb-9ebd87909294",
   "metadata": {},
   "source": [
    "step3：**policy evaluation和policy improvement的迭代可以收敛到$V^{*}(s), \\forall s\\in \\mathcal{S}$。**\n",
    "  - 证明思路：每一轮policy evaluation + policy improvement的迭代完成后得到的$V^{\\pi}(s)$比value iteration每次迭代后的值更大，更接近$V^{*}(s)$。而value iteration可以收敛，因此policy iteration也能收敛。[详见西湖书4.2.1 page75]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc6399d-85b0-4c10-9270-f7896688ae22",
   "metadata": {},
   "source": [
    "### V.3 Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0e3027-33fa-45a0-9dfb-be225a9a8db6",
   "metadata": {},
   "source": [
    "- <font color=brown>略，详见TD章节。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36b0344-c9fe-4ebf-83be-a4f50d2314e1",
   "metadata": {},
   "source": [
    "## VI. 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18c5b6a-850f-4f80-b984-dc0dee93e35a",
   "metadata": {},
   "source": [
    "#### 1. value iteration和policy iteration的适用场景"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33910ab8-0e68-4752-981a-f08e3db3739f",
   "metadata": {},
   "source": [
    "- <font color=blue>**使用条件**</font>：\n",
    "  - 几个核心条件：\n",
    "    1. 满足MDP环境中的所有条件\n",
    "    2. 离散的state-action space，也就是状态s和行为a都离散且有限\n",
    "    3. infinite horizon\n",
    "    4. 状态转移矩阵已知\n",
    "  - 有了这三个条件才能写出Bellman Equation及另外两种扩展形式BOE和Bellman Equation for action value。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af3a68f-9c1b-4f37-bf31-63616c772e67",
   "metadata": {},
   "source": [
    "- <font color=blue>**按照Bellman Equation的形式可以推出最优策略有以下特点**</font>：\n",
    "  1. 最大行为价值$Q^*(s, a)$和最大状态价值$V^*(s)$存在且唯一\n",
    "  2. 最优策略$\\pi^*(a|s)$存在且稳定，但不一定唯一。\n",
    "     - 因为在某个状态s下，可能存在$Q^*(s, a_1)=Q^*(s, a_2)$的情况。此时$\\pi_1(s)=a_1和\\pi_2(s)=a_2$这两个策略的回报一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec73076-68e9-4dfe-926e-20281c0240ee",
   "metadata": {},
   "source": [
    "#### 2. <font color=red>方法的不足</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a8f183-e567-40ba-b16b-4ad4d5c8f0e4",
   "metadata": {},
   "source": [
    "1. 现实中，状态转移矩阵$P(s'|s, a)$和reward函数往往并不知道，也就是没有关于environment的真实model，只能用sampling based approximation，这就衍生了关于MC的一系列方法。\n",
    "2. state-action space很可能是连续的，无法枚举。即使state-action space是离散的，如果s和a的取值空间太大，此时由于上面的两种算法也不可行，因为复杂度太高。一种思路是用函数拟合来估计Q function，典型方式是用神经网络作为函数的拟合器:$Q^{\\pi}(s, a)=NN(s, a;\\theta_{\\pi})$\n",
    "- <font color=green>**以上改进方法对应产生了Q-learning和approximate Q-learning等其他算法。**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fa5251",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 附录"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb8dc57-190d-4c09-b4f2-b94186279dd7",
   "metadata": {},
   "source": [
    "### 附录1：对preference independent性质的说明"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c044c1-cffe-4f2b-96ae-33394cbc8e2f",
   "metadata": {},
   "source": [
    "1. 定义: Utility function的自变量称为attributes。有attributes X,Y。如果preferences for different levels of X do not depend on the level of Y, and vice versa就称他们是preference independent的。 \n",
    "2. 数学表达: 如果X和Y是preference independent的, the utility function U(X,Y)可以写成U(X,Y) = f(X) + g(Y)。\n",
    "3. 例子：如果改变Y的大小会影响X的utility，那么就不满足preference independent条件。比如当苹果的拥有量很高的时候，拥有更多香蕉的效用会下降，因为他们都是甜水果。但是当苹果拥有量很多的时候，不会影响拥有汽车的效用。此时苹果和汽车是preference independent的，但苹果和香蕉不是。\n",
    "4. 应用说明：在MDP设定下，preference independent按含在markov假设中。每个time step上的states就是attributes。$R_t$只取决于$S_t$，不受其他时点上的states影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c93790-ea94-41df-a999-930b22ab3101",
   "metadata": {},
   "source": [
    "### 附录2：Bellman Equation的推导过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c710d856-a96b-426b-9ccd-2f41d9a91130",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 1. time horizon无限的条件下，状态价值函数的Bellman equation形态\n",
    "$$\n",
    "\\color{Blue}  {V^{\\pi}(s)  = E(U|s) = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma V^{\\pi}(s^{'}) \\right ]} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ef4321-fa87-41bf-b934-4ae563d67c88",
   "metadata": {},
   "source": [
    "- **第一种推导方式**："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e99a4ee-f000-411b-bcff-30552d23b139",
   "metadata": {},
   "source": [
    "$$\\color{grey}{\\begin{align}\n",
    "V^{\\pi}(s) & = E(U|s) \\\\& = E(U_t|s_t=s) \\\\\n",
    "& = E(R_t + \\gamma U_{t+1}|s_t=s)\\\\\n",
    "& = E(R_t|s_t=s) + \\gamma E(U_{t+1}|s_t=s)\\\\\n",
    "\\\\\n",
    "E(R_t|s_t=s)  \n",
    "& = \\sum_{s'\\in \\mathcal{S}}R(s, s')P(s'|s)\\\\\n",
    "& = \\sum_{a\\in \\mathcal{A} }\\pi(a|s)\\sum_{s'\\in \\mathcal{S}} P(s'|s, a)R(s, a, s') \\\\\n",
    "\\end{align}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310ac649-b273-442f-acf8-c71b720c3351",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "$$\\color{grey}{\\begin{align}\n",
    "E(U_{t+1}|s_t=s) & = \\sum_{s'\\in \\mathcal{S}}E(U_{t+1}|s_{t+1}=s', s_t=s)P(s_{t+1}=s'|s_t=s)\\\\\n",
    "& = \\sum_{s'\\in \\mathcal{S}}E(U_{t+1}|s_{t+1}=s')P(s_{t+1}=s'|s_t=s),\\  \\because Markov\\ condition\\\\\n",
    "& = \\sum_{s'\\in \\mathcal{S}}E(U|s')P(s'|s),\\  \\because infinite\\ time\\ horizon\\\\\n",
    "& = \\sum_{s'\\in \\mathcal{S}}V^{\\pi }(s')P(s'|s)\\\\\n",
    "& = \\sum_{a\\in \\mathcal{A} }\\pi(a|s)\\sum_{s'\\in \\mathcal{S}}P(s'|s, a)V^{\\pi }(s') \\\\\n",
    "\\end{align}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dd0b80-3aae-4b50-934c-f6707ba0aec3",
   "metadata": {},
   "source": [
    "- **第二种推导方式**："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dfb905-36fc-4c93-8a93-33a4708f1766",
   "metadata": {},
   "source": [
    "- 证明：\n",
    "  - (1)已知s，求效用的期望\n",
    "$$\n",
    "\\begin{align}\n",
    "E(U_0|s)\n",
    "& = E(R_0+\\gamma R_{1}+\\gamma^2 R_{2}+...+\\gamma^{n} R_{n}+...|s) \\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)E(R_0+\\gamma R_{1}+\\gamma^2 R_{2}+...+\\gamma^{n} R_{n}+...|s,a)\\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)E(R_0+\\gamma R_{1}+\\gamma^2 R_{2}+...+\\gamma^{n} R_{n}+...|s,a,s^{'})\\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma E\\left ( R_1+\\gamma R_{2}+...+\\gamma^{n-1} R_{n}+...|s, a, s^{'} \\right )  \\right ] \\\\ \n",
    "& = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma E(U_1|S_0=s, A_0=a, S_1=s^{'}) \\right ] \\\\\n",
    "& \\because 马尔科夫条件和reward函数形式决定了U_1不受S_0和A_0取值的影响 \\\\\n",
    "& = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma E(U_1|s^{'}) \\right ]\n",
    "\\end{align} \n",
    "$$\n",
    "  - **(2)time horizon无限的条件下，状态价值函数的递归形式就是Bellman方程形态**\n",
    "$$\n",
    "\\begin{align}\n",
    "\\because E(U_t|s) & =E(U|s), E(U_{t+1}|s^{'})=E(U|s^{'})\\\\\n",
    "\\therefore E(U|s) & = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma E(U|s^{'}) \\right ]\\\\\n",
    "\\therefore V^{\\pi}(s) & = E(U|s) = \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma V^{\\pi}(s^{'}) \\right ]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303b574a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2. time horizon无限的条件下，行为价值函数的Bellman equation形态\n",
    "$$\n",
    "\\color{Blue} {\n",
    "\\begin{align}\n",
    "Q^{\\pi}(s,a)  = E(U|s,a) & = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma V^{\\pi}(s^{'}) \\right ] \\\\\n",
    "& = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma \\sum_{a^{'}\\in \\mathcal{A}}\\pi(a^{'}|s^{'})Q^{\\pi}(s^{'},a^{'}) \\right ] \n",
    "\\end{align}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ca6e3f-8201-4d11-adb5-060c4fd7cd69",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "- 证明：  \n",
    "  - (1)已知(s,a),求效用的期望\n",
    "$$\n",
    "\\begin{align}\n",
    "E(U_0|s,a)\n",
    "& = E(R_0+\\gamma R_{1}+\\gamma^2 R_{2}+...+\\gamma^{n} R_{n}+...|s,a) \\\\\n",
    "& = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)E(R_0+\\gamma R_{1}+\\gamma^2 R_{2}+...+\\gamma^{n} R_{n}+...|s,a,s^{'})\\\\\n",
    "& = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma E\\left ( R_1+\\gamma R_{2}+...+\\gamma^{n-1} R_{n}+...|s, a, s^{'} \\right )  \\right ] \\\\ \n",
    "& = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma E(U_1|s, a, s^{'}) \\right ] \\\\\n",
    "& = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma E(U_1|s^{'}) \\right ]\n",
    "\\end{align}\n",
    "$$\n",
    "  - **(2)time horizon无限的条件下，行为价值函数的递归形式**\n",
    "$$\n",
    "\\begin{align}\n",
    "\\because E(U_t|s,a) & = E(U|s,a), E(U_{t+1}|s^{'})=E(U|s^{'}) \\\\\n",
    "\\therefore E(U|s,a) & = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma E(U|s^{'}) \\right ] \\\\\n",
    "\\therefore Q^{\\pi}(s,a) & =E(U|s,a) =\\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma V^{\\pi}(s^{'}) \\right ]\n",
    "\\end{align}\n",
    "$$\n",
    "  - (3)转换成贝尔曼期望方程的形态：\n",
    "$$\n",
    "\\begin{align}\n",
    "Q^{\\pi}(s,a) & = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma E(U|s^{'}) \\right ] \\\\\n",
    "& = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma \\sum_{a^{'}\\in \\mathcal{A}}\\pi(a^{'}|s^{'})E(U|s^{'},a^{'}) \\right ] \\\\\n",
    "& = \\sum_{s^{'}\\in \\mathcal{S}}P(s^{'}|s, a)\\left [ R(s, a, s^{'})+\\gamma \\sum_{a^{'}\\in \\mathcal{A}}\\pi(a^{'}|s^{'})Q^{\\pi}(s^{'},a^{'}) \\right ] \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
